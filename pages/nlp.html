<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Natural Language Processing - ML for NOAI</title>
    <link rel="stylesheet" href="../css/style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Fira+Code&display=swap" rel="stylesheet">
</head>
<body>
    <nav class="navbar">
        <div class="nav-container">
            <a href="../index.html" class="nav-logo">
                <span class="logo-icon">ü§ñ</span>
                <span>ML for NOAI</span>
            </a>
            <button class="nav-toggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            <ul class="nav-menu">
                <li><a href="../index.html" class="nav-link">Home</a></li>
                <li><a href="fundamentals.html" class="nav-link">Fundamentals</a></li>
                <li><a href="supervised.html" class="nav-link">Supervised Learning</a></li>
                <li><a href="unsupervised.html" class="nav-link">Unsupervised Learning</a></li>
                <li><a href="neural-networks.html" class="nav-link">Neural Networks</a></li>
                <li><a href="computer-vision.html" class="nav-link">Computer Vision</a></li>
                <li><a href="nlp.html" class="nav-link active">NLP</a></li>
            </ul>
        </div>
    </nav>

    <div class="breadcrumb">
        <div class="container">
            <ul>
                <li><a href="../index.html">Home</a></li>
                <li>Natural Language Processing</li>
            </ul>
        </div>
    </div>

    <header class="page-header">
        <div class="container">
            <h1>üí¨ Natural Language Processing</h1>
            <p class="subtitle">Teaching machines to understand, interpret, and generate human language.</p>
        </div>
    </header>

    <div class="progress-container">
        <div class="container">
            <div class="progress-bar">
                <div class="progress-fill"></div>
            </div>
        </div>
    </div>

    <main>
        <section id="overview" class="content-section">
            <div class="container">
                <h2>Introduction to NLP</h2>
                <p>Natural Language Processing (NLP) enables computers to understand, interpret, and generate human language. It bridges the gap between human communication and machine understanding.</p>

                <div class="concept-grid">
                    <div class="concept-card">
                        <h4>üìù Text Classification</h4>
                        <p>Categorize text into predefined labels</p>
                        <p><em>Spam detection, sentiment analysis</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>üè∑Ô∏è Named Entity Recognition</h4>
                        <p>Identify entities like names, places, dates</p>
                        <p><em>"Apple Inc. is in California"</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>‚ùì Question Answering</h4>
                        <p>Extract answers from text given questions</p>
                        <p><em>Q: "Who founded Tesla?" A: "Elon Musk"</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>üîÑ Machine Translation</h4>
                        <p>Translate between languages</p>
                        <p><em>English ‚Üí Chinese</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>üìä Text Summarization</h4>
                        <p>Create concise summaries of documents</p>
                        <p><em>Long article ‚Üí Key points</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>‚úçÔ∏è Text Generation</h4>
                        <p>Generate human-like text</p>
                        <p><em>ChatGPT, story writing</em></p>
                    </div>
                </div>
            </div>
        </section>

        <section id="preprocessing" class="content-section">
            <div class="container">
                <h2>Text Preprocessing</h2>
                <p>Before feeding text to models, we need to clean and convert it to numerical form.</p>

                <h3>Tokenization</h3>
                <p>Breaking text into smaller units (tokens).</p>

                <div class="concept-grid">
                    <div class="concept-card">
                        <h4>Word Tokenization</h4>
                        <p>"I love NLP" ‚Üí ["I", "love", "NLP"]</p>
                        <p><em>Simple but has vocabulary issues</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>Subword Tokenization</h4>
                        <p>"unhappiness" ‚Üí ["un", "happiness"]</p>
                        <p><em>Handles rare words better (BPE, WordPiece)</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>Character Tokenization</h4>
                        <p>"hello" ‚Üí ["h", "e", "l", "l", "o"]</p>
                        <p><em>Small vocabulary, longer sequences</em></p>
                    </div>
                </div>

                <h3>Common Preprocessing Steps</h3>
                <ul>
                    <li><strong>Lowercasing:</strong> "Hello" ‚Üí "hello"</li>
                    <li><strong>Removing punctuation:</strong> "Hello!" ‚Üí "Hello"</li>
                    <li><strong>Removing stopwords:</strong> Remove common words (the, is, at)</li>
                    <li><strong>Stemming:</strong> "running" ‚Üí "run" (chop endings)</li>
                    <li><strong>Lemmatization:</strong> "better" ‚Üí "good" (use dictionary)</li>
                </ul>

                <pre><code># Python: Text Preprocessing
import re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer

text = "The cats are running quickly through the gardens!"

# Lowercase
text = text.lower()

# Remove punctuation
text = re.sub(r'[^\w\s]', '', text)

# Tokenize
tokens = word_tokenize(text)
# ['the', 'cats', 'are', 'running', 'quickly', 'through', 'the', 'gardens']

# Remove stopwords
stop_words = set(stopwords.words('english'))
tokens = [t for t in tokens if t not in stop_words]
# ['cats', 'running', 'quickly', 'gardens']

# Stemming
stemmer = PorterStemmer()
stems = [stemmer.stem(t) for t in tokens]
# ['cat', 'run', 'quick', 'garden']</code></pre>

                <div class="tip-box">
                    Modern deep learning models (like BERT) often work with raw text and learn their own representations. Heavy preprocessing may not be needed!
                </div>
            </div>
        </section>

        <section id="embeddings" class="content-section">
            <div class="container">
                <h2>Word Embeddings</h2>
                <p>Word embeddings convert words into dense vectors that capture semantic meaning.</p>

                <h3>Why Embeddings?</h3>
                <p>One-hot encoding (where "cat" = [1,0,0,...]) has problems:</p>
                <ul>
                    <li>Vectors are huge (vocabulary size)</li>
                    <li>All words are equally different (no similarity)</li>
                    <li>"cat" and "dog" should be closer than "cat" and "airplane"</li>
                </ul>

                <div class="algorithm-card">
                    <h3>Word2Vec</h3>
                    <div class="algorithm-meta">
                        <span>üìä Type: Embedding</span>
                        <span>üìö NOAI: Theory + Practice</span>
                    </div>

                    <p>Learn word vectors by predicting context. Two architectures:</p>

                    <h4>Skip-gram</h4>
                    <p>Given a word, predict surrounding words</p>
                    <p><em>"The [cat] sat on" ‚Üí predict "The", "sat", "on"</em></p>

                    <h4>CBOW (Continuous Bag of Words)</h4>
                    <p>Given surrounding words, predict the center word</p>
                    <p><em>"The __ sat on" ‚Üí predict "cat"</em></p>

                    <h4>Key Property</h4>
                    <div class="formula">
                        king - man + woman ‚âà queen
                    </div>
                    <p>Embeddings capture semantic relationships!</p>
                </div>

                <h3>Using Pre-trained Embeddings</h3>
                <pre><code># Python: Word Embeddings with Gensim
from gensim.models import KeyedVectors

# Load pre-trained Word2Vec (Google News)
model = KeyedVectors.load_word2vec_format('GoogleNews-vectors.bin', binary=True)

# Get word vector
vector = model['computer']  # 300-dimensional vector

# Find similar words
similar = model.most_similar('king', topn=5)
# [('kings', 0.71), ('queen', 0.65), ('monarch', 0.64), ...]

# Analogy: king - man + woman = ?
result = model.most_similar(positive=['king', 'woman'], negative=['man'])
# [('queen', 0.71), ...]</code></pre>
            </div>
        </section>

        <section id="rnn" class="content-section">
            <div class="container">
                <h2>Recurrent Neural Networks (RNNs)</h2>
                <p>RNNs process sequences by maintaining a "memory" (hidden state) that captures information from previous steps.</p>

                <div class="algorithm-card">
                    <h3>Basic RNN</h3>
                    <div class="algorithm-meta">
                        <span>üîÑ Type: Sequence Model</span>
                        <span>üìö NOAI: Theory</span>
                    </div>

                    <div class="formula">
                        <div class="formula-title">RNN Equations</div>
                        h‚Çú = tanh(W‚Çï‚Çï √ó h‚Çú‚Çã‚ÇÅ + W‚Çì‚Çï √ó x‚Çú + b)
                        y‚Çú = W‚Çï·µß √ó h‚Çú

                        h‚Çú = hidden state at time t
                        x‚Çú = input at time t
                    </div>

                    <h4>Problem: Vanishing Gradient</h4>
                    <p>For long sequences, gradients become very small, making it hard to learn long-range dependencies.</p>
                </div>

                <h3>LSTM (Long Short-Term Memory)</h3>
                <div class="algorithm-card">
                    <div class="algorithm-meta">
                        <span>üß† Type: Gated RNN</span>
                        <span>üìö NOAI: Practice</span>
                    </div>

                    <p>LSTM solves vanishing gradients with <strong>gates</strong> that control information flow:</p>

                    <ul>
                        <li><strong>Forget Gate:</strong> What to remove from memory</li>
                        <li><strong>Input Gate:</strong> What new information to store</li>
                        <li><strong>Output Gate:</strong> What to output</li>
                        <li><strong>Cell State:</strong> Long-term memory (can persist unchanged)</li>
                    </ul>
                </div>

                <pre><code># PyTorch: LSTM for Text Classification
import torch.nn as nn

class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True,
                           bidirectional=True)
        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # *2 for bidirectional

    def forward(self, x):
        # x shape: (batch, seq_len)
        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)
        lstm_out, (hidden, cell) = self.lstm(embedded)

        # Use final hidden state from both directions
        hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)
        output = self.fc(hidden)
        return output</code></pre>
            </div>
        </section>

        <section id="attention" class="content-section">
            <div class="container">
                <h2>Attention Mechanism</h2>
                <p>Attention allows models to focus on relevant parts of the input when producing output.</p>

                <div class="algorithm-card">
                    <h3>The Attention Idea</h3>
                    <div class="algorithm-meta">
                        <span>üéØ Type: Mechanism</span>
                        <span>üìö NOAI: Theory + Practice</span>
                    </div>

                    <h4>Problem with RNNs</h4>
                    <p>All information must pass through a fixed-size hidden state (bottleneck).</p>

                    <h4>Solution: Attention</h4>
                    <p>Look at ALL input positions and compute weighted importance for each.</p>

                    <div class="formula">
                        <div class="formula-title">Attention Computation</div>
                        1. Query (Q): what we're looking for
                        2. Keys (K): what we're searching through
                        3. Values (V): the actual content

                        Attention(Q, K, V) = softmax(QK·µÄ/‚àöd‚Çñ) √ó V
                    </div>
                </div>

                <h3>Self-Attention</h3>
                <p>When Q, K, V all come from the same sequence. Each word attends to all other words in the sentence.</p>
                <p><em>"The cat sat on the mat because it was tired"</em></p>
                <p>Self-attention helps "it" understand it refers to "cat".</p>
            </div>
        </section>

        <section id="transformers" class="content-section">
            <div class="container">
                <h2>Transformers</h2>
                <p>Transformers replaced RNNs as the dominant architecture for NLP. They use only attention‚Äîno recurrence!</p>

                <div class="algorithm-card">
                    <h3>Transformer Architecture</h3>
                    <div class="algorithm-meta">
                        <span>üèóÔ∏è Type: Architecture</span>
                        <span>üìö NOAI: Theory + Practice</span>
                        <span>üèÜ Foundation of Modern NLP</span>
                    </div>

                    <h4>Key Components</h4>
                    <ul>
                        <li><strong>Multi-Head Attention:</strong> Multiple attention layers in parallel</li>
                        <li><strong>Positional Encoding:</strong> Adds position information (since no recurrence)</li>
                        <li><strong>Feed-Forward Networks:</strong> Process attention outputs</li>
                        <li><strong>Layer Normalization:</strong> Stabilize training</li>
                        <li><strong>Residual Connections:</strong> Skip connections like ResNet</li>
                    </ul>

                    <h4>Advantages over RNNs</h4>
                    <ul>
                        <li><strong>Parallelization:</strong> Process all positions simultaneously</li>
                        <li><strong>Long-range dependencies:</strong> Direct connections between any positions</li>
                        <li><strong>Scalability:</strong> Train much larger models</li>
                    </ul>
                </div>

                <h3>Encoder vs Decoder</h3>
                <div class="concept-grid">
                    <div class="concept-card">
                        <h4>Encoder-only</h4>
                        <p>Understands input text</p>
                        <p><em>Example: BERT</em></p>
                        <p>Tasks: Classification, NER</p>
                    </div>
                    <div class="concept-card">
                        <h4>Decoder-only</h4>
                        <p>Generates text autoregressively</p>
                        <p><em>Example: GPT</em></p>
                        <p>Tasks: Text generation</p>
                    </div>
                    <div class="concept-card">
                        <h4>Encoder-Decoder</h4>
                        <p>Maps input to output sequence</p>
                        <p><em>Example: T5, BART</em></p>
                        <p>Tasks: Translation, summarization</p>
                    </div>
                </div>
            </div>
        </section>

        <section id="bert" class="content-section">
            <div class="container">
                <h2>BERT and Pre-trained Models</h2>
                <p>Pre-trained language models revolutionized NLP by learning general language understanding from massive text corpora.</p>

                <div class="algorithm-card">
                    <h3>BERT (Bidirectional Encoder Representations from Transformers)</h3>
                    <div class="algorithm-meta">
                        <span>ü§ñ Type: Pre-trained Model</span>
                        <span>üìö NOAI: Theory + Practice</span>
                    </div>

                    <h4>Pre-training Tasks</h4>
                    <ul>
                        <li><strong>Masked Language Model (MLM):</strong> Predict masked words
                            <p>"The [MASK] sat on the mat" ‚Üí predict "cat"</p>
                        </li>
                        <li><strong>Next Sentence Prediction:</strong> Are two sentences consecutive?</li>
                    </ul>

                    <h4>Why Bidirectional?</h4>
                    <p>Unlike GPT (left-to-right), BERT sees context from BOTH directions.</p>
                </div>

                <h3>Using BERT with Hugging Face</h3>
                <pre><code># Python: BERT with Hugging Face Transformers
from transformers import BertTokenizer, BertForSequenceClassification
from transformers import Trainer, TrainingArguments

# Load pre-trained BERT
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=2  # For binary classification
)

# Tokenize input
text = "This movie was amazing!"
inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)

# Get prediction
outputs = model(**inputs)
predictions = outputs.logits.argmax(dim=-1)

# Fine-tuning
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    learning_rate=2e-5,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)
trainer.train()</code></pre>

                <h3>Popular Pre-trained Models</h3>
                <table class="comparison-table">
                    <thead>
                        <tr>
                            <th>Model</th>
                            <th>Type</th>
                            <th>Best For</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>BERT</strong></td>
                            <td>Encoder</td>
                            <td>Classification, NER, QA</td>
                        </tr>
                        <tr>
                            <td><strong>RoBERTa</strong></td>
                            <td>Encoder</td>
                            <td>Same as BERT, better performance</td>
                        </tr>
                        <tr>
                            <td><strong>GPT-2/3/4</strong></td>
                            <td>Decoder</td>
                            <td>Text generation</td>
                        </tr>
                        <tr>
                            <td><strong>T5</strong></td>
                            <td>Encoder-Decoder</td>
                            <td>Any text-to-text task</td>
                        </tr>
                        <tr>
                            <td><strong>DistilBERT</strong></td>
                            <td>Encoder</td>
                            <td>Faster, smaller BERT</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </section>

        <section id="text-classification" class="content-section">
            <div class="container">
                <h2>Text Classification Pipeline</h2>
                <p>A complete workflow for text classification tasks like sentiment analysis.</p>

                <pre><code># Complete Text Classification with BERT
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
from sklearn.model_selection import train_test_split

# 1. Custom Dataset
class TextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]

        encoding = self.tokenizer(
            text,
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'label': torch.tensor(label)
        }

# 2. Setup
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# 3. Prepare data
train_texts, val_texts, train_labels, val_labels = train_test_split(
    texts, labels, test_size=0.2
)

train_dataset = TextDataset(train_texts, train_labels, tokenizer)
val_dataset = TextDataset(val_texts, val_labels, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16)

# 4. Training loop
optimizer = AdamW(model.parameters(), lr=2e-5)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

for epoch in range(3):
    model.train()
    for batch in train_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print(f"Epoch {epoch+1} completed")</code></pre>
            </div>
        </section>

        <section id="language-modeling" class="content-section">
            <div class="container">
                <h2>Language Modeling</h2>
                <p>Language models predict the probability of text sequences. They're the foundation of modern NLP.</p>

                <div class="concept-grid">
                    <div class="concept-card">
                        <h4>Autoregressive LM</h4>
                        <p>Predict next word given previous words</p>
                        <p>P(w‚Çú | w‚ÇÅ, w‚ÇÇ, ..., w‚Çú‚Çã‚ÇÅ)</p>
                        <p><em>Examples: GPT series</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>Masked LM</h4>
                        <p>Predict masked words given context</p>
                        <p>P(w‚Çò‚Çê‚Çõ‚Çñ‚Çë‚Çê | context)</p>
                        <p><em>Examples: BERT</em></p>
                    </div>
                </div>

                <h3>Perplexity</h3>
                <p>The standard metric for language models. Lower is better.</p>
                <div class="formula">
                    <div class="formula-title">Perplexity</div>
                    PPL = exp(average negative log-likelihood)

                    PPL = 10 means model is "confused" between 10 choices on average
                </div>
            </div>
        </section>

        <section id="quiz" class="content-section">
            <div class="container">
                <h2>NOAI Practice Questions - Natural Language Processing</h2>

                <div class="tip-box">
                    <strong>NOAI Exam Tips:</strong> NLP questions focus on understanding transformers, attention mechanisms, and the differences between models like BERT and GPT. Remember: BERT is bidirectional (encoder), GPT is autoregressive (decoder), and attention allows focusing on relevant input parts!
                </div>

                <h3>Tokenization & Text Processing</h3>

                <div class="quiz-container" data-correct="b" data-explanation="Subword tokenization (BPE, WordPiece) handles rare and out-of-vocabulary words by breaking them into smaller pieces. It balances vocabulary size with sequence length.">
                    <p class="quiz-question">1. What is the main advantage of subword tokenization over word tokenization?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) It produces shorter sequences</span></div>
                        <div class="quiz-option" data-value="b"><span>B) It handles rare and out-of-vocabulary words better</span></div>
                        <div class="quiz-option" data-value="c"><span>C) It requires no training</span></div>
                        <div class="quiz-option" data-value="d"><span>D) It preserves punctuation</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="Character tokenization has a very small fixed vocabulary (alphabet size) but produces very long sequences. It's rarely used alone in modern NLP but can handle any text.">
                    <p class="quiz-question">2. What is a disadvantage of character-level tokenization?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Large vocabulary size</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Cannot handle rare words</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Very long sequences that are harder to model</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Requires language-specific rules</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h3>Word Embeddings</h3>

                <div class="quiz-container" data-correct="c" data-explanation="Word embeddings capture semantic meaning, placing similar words closer together in vector space, unlike one-hot encoding where all words are equally distant.">
                    <p class="quiz-question">3. Why are word embeddings better than one-hot encoding?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) They use less memory</span></div>
                        <div class="quiz-option" data-value="b"><span>B) They're faster to compute</span></div>
                        <div class="quiz-option" data-value="c"><span>C) They capture semantic similarity between words</span></div>
                        <div class="quiz-option" data-value="d"><span>D) They work with any language automatically</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="Skip-gram predicts context words given a center word ('cat' -> predict 'the', 'sat', 'on'). CBOW does the opposite - predicts center word from context.">
                    <p class="quiz-question">4. In Word2Vec, what does the Skip-gram model predict?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Context words given a center word</span></div>
                        <div class="quiz-option" data-value="b"><span>B) The center word given context words</span></div>
                        <div class="quiz-option" data-value="c"><span>C) The next sentence</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Part-of-speech tags</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="CBOW (Continuous Bag of Words) predicts the center word from surrounding context words. Skip-gram does the opposite. CBOW is faster but Skip-gram works better for rare words.">
                    <p class="quiz-question">5. How does CBOW differ from Skip-gram in Word2Vec?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) CBOW uses character-level processing</span></div>
                        <div class="quiz-option" data-value="b"><span>B) CBOW predicts center word from context, Skip-gram predicts context from center</span></div>
                        <div class="quiz-option" data-value="c"><span>C) CBOW produces higher-dimensional vectors</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Skip-gram cannot handle multiple meanings</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h3>RNN & LSTM</h3>

                <div class="quiz-container" data-correct="d" data-explanation="The vanishing gradient problem occurs in RNNs when gradients become very small during backpropagation through many time steps, making it hard to learn long-range dependencies.">
                    <p class="quiz-question">6. What problem do basic RNNs face with long sequences?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) They run out of memory</span></div>
                        <div class="quiz-option" data-value="b"><span>B) They cannot process variable-length inputs</span></div>
                        <div class="quiz-option" data-value="c"><span>C) They require too much training data</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Vanishing gradients make learning long-range dependencies difficult</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="LSTMs use gate mechanisms (forget, input, output) that control information flow and allow gradients to flow through the cell state, solving the vanishing gradient problem.">
                    <p class="quiz-question">7. How do LSTMs address the vanishing gradient problem?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) By using larger hidden states</span></div>
                        <div class="quiz-option" data-value="b"><span>B) By processing sequences backwards</span></div>
                        <div class="quiz-option" data-value="c"><span>C) By using gate mechanisms that control information flow</span></div>
                        <div class="quiz-option" data-value="d"><span>D) By reducing sequence length</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h3>Attention Mechanism</h3>

                <div class="quiz-container" data-correct="a" data-explanation="NOAI 2025 Style: Attention mechanisms allow the model to focus on relevant parts of the input sequence when producing each output, rather than compressing everything into a fixed-size vector.">
                    <p class="quiz-question">8. [NOAI Style] What is the primary purpose of attention mechanisms in neural networks?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Allow the model to focus on relevant parts of the input sequence</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Reduce the number of model parameters</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Speed up inference time</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Handle variable-length inputs</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="In self-attention, each token attends to all other tokens in the same sequence, computing weighted representations. This captures relationships like 'it' referring to 'cat' in 'The cat sat because it was tired'.">
                    <p class="quiz-question">9. What happens in self-attention?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) The model attends only to the previous token</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Each token attends to all tokens in the same sequence</span></div>
                        <div class="quiz-option" data-value="c"><span>C) The model attends to external knowledge bases</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Attention is applied only to the last layer</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h3>Transformer Architecture</h3>

                <div class="quiz-container" data-correct="b" data-explanation="Transformers use positional encodings to add position information since, unlike RNNs, they process all positions in parallel and have no inherent notion of order.">
                    <p class="quiz-question">10. How do Transformers know the order of words without recurrence?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) They don't need order information</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Positional encodings added to input embeddings</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Attention weights implicitly encode position</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Layer normalization preserves order</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="Multi-head attention runs multiple attention operations in parallel with different learned projections, allowing the model to capture different types of relationships simultaneously.">
                    <p class="quiz-question">11. What is the purpose of multi-head attention in Transformers?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) To process multiple sequences at once</span></div>
                        <div class="quiz-option" data-value="b"><span>B) To reduce computational cost</span></div>
                        <div class="quiz-option" data-value="c"><span>C) To capture different types of relationships in parallel</span></div>
                        <div class="quiz-option" data-value="d"><span>D) To handle longer sequences</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h3>BERT vs GPT</h3>

                <div class="quiz-container" data-correct="a" data-explanation="BERT uses Masked Language Modeling (MLM) where random words are masked and the model learns to predict them using bidirectional context from both left and right.">
                    <p class="quiz-question">12. What pre-training task does BERT primarily use?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Masked Language Modeling (MLM)</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Next word prediction</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Machine translation</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Sentiment classification</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="NOAI 2025 Style: GPT uses causal (unidirectional) masking to prevent the model from attending to future tokens during training. This ensures autoregressive generation where each token only depends on previous tokens.">
                    <p class="quiz-question">13. [NOAI Style] Why does GPT use causal masking during training?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) To reduce memory usage</span></div>
                        <div class="quiz-option" data-value="b"><span>B) To improve training speed</span></div>
                        <div class="quiz-option" data-value="c"><span>C) To handle multiple languages</span></div>
                        <div class="quiz-option" data-value="d"><span>D) To prevent the model from attending to future tokens</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="BERT is encoder-only (bidirectional, good for understanding tasks). GPT is decoder-only (unidirectional, good for generation). BERT sees context from both sides; GPT only sees past tokens.">
                    <p class="quiz-question">14. What is the key architectural difference between BERT and GPT?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) BERT is smaller than GPT</span></div>
                        <div class="quiz-option" data-value="b"><span>B) BERT is encoder-only (bidirectional), GPT is decoder-only (unidirectional)</span></div>
                        <div class="quiz-option" data-value="c"><span>C) GPT uses attention, BERT does not</span></div>
                        <div class="quiz-option" data-value="d"><span>D) BERT can only handle English text</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="NOAI 2025 Style: The [CLS] token in BERT is a special token at the start of input that aggregates sequence-level information. Its final hidden state represents a pooled representation used for classification tasks.">
                    <p class="quiz-question">15. [NOAI Style] What is the purpose of the [CLS] token in BERT?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) To mark the end of a sentence</span></div>
                        <div class="quiz-option" data-value="b"><span>B) To separate two sentences in the input</span></div>
                        <div class="quiz-option" data-value="c"><span>C) To provide a pooled representation for classification tasks</span></div>
                        <div class="quiz-option" data-value="d"><span>D) To indicate masked positions</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h3>Pre-training & Fine-tuning</h3>

                <div class="quiz-container" data-correct="b" data-explanation="NOAI 2025 Style: BERT (and similar pre-trained models) are commonly used for transfer learning in NLP, where a model pre-trained on large corpora is fine-tuned on specific downstream tasks.">
                    <p class="quiz-question">16. [NOAI Style] Which model is commonly used as a starting point for transfer learning in NLP tasks?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Word2Vec</span></div>
                        <div class="quiz-option" data-value="b"><span>B) BERT</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Basic RNN</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Bag of Words</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="Next Sentence Prediction (NSP) was one of BERT's original pre-training tasks, where the model learns to predict if two sentences are consecutive in the original text.">
                    <p class="quiz-question">17. What does BERT's Next Sentence Prediction (NSP) task involve?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Predicting if two sentences are consecutive in the original text</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Generating the next sentence given context</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Translating sentences to another language</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Classifying sentence sentiment</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="Fine-tuning updates all model weights on task-specific data with a small learning rate. Feature extraction freezes pre-trained weights and only trains a new classifier head.">
                    <p class="quiz-question">18. What is the difference between fine-tuning and feature extraction in transfer learning?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Fine-tuning is faster</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Feature extraction requires more data</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Fine-tuning only works with BERT</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Fine-tuning updates all weights, feature extraction freezes pre-trained weights</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h3>Text Generation & Decoding</h3>

                <div class="quiz-container" data-correct="b" data-explanation="NOAI 2025 Style: Beam search optimizes the decoding process by exploring multiple candidate output sequences simultaneously and keeping the top-k most probable sequences at each step.">
                    <p class="quiz-question">19. [NOAI Style] What is the purpose of beam search in text generation?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) To speed up training</span></div>
                        <div class="quiz-option" data-value="b"><span>B) To optimize decoding by exploring multiple output sequences</span></div>
                        <div class="quiz-option" data-value="c"><span>C) To reduce model size</span></div>
                        <div class="quiz-option" data-value="d"><span>D) To handle longer input sequences</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="Greedy decoding always picks the highest probability token at each step. Beam search maintains multiple candidates. Greedy is faster but may miss globally better sequences.">
                    <p class="quiz-question">20. How does greedy decoding differ from beam search?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Greedy decoding is slower but more accurate</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Beam search only considers the last token</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Greedy always picks the highest probability token, beam search keeps multiple candidates</span></div>
                        <div class="quiz-option" data-value="d"><span>D) There is no practical difference</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h3>Additional NLP Practice</h3>

                <h4>Advanced Tokenization</h4>

                <div class="quiz-container" data-correct="b" data-explanation="BPE (Byte-Pair Encoding) iteratively merges the most frequent pairs of adjacent tokens, building a vocabulary of subword units that balances vocabulary size with handling of rare words.">
                    <p class="quiz-question">21. How does Byte-Pair Encoding (BPE) build its vocabulary?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) By splitting words at morpheme boundaries</span></div>
                        <div class="quiz-option" data-value="b"><span>B) By iteratively merging the most frequent character pairs</span></div>
                        <div class="quiz-option" data-value="c"><span>C) By using a fixed dictionary of common words</span></div>
                        <div class="quiz-option" data-value="d"><span>D) By random sampling from the corpus</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="WordPiece, used in BERT, is similar to BPE but uses likelihood-based scoring to decide which merges to perform, optimizing for better language modeling performance.">
                    <p class="quiz-question">22. [NOAI Style] What tokenization algorithm does BERT use?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) WordPiece</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Word-level tokenization</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Character-level tokenization</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Sentence-level tokenization</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="The [SEP] token in BERT separates two sentences when the input contains a sentence pair, allowing the model to distinguish between segments for tasks like question answering.">
                    <p class="quiz-question">23. What is the purpose of the [SEP] token in BERT?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) To mark positions for masking</span></div>
                        <div class="quiz-option" data-value="b"><span>B) To indicate the start of input</span></div>
                        <div class="quiz-option" data-value="c"><span>C) To pad sequences to equal length</span></div>
                        <div class="quiz-option" data-value="d"><span>D) To separate two sentences in the input</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Word Embeddings Deep Dive</h4>

                <div class="quiz-container" data-correct="c" data-explanation="GloVe (Global Vectors) learns embeddings by factorizing a word co-occurrence matrix, capturing global statistical information unlike Word2Vec's local context windows.">
                    <p class="quiz-question">24. What distinguishes GloVe from Word2Vec?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) GloVe uses neural networks, Word2Vec doesn't</span></div>
                        <div class="quiz-option" data-value="b"><span>B) GloVe produces smaller vectors</span></div>
                        <div class="quiz-option" data-value="c"><span>C) GloVe learns from global word co-occurrence statistics</span></div>
                        <div class="quiz-option" data-value="d"><span>D) GloVe handles subword information</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="FastText represents words as bags of character n-grams, allowing it to generate embeddings for out-of-vocabulary words by combining their character n-gram vectors.">
                    <p class="quiz-question">25. How does FastText handle out-of-vocabulary (OOV) words?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) It assigns a zero vector to OOV words</span></div>
                        <div class="quiz-option" data-value="b"><span>B) It uses character n-gram embeddings to construct word vectors</span></div>
                        <div class="quiz-option" data-value="c"><span>C) It replaces OOV words with the nearest vocabulary word</span></div>
                        <div class="quiz-option" data-value="d"><span>D) It requires all words to be in the vocabulary</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="Static embeddings (Word2Vec, GloVe) give each word a single fixed vector regardless of context. 'Bank' has the same embedding whether meaning 'river bank' or 'financial bank'.">
                    <p class="quiz-question">26. What is a limitation of static word embeddings like Word2Vec?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) They give the same vector for a word regardless of context</span></div>
                        <div class="quiz-option" data-value="b"><span>B) They cannot represent common words</span></div>
                        <div class="quiz-option" data-value="c"><span>C) They require labeled training data</span></div>
                        <div class="quiz-option" data-value="d"><span>D) They only work for English text</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="Contextual embeddings from models like BERT and ELMo produce different vectors for the same word depending on its context, capturing polysemy (multiple meanings).">
                    <p class="quiz-question">27. [NOAI Style] What makes BERT embeddings 'contextual'?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) They are computed faster than Word2Vec</span></div>
                        <div class="quiz-option" data-value="b"><span>B) They use a larger vocabulary</span></div>
                        <div class="quiz-option" data-value="c"><span>C) They handle multiple languages</span></div>
                        <div class="quiz-option" data-value="d"><span>D) The same word gets different representations based on surrounding words</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Sequence Models</h4>

                <div class="quiz-container" data-correct="c" data-explanation="GRU (Gated Recurrent Unit) simplifies LSTM by combining the forget and input gates into a single 'update gate' and merging the cell state with hidden state, using fewer parameters.">
                    <p class="quiz-question">28. How does GRU differ from LSTM?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) GRU has more gates than LSTM</span></div>
                        <div class="quiz-option" data-value="b"><span>B) GRU cannot handle long sequences</span></div>
                        <div class="quiz-option" data-value="c"><span>C) GRU has fewer gates and parameters, combining forget and input gates</span></div>
                        <div class="quiz-option" data-value="d"><span>D) GRU requires bidirectional processing</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="Bidirectional RNNs process sequences in both forward and backward directions, then combine the outputs. This provides context from both past and future for each position.">
                    <p class="quiz-question">29. What is the purpose of bidirectional RNNs?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) To double the processing speed</span></div>
                        <div class="quiz-option" data-value="b"><span>B) To capture context from both past and future positions</span></div>
                        <div class="quiz-option" data-value="c"><span>C) To reduce the number of parameters</span></div>
                        <div class="quiz-option" data-value="d"><span>D) To handle variable-length sequences</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="Teacher forcing feeds the ground truth previous token during training instead of the model's own prediction. This accelerates training but can cause exposure bias at inference time.">
                    <p class="quiz-question">30. What is 'teacher forcing' in sequence-to-sequence models?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Using ground truth tokens as input during training</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Training with multiple teacher models</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Forcing the model to generate longer sequences</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Using human feedback to correct predictions</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Transformer Details</h4>

                <div class="quiz-container" data-correct="d" data-explanation="The scaling factor ‚àöd‚Çñ in attention prevents dot products from becoming too large when the dimension is high, which would push softmax into regions with very small gradients.">
                    <p class="quiz-question">31. [NOAI Style] Why does scaled dot-product attention divide by ‚àöd‚Çñ?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) To reduce computation time</span></div>
                        <div class="quiz-option" data-value="b"><span>B) To normalize the output to unit length</span></div>
                        <div class="quiz-option" data-value="c"><span>C) To make gradients larger</span></div>
                        <div class="quiz-option" data-value="d"><span>D) To prevent softmax saturation when dimensions are large</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="Layer normalization in Transformers normalizes across the feature dimension for each token independently, stabilizing training and allowing faster convergence.">
                    <p class="quiz-question">32. What does layer normalization do in Transformers?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Normalizes across the batch dimension</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Normalizes attention weights</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Normalizes features for each token independently</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Normalizes the entire layer's outputs together</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="Sinusoidal positional encodings use sine and cosine functions of different frequencies to create unique position vectors. They allow the model to extrapolate to longer sequences than seen in training.">
                    <p class="quiz-question">33. What type of positional encoding does the original Transformer use?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Learned position embeddings only</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Fixed sinusoidal functions of position</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Relative position encodings</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Binary position indicators</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="Cross-attention in the decoder attends to encoder outputs, allowing the decoder to focus on relevant parts of the input when generating each output token (e.g., in translation).">
                    <p class="quiz-question">34. What is the role of cross-attention in encoder-decoder Transformers?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Allows decoder to attend to encoder outputs</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Connects different attention heads</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Processes multiple input sequences</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Normalizes attention across layers</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>BERT Variants and Improvements</h4>

                <div class="quiz-container" data-correct="c" data-explanation="RoBERTa removes Next Sentence Prediction, uses dynamic masking, trains with larger batches and more data, showing that BERT was significantly undertrained.">
                    <p class="quiz-question">35. [NOAI Style] What key change did RoBERTa make to improve upon BERT?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Added more transformer layers</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Used character-level tokenization</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Removed Next Sentence Prediction and used dynamic masking</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Changed from encoder to decoder architecture</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="DistilBERT uses knowledge distillation to create a smaller, faster model that retains ~97% of BERT's performance with 40% fewer parameters and 60% faster inference.">
                    <p class="quiz-question">36. How does DistilBERT achieve its efficiency compared to BERT?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) By using smaller vocabulary</span></div>
                        <div class="quiz-option" data-value="b"><span>B) By knowledge distillation - learning from BERT's outputs</span></div>
                        <div class="quiz-option" data-value="c"><span>C) By removing attention mechanisms</span></div>
                        <div class="quiz-option" data-value="d"><span>D) By training on less data</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="ALBERT reduces parameters through factorized embedding parameterization (separating vocabulary and hidden size) and cross-layer parameter sharing, while maintaining performance.">
                    <p class="quiz-question">37. What technique does ALBERT use to reduce parameters?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Pruning attention heads</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Reducing vocabulary size</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Using fewer transformer layers</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Factorized embeddings and cross-layer parameter sharing</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>GPT and Language Models</h4>

                <div class="quiz-container" data-correct="a" data-explanation="GPT is trained on next token prediction (causal language modeling), predicting each token given all previous tokens. This is different from BERT's masked language modeling.">
                    <p class="quiz-question">38. What objective is GPT trained on?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Next token prediction (causal language modeling)</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Masked language modeling</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Sentence classification</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Question answering</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="Perplexity measures how 'surprised' a language model is by test data. Lower perplexity means the model assigns higher probability to the actual text. PPL = exp(average cross-entropy loss).">
                    <p class="quiz-question">39. [NOAI Style] What does perplexity measure in language models?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Model size in parameters</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Training speed</span></div>
                        <div class="quiz-option" data-value="c"><span>C) How well the model predicts the test data (lower is better)</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Vocabulary coverage</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="Temperature controls randomness in sampling. Higher temperature makes the distribution more uniform (more random), while lower temperature makes it sharper (more deterministic).">
                    <p class="quiz-question">40. What effect does increasing temperature have on text generation?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Makes output more deterministic and focused</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Makes output more random and diverse</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Generates longer sequences</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Improves grammatical correctness</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="Top-k sampling limits choices to the k most likely tokens, then samples from this reduced set. Top-p (nucleus) sampling includes tokens until cumulative probability reaches p.">
                    <p class="quiz-question">41. What is the difference between top-k and top-p (nucleus) sampling?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Top-k is deterministic, top-p is stochastic</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Top-k works on sentences, top-p on words</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Top-p is faster than top-k</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Top-k uses fixed number of tokens, top-p uses cumulative probability threshold</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>NLP Tasks</h4>

                <div class="quiz-container" data-correct="a" data-explanation="Named Entity Recognition (NER) identifies and classifies entities like person names, organizations, locations, dates, etc. in text. It's a sequence labeling task using schemes like BIO.">
                    <p class="quiz-question">42. What does Named Entity Recognition (NER) do?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Identifies and classifies named entities like people, organizations, locations</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Generates names for new entities</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Translates entity names between languages</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Detects whether text contains any entities</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="BIO tagging marks B (Beginning) for the first token of an entity, I (Inside) for subsequent tokens, and O (Outside) for non-entity tokens.">
                    <p class="quiz-question">43. [NOAI Style] In BIO tagging for NER, what does 'B-PER' indicate?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) A token between two person names</span></div>
                        <div class="quiz-option" data-value="b"><span>B) A generic person reference</span></div>
                        <div class="quiz-option" data-value="c"><span>C) The beginning token of a person entity</span></div>
                        <div class="quiz-option" data-value="d"><span>D) A token that could be a person</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="Extractive summarization selects and concatenates important sentences from the source. Abstractive summarization generates new text that may use different words, like how humans summarize.">
                    <p class="quiz-question">44. What is the difference between extractive and abstractive summarization?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Extractive is longer than abstractive</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Extractive selects original sentences, abstractive generates new text</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Abstractive only works on news articles</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Extractive requires human review</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="SQuAD-style extractive QA finds the answer span within the given context passage. The model predicts start and end positions of the answer in the passage.">
                    <p class="quiz-question">45. In extractive question answering, what does the model predict?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Start and end positions of the answer span in the context</span></div>
                        <div class="quiz-option" data-value="b"><span>B) A yes/no answer</span></div>
                        <div class="quiz-option" data-value="c"><span>C) A generated text answer</span></div>
                        <div class="quiz-option" data-value="d"><span>D) The relevance score of the question</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Machine Translation</h4>

                <div class="quiz-container" data-correct="d" data-explanation="BLEU (Bilingual Evaluation Understudy) measures n-gram overlap between machine translation output and reference translations, with a brevity penalty for short outputs.">
                    <p class="quiz-question">46. What does the BLEU score measure in machine translation?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Translation speed</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Semantic similarity to the source</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Grammatical correctness</span></div>
                        <div class="quiz-option" data-value="d"><span>D) N-gram overlap with reference translations</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="Encoder-decoder models like T5 and BART are well-suited for translation because they encode the source sentence into a representation, then decode it into the target language.">
                    <p class="quiz-question">47. [NOAI Style] Which transformer architecture is most suitable for machine translation?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Encoder-only (like BERT)</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Encoder-decoder (like T5)</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Decoder-only (like GPT)</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Any architecture works equally well</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Modern LLM Concepts</h4>

                <div class="quiz-container" data-correct="c" data-explanation="In-context learning allows LLMs to perform tasks by providing examples in the prompt, without updating model weights. The model learns the pattern from examples and applies it.">
                    <p class="quiz-question">48. What is 'in-context learning' in large language models?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Training on domain-specific data</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Fine-tuning on new tasks</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Learning from examples provided in the prompt without weight updates</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Using retrieval to augment generation</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="Few-shot prompting provides a few examples of the task in the prompt before the actual query. Zero-shot provides no examples, just instructions. One-shot uses exactly one example.">
                    <p class="quiz-question">49. What distinguishes few-shot prompting from zero-shot prompting?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Few-shot includes examples in the prompt, zero-shot uses only instructions</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Few-shot trains on a small dataset</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Zero-shot is more accurate</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Few-shot requires model fine-tuning</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="Chain-of-thought prompting encourages the model to show its reasoning step-by-step before giving the final answer. This improves performance on complex reasoning tasks.">
                    <p class="quiz-question">50. [NOAI Style] What is chain-of-thought prompting?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Linking multiple prompts together</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Training on sequential data</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Using attention across prompt chains</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Prompting the model to show reasoning steps before answering</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="RLHF (Reinforcement Learning from Human Feedback) trains models using human preference data. Humans rank model outputs, and the model learns to generate preferred responses.">
                    <p class="quiz-question">51. What is RLHF used for in training LLMs like ChatGPT?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Reducing model size</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Aligning model outputs with human preferences</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Improving tokenization</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Training on more data</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="Instruction tuning fine-tunes models on diverse instruction-following examples, teaching the model to follow human instructions across many tasks, improving zero-shot capabilities.">
                    <p class="quiz-question">52. What is the purpose of instruction tuning?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Teaching models to generate instructions</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Reducing inference latency</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Training models to follow diverse human instructions</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Compressing model weights</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Evaluation and Metrics</h4>

                <div class="quiz-container" data-correct="a" data-explanation="ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measures overlap between generated and reference summaries, commonly ROUGE-1 (unigram), ROUGE-2 (bigram), and ROUGE-L (longest common subsequence).">
                    <p class="quiz-question">53. What does ROUGE measure in text summarization?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Overlap between generated summary and reference summaries</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Grammatical correctness of the summary</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Reading difficulty level</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Summary length appropriateness</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="F1 score for NER combines precision (what fraction of predicted entities are correct) and recall (what fraction of actual entities were found) into a single metric.">
                    <p class="quiz-question">54. [NOAI Style] Which metric is commonly used to evaluate NER models?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) BLEU score</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Perplexity</span></div>
                        <div class="quiz-option" data-value="c"><span>C) ROUGE score</span></div>
                        <div class="quiz-option" data-value="d"><span>D) F1 score (entity-level precision and recall)</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Practical NLP Considerations</h4>

                <div class="quiz-container" data-correct="b" data-explanation="Attention mask tells the model which tokens are real content vs. padding. Padded positions get masked out so they don't affect the model's computations.">
                    <p class="quiz-question">55. What is the purpose of attention masks in transformer inputs?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) To select which attention heads to use</span></div>
                        <div class="quiz-option" data-value="b"><span>B) To indicate which tokens are padding vs. real content</span></div>
                        <div class="quiz-option" data-value="c"><span>C) To mask sensitive information</span></div>
                        <div class="quiz-option" data-value="d"><span>D) To limit the context window size</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="Gradient accumulation simulates larger batch sizes by accumulating gradients over multiple forward passes before updating weights. This enables training with limited GPU memory.">
                    <p class="quiz-question">56. What is gradient accumulation used for when training large NLP models?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Speeding up training</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Reducing model size</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Simulating larger batch sizes with limited GPU memory</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Improving model accuracy</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="Mixed precision training uses FP16 (half precision) for most computations while keeping critical operations in FP32, reducing memory usage and speeding up training with minimal accuracy loss.">
                    <p class="quiz-question">57. What is the benefit of mixed precision training (FP16)?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Faster training and lower memory usage</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Higher model accuracy</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Better handling of rare words</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Improved tokenization</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="LoRA (Low-Rank Adaptation) adds small trainable rank decomposition matrices to frozen pre-trained weights, enabling efficient fine-tuning with far fewer parameters.">
                    <p class="quiz-question">58. [NOAI Style] What is LoRA (Low-Rank Adaptation) used for?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Pre-training language models</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Tokenization of long sequences</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Data augmentation</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Parameter-efficient fine-tuning of large models</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="Retrieval-Augmented Generation (RAG) retrieves relevant documents from a knowledge base and includes them in the prompt, allowing models to access up-to-date information beyond their training data.">
                    <p class="quiz-question">59. What is Retrieval-Augmented Generation (RAG)?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Generating training data through retrieval</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Combining retrieval from a knowledge base with text generation</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Using random sampling in generation</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Regenerating text until it matches criteria</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="Hallucination in LLMs refers to generating plausible-sounding but factually incorrect or made-up information. It's a major challenge in deploying LLMs for factual tasks.">
                    <p class="quiz-question">60. What is 'hallucination' in the context of LLMs?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) The model refusing to answer questions</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Extremely slow inference speed</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Generating plausible-sounding but factually incorrect information</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Producing outputs in the wrong language</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="algorithm-card">
                    <h3>NOAI Memory Aids - NLP</h3>
                    <ul>
                        <li><strong>BERT = Bidirectional:</strong> Sees context from BOTH sides (encoder-only, good for understanding)</li>
                        <li><strong>GPT = Generative:</strong> Sees only PAST tokens (decoder-only, good for generation)</li>
                        <li><strong>Attention = Spotlight:</strong> Shines light on relevant parts of input</li>
                        <li><strong>[CLS] = Summary:</strong> Aggregates sequence information for classification</li>
                        <li><strong>Causal Mask = No Cheating:</strong> Prevents looking at future tokens during training</li>
                        <li><strong>MLM = Fill in the Blank:</strong> BERT's pre-training task, predicts masked words</li>
                        <li><strong>Beam Search = Multiple Paths:</strong> Explores several sequences, picks best overall</li>
                        <li><strong>Skip-gram = Center->Context:</strong> Predicts surrounding words from center word</li>
                        <li><strong>CBOW = Context->Center:</strong> Predicts center word from surrounding context</li>
                        <li><strong>Positional Encoding = Order Matters:</strong> Tells Transformer where each token is in sequence</li>
                    </ul>
                </div>

                <div class="tip-box">
                    <strong>Common NOAI Traps:</strong>
                    <ul>
                        <li>BERT is NOT for text generation (it's bidirectional, can't generate left-to-right)</li>
                        <li>GPT uses causal masking, NOT random masking like BERT</li>
                        <li>Attention doesn't replace embeddings - they work together</li>
                        <li>Fine-tuning updates ALL weights, not just the classifier head</li>
                        <li>Transformers still need positional information - it's just added differently than RNNs</li>
                    </ul>
                </div>
            </div>
        </section>

        <div class="container">
            <nav class="page-navigation">
                <a href="computer-vision.html" class="page-nav-link prev">
                    <span class="page-nav-label">‚Üê Previous</span>
                    <span class="page-nav-title">Computer Vision</span>
                </a>
                <a href="../index.html" class="page-nav-link next">
                    <span class="page-nav-label">Back to ‚Üí</span>
                    <span class="page-nav-title">Home</span>
                </a>
            </nav>
        </div>
    </main>

    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-section">
                    <h4>ML for NOAI</h4>
                    <p>An educational resource for Singapore secondary school students preparing for the National Olympiad in AI.</p>
                </div>
                <div class="footer-section">
                    <h4>Quick Links</h4>
                    <ul>
                        <li><a href="fundamentals.html">ML Fundamentals</a></li>
                        <li><a href="supervised.html">Supervised Learning</a></li>
                        <li><a href="neural-networks.html">Neural Networks</a></li>
                    </ul>
                </div>
                <div class="footer-section">
                    <h4>References</h4>
                    <ul>
                        <li><a href="https://ioai-official.org/" target="_blank">IOAI Official</a></li>
                        <li><a href="https://aisingapore.org/" target="_blank">AI Singapore</a></li>
                    </ul>
                </div>
            </div>
            <div class="footer-bottom">
                <p>Educational content aligned with IOAI Syllabus. Not affiliated with AI Singapore or IOAI.</p>
            </div>
        </div>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>
