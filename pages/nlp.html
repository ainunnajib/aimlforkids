<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Natural Language Processing - ML for NOAI</title>
    <link rel="stylesheet" href="../css/style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Fira+Code&display=swap" rel="stylesheet">
</head>
<body>
    <nav class="navbar">
        <div class="nav-container">
            <a href="../index.html" class="nav-logo">
                <span class="logo-icon">ü§ñ</span>
                <span>ML for NOAI</span>
            </a>
            <button class="nav-toggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            <ul class="nav-menu">
                <li><a href="../index.html" class="nav-link">Home</a></li>
                <li><a href="fundamentals.html" class="nav-link">Fundamentals</a></li>
                <li><a href="supervised.html" class="nav-link">Supervised Learning</a></li>
                <li><a href="unsupervised.html" class="nav-link">Unsupervised Learning</a></li>
                <li><a href="neural-networks.html" class="nav-link">Neural Networks</a></li>
                <li><a href="computer-vision.html" class="nav-link">Computer Vision</a></li>
                <li><a href="nlp.html" class="nav-link active">NLP</a></li>
            </ul>
        </div>
    </nav>

    <div class="breadcrumb">
        <div class="container">
            <ul>
                <li><a href="../index.html">Home</a></li>
                <li>Natural Language Processing</li>
            </ul>
        </div>
    </div>

    <header class="page-header">
        <div class="container">
            <h1>üí¨ Natural Language Processing</h1>
            <p class="subtitle">Teaching machines to understand, interpret, and generate human language.</p>
        </div>
    </header>

    <div class="progress-container">
        <div class="container">
            <div class="progress-bar">
                <div class="progress-fill"></div>
            </div>
        </div>
    </div>

    <main>
        <section id="overview" class="content-section">
            <div class="container">
                <h2>Introduction to NLP</h2>
                <p>Natural Language Processing (NLP) enables computers to understand, interpret, and generate human language. It bridges the gap between human communication and machine understanding.</p>

                <div class="concept-grid">
                    <div class="concept-card">
                        <h4>üìù Text Classification</h4>
                        <p>Categorize text into predefined labels</p>
                        <p><em>Spam detection, sentiment analysis</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>üè∑Ô∏è Named Entity Recognition</h4>
                        <p>Identify entities like names, places, dates</p>
                        <p><em>"Apple Inc. is in California"</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>‚ùì Question Answering</h4>
                        <p>Extract answers from text given questions</p>
                        <p><em>Q: "Who founded Tesla?" A: "Elon Musk"</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>üîÑ Machine Translation</h4>
                        <p>Translate between languages</p>
                        <p><em>English ‚Üí Chinese</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>üìä Text Summarization</h4>
                        <p>Create concise summaries of documents</p>
                        <p><em>Long article ‚Üí Key points</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>‚úçÔ∏è Text Generation</h4>
                        <p>Generate human-like text</p>
                        <p><em>ChatGPT, story writing</em></p>
                    </div>
                </div>
            </div>
        </section>

        <section id="preprocessing" class="content-section">
            <div class="container">
                <h2>Text Preprocessing</h2>
                <p>Before feeding text to models, we need to clean and convert it to numerical form.</p>

                <h3>Tokenization</h3>
                <p>Breaking text into smaller units (tokens).</p>

                <div class="concept-grid">
                    <div class="concept-card">
                        <h4>Word Tokenization</h4>
                        <p>"I love NLP" ‚Üí ["I", "love", "NLP"]</p>
                        <p><em>Simple but has vocabulary issues</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>Subword Tokenization</h4>
                        <p>"unhappiness" ‚Üí ["un", "happiness"]</p>
                        <p><em>Handles rare words better (BPE, WordPiece)</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>Character Tokenization</h4>
                        <p>"hello" ‚Üí ["h", "e", "l", "l", "o"]</p>
                        <p><em>Small vocabulary, longer sequences</em></p>
                    </div>
                </div>

                <h3>Common Preprocessing Steps</h3>
                <ul>
                    <li><strong>Lowercasing:</strong> "Hello" ‚Üí "hello"</li>
                    <li><strong>Removing punctuation:</strong> "Hello!" ‚Üí "Hello"</li>
                    <li><strong>Removing stopwords:</strong> Remove common words (the, is, at)</li>
                    <li><strong>Stemming:</strong> "running" ‚Üí "run" (chop endings)</li>
                    <li><strong>Lemmatization:</strong> "better" ‚Üí "good" (use dictionary)</li>
                </ul>

                <pre><code># Python: Text Preprocessing
import re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer

text = "The cats are running quickly through the gardens!"

# Lowercase
text = text.lower()

# Remove punctuation
text = re.sub(r'[^\w\s]', '', text)

# Tokenize
tokens = word_tokenize(text)
# ['the', 'cats', 'are', 'running', 'quickly', 'through', 'the', 'gardens']

# Remove stopwords
stop_words = set(stopwords.words('english'))
tokens = [t for t in tokens if t not in stop_words]
# ['cats', 'running', 'quickly', 'gardens']

# Stemming
stemmer = PorterStemmer()
stems = [stemmer.stem(t) for t in tokens]
# ['cat', 'run', 'quick', 'garden']</code></pre>

                <div class="tip-box">
                    Modern deep learning models (like BERT) often work with raw text and learn their own representations. Heavy preprocessing may not be needed!
                </div>
            </div>
        </section>

        <section id="embeddings" class="content-section">
            <div class="container">
                <h2>Word Embeddings</h2>
                <p>Word embeddings convert words into dense vectors that capture semantic meaning.</p>

                <h3>Why Embeddings?</h3>
                <p>One-hot encoding (where "cat" = [1,0,0,...]) has problems:</p>
                <ul>
                    <li>Vectors are huge (vocabulary size)</li>
                    <li>All words are equally different (no similarity)</li>
                    <li>"cat" and "dog" should be closer than "cat" and "airplane"</li>
                </ul>

                <div class="algorithm-card">
                    <h3>Word2Vec</h3>
                    <div class="algorithm-meta">
                        <span>üìä Type: Embedding</span>
                        <span>üìö NOAI: Theory + Practice</span>
                    </div>

                    <p>Learn word vectors by predicting context. Two architectures:</p>

                    <h4>Skip-gram</h4>
                    <p>Given a word, predict surrounding words</p>
                    <p><em>"The [cat] sat on" ‚Üí predict "The", "sat", "on"</em></p>

                    <h4>CBOW (Continuous Bag of Words)</h4>
                    <p>Given surrounding words, predict the center word</p>
                    <p><em>"The __ sat on" ‚Üí predict "cat"</em></p>

                    <h4>Key Property</h4>
                    <div class="formula">
                        king - man + woman ‚âà queen
                    </div>
                    <p>Embeddings capture semantic relationships!</p>
                </div>

                <h3>Using Pre-trained Embeddings</h3>
                <pre><code># Python: Word Embeddings with Gensim
from gensim.models import KeyedVectors

# Load pre-trained Word2Vec (Google News)
model = KeyedVectors.load_word2vec_format('GoogleNews-vectors.bin', binary=True)

# Get word vector
vector = model['computer']  # 300-dimensional vector

# Find similar words
similar = model.most_similar('king', topn=5)
# [('kings', 0.71), ('queen', 0.65), ('monarch', 0.64), ...]

# Analogy: king - man + woman = ?
result = model.most_similar(positive=['king', 'woman'], negative=['man'])
# [('queen', 0.71), ...]</code></pre>
            </div>
        </section>

        <section id="rnn" class="content-section">
            <div class="container">
                <h2>Recurrent Neural Networks (RNNs)</h2>
                <p>RNNs process sequences by maintaining a "memory" (hidden state) that captures information from previous steps.</p>

                <div class="algorithm-card">
                    <h3>Basic RNN</h3>
                    <div class="algorithm-meta">
                        <span>üîÑ Type: Sequence Model</span>
                        <span>üìö NOAI: Theory</span>
                    </div>

                    <div class="formula">
                        <div class="formula-title">RNN Equations</div>
                        h‚Çú = tanh(W‚Çï‚Çï √ó h‚Çú‚Çã‚ÇÅ + W‚Çì‚Çï √ó x‚Çú + b)
                        y‚Çú = W‚Çï·µß √ó h‚Çú

                        h‚Çú = hidden state at time t
                        x‚Çú = input at time t
                    </div>

                    <h4>Problem: Vanishing Gradient</h4>
                    <p>For long sequences, gradients become very small, making it hard to learn long-range dependencies.</p>
                </div>

                <h3>LSTM (Long Short-Term Memory)</h3>
                <div class="algorithm-card">
                    <div class="algorithm-meta">
                        <span>üß† Type: Gated RNN</span>
                        <span>üìö NOAI: Practice</span>
                    </div>

                    <p>LSTM solves vanishing gradients with <strong>gates</strong> that control information flow:</p>

                    <ul>
                        <li><strong>Forget Gate:</strong> What to remove from memory</li>
                        <li><strong>Input Gate:</strong> What new information to store</li>
                        <li><strong>Output Gate:</strong> What to output</li>
                        <li><strong>Cell State:</strong> Long-term memory (can persist unchanged)</li>
                    </ul>
                </div>

                <pre><code># PyTorch: LSTM for Text Classification
import torch.nn as nn

class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True,
                           bidirectional=True)
        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # *2 for bidirectional

    def forward(self, x):
        # x shape: (batch, seq_len)
        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)
        lstm_out, (hidden, cell) = self.lstm(embedded)

        # Use final hidden state from both directions
        hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)
        output = self.fc(hidden)
        return output</code></pre>
            </div>
        </section>

        <section id="attention" class="content-section">
            <div class="container">
                <h2>Attention Mechanism</h2>
                <p>Attention allows models to focus on relevant parts of the input when producing output.</p>

                <div class="algorithm-card">
                    <h3>The Attention Idea</h3>
                    <div class="algorithm-meta">
                        <span>üéØ Type: Mechanism</span>
                        <span>üìö NOAI: Theory + Practice</span>
                    </div>

                    <h4>Problem with RNNs</h4>
                    <p>All information must pass through a fixed-size hidden state (bottleneck).</p>

                    <h4>Solution: Attention</h4>
                    <p>Look at ALL input positions and compute weighted importance for each.</p>

                    <div class="formula">
                        <div class="formula-title">Attention Computation</div>
                        1. Query (Q): what we're looking for
                        2. Keys (K): what we're searching through
                        3. Values (V): the actual content

                        Attention(Q, K, V) = softmax(QK·µÄ/‚àöd‚Çñ) √ó V
                    </div>
                </div>

                <h3>Self-Attention</h3>
                <p>When Q, K, V all come from the same sequence. Each word attends to all other words in the sentence.</p>
                <p><em>"The cat sat on the mat because it was tired"</em></p>
                <p>Self-attention helps "it" understand it refers to "cat".</p>
            </div>
        </section>

        <section id="transformers" class="content-section">
            <div class="container">
                <h2>Transformers</h2>
                <p>Transformers replaced RNNs as the dominant architecture for NLP. They use only attention‚Äîno recurrence!</p>

                <div class="algorithm-card">
                    <h3>Transformer Architecture</h3>
                    <div class="algorithm-meta">
                        <span>üèóÔ∏è Type: Architecture</span>
                        <span>üìö NOAI: Theory + Practice</span>
                        <span>üèÜ Foundation of Modern NLP</span>
                    </div>

                    <h4>Key Components</h4>
                    <ul>
                        <li><strong>Multi-Head Attention:</strong> Multiple attention layers in parallel</li>
                        <li><strong>Positional Encoding:</strong> Adds position information (since no recurrence)</li>
                        <li><strong>Feed-Forward Networks:</strong> Process attention outputs</li>
                        <li><strong>Layer Normalization:</strong> Stabilize training</li>
                        <li><strong>Residual Connections:</strong> Skip connections like ResNet</li>
                    </ul>

                    <h4>Advantages over RNNs</h4>
                    <ul>
                        <li><strong>Parallelization:</strong> Process all positions simultaneously</li>
                        <li><strong>Long-range dependencies:</strong> Direct connections between any positions</li>
                        <li><strong>Scalability:</strong> Train much larger models</li>
                    </ul>
                </div>

                <h3>Encoder vs Decoder</h3>
                <div class="concept-grid">
                    <div class="concept-card">
                        <h4>Encoder-only</h4>
                        <p>Understands input text</p>
                        <p><em>Example: BERT</em></p>
                        <p>Tasks: Classification, NER</p>
                    </div>
                    <div class="concept-card">
                        <h4>Decoder-only</h4>
                        <p>Generates text autoregressively</p>
                        <p><em>Example: GPT</em></p>
                        <p>Tasks: Text generation</p>
                    </div>
                    <div class="concept-card">
                        <h4>Encoder-Decoder</h4>
                        <p>Maps input to output sequence</p>
                        <p><em>Example: T5, BART</em></p>
                        <p>Tasks: Translation, summarization</p>
                    </div>
                </div>
            </div>
        </section>

        <section id="bert" class="content-section">
            <div class="container">
                <h2>BERT and Pre-trained Models</h2>
                <p>Pre-trained language models revolutionized NLP by learning general language understanding from massive text corpora.</p>

                <div class="algorithm-card">
                    <h3>BERT (Bidirectional Encoder Representations from Transformers)</h3>
                    <div class="algorithm-meta">
                        <span>ü§ñ Type: Pre-trained Model</span>
                        <span>üìö NOAI: Theory + Practice</span>
                    </div>

                    <h4>Pre-training Tasks</h4>
                    <ul>
                        <li><strong>Masked Language Model (MLM):</strong> Predict masked words
                            <p>"The [MASK] sat on the mat" ‚Üí predict "cat"</p>
                        </li>
                        <li><strong>Next Sentence Prediction:</strong> Are two sentences consecutive?</li>
                    </ul>

                    <h4>Why Bidirectional?</h4>
                    <p>Unlike GPT (left-to-right), BERT sees context from BOTH directions.</p>
                </div>

                <h3>Using BERT with Hugging Face</h3>
                <pre><code># Python: BERT with Hugging Face Transformers
from transformers import BertTokenizer, BertForSequenceClassification
from transformers import Trainer, TrainingArguments

# Load pre-trained BERT
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=2  # For binary classification
)

# Tokenize input
text = "This movie was amazing!"
inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)

# Get prediction
outputs = model(**inputs)
predictions = outputs.logits.argmax(dim=-1)

# Fine-tuning
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    learning_rate=2e-5,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)
trainer.train()</code></pre>

                <h3>Popular Pre-trained Models</h3>
                <table class="comparison-table">
                    <thead>
                        <tr>
                            <th>Model</th>
                            <th>Type</th>
                            <th>Best For</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>BERT</strong></td>
                            <td>Encoder</td>
                            <td>Classification, NER, QA</td>
                        </tr>
                        <tr>
                            <td><strong>RoBERTa</strong></td>
                            <td>Encoder</td>
                            <td>Same as BERT, better performance</td>
                        </tr>
                        <tr>
                            <td><strong>GPT-2/3/4</strong></td>
                            <td>Decoder</td>
                            <td>Text generation</td>
                        </tr>
                        <tr>
                            <td><strong>T5</strong></td>
                            <td>Encoder-Decoder</td>
                            <td>Any text-to-text task</td>
                        </tr>
                        <tr>
                            <td><strong>DistilBERT</strong></td>
                            <td>Encoder</td>
                            <td>Faster, smaller BERT</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </section>

        <section id="text-classification" class="content-section">
            <div class="container">
                <h2>Text Classification Pipeline</h2>
                <p>A complete workflow for text classification tasks like sentiment analysis.</p>

                <pre><code># Complete Text Classification with BERT
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
from sklearn.model_selection import train_test_split

# 1. Custom Dataset
class TextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]

        encoding = self.tokenizer(
            text,
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'label': torch.tensor(label)
        }

# 2. Setup
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# 3. Prepare data
train_texts, val_texts, train_labels, val_labels = train_test_split(
    texts, labels, test_size=0.2
)

train_dataset = TextDataset(train_texts, train_labels, tokenizer)
val_dataset = TextDataset(val_texts, val_labels, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16)

# 4. Training loop
optimizer = AdamW(model.parameters(), lr=2e-5)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

for epoch in range(3):
    model.train()
    for batch in train_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print(f"Epoch {epoch+1} completed")</code></pre>
            </div>
        </section>

        <section id="language-modeling" class="content-section">
            <div class="container">
                <h2>Language Modeling</h2>
                <p>Language models predict the probability of text sequences. They're the foundation of modern NLP.</p>

                <div class="concept-grid">
                    <div class="concept-card">
                        <h4>Autoregressive LM</h4>
                        <p>Predict next word given previous words</p>
                        <p>P(w‚Çú | w‚ÇÅ, w‚ÇÇ, ..., w‚Çú‚Çã‚ÇÅ)</p>
                        <p><em>Examples: GPT series</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>Masked LM</h4>
                        <p>Predict masked words given context</p>
                        <p>P(w‚Çò‚Çê‚Çõ‚Çñ‚Çë‚Çê | context)</p>
                        <p><em>Examples: BERT</em></p>
                    </div>
                </div>

                <h3>Perplexity</h3>
                <p>The standard metric for language models. Lower is better.</p>
                <div class="formula">
                    <div class="formula-title">Perplexity</div>
                    PPL = exp(average negative log-likelihood)

                    PPL = 10 means model is "confused" between 10 choices on average
                </div>
            </div>
        </section>

        <section id="quiz" class="content-section">
            <div class="container">
                <h2>Test Your Understanding</h2>

                <div class="quiz-container" data-correct="c" data-explanation="Word embeddings capture semantic meaning, placing similar words closer together in vector space, unlike one-hot encoding where all words are equally distant.">
                    <p class="quiz-question">1. Why are word embeddings better than one-hot encoding?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) They use less memory</span></div>
                        <div class="quiz-option" data-value="b"><span>B) They're faster to compute</span></div>
                        <div class="quiz-option" data-value="c"><span>C) They capture semantic similarity between words</span></div>
                        <div class="quiz-option" data-value="d"><span>D) They work with any language</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="Transformers use positional encodings to add position information since, unlike RNNs, they process all positions in parallel and have no inherent notion of order.">
                    <p class="quiz-question">2. How do Transformers know the order of words without recurrence?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) They don't need order information</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Positional encodings</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Attention weights</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Layer normalization</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="BERT uses Masked Language Modeling (MLM) where random words are masked and the model learns to predict them using bidirectional context.">
                    <p class="quiz-question">3. What pre-training task does BERT primarily use?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Masked Language Modeling</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Next word prediction</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Translation</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Sentiment classification</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>
            </div>
        </section>

        <div class="container">
            <nav class="page-navigation">
                <a href="computer-vision.html" class="page-nav-link prev">
                    <span class="page-nav-label">‚Üê Previous</span>
                    <span class="page-nav-title">Computer Vision</span>
                </a>
                <a href="../index.html" class="page-nav-link next">
                    <span class="page-nav-label">Back to ‚Üí</span>
                    <span class="page-nav-title">Home</span>
                </a>
            </nav>
        </div>
    </main>

    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-section">
                    <h4>ML for NOAI</h4>
                    <p>An educational resource for Singapore secondary school students preparing for the National Olympiad in AI.</p>
                </div>
                <div class="footer-section">
                    <h4>Quick Links</h4>
                    <ul>
                        <li><a href="fundamentals.html">ML Fundamentals</a></li>
                        <li><a href="supervised.html">Supervised Learning</a></li>
                        <li><a href="neural-networks.html">Neural Networks</a></li>
                    </ul>
                </div>
                <div class="footer-section">
                    <h4>References</h4>
                    <ul>
                        <li><a href="https://ioai-official.org/" target="_blank">IOAI Official</a></li>
                        <li><a href="https://aisingapore.org/" target="_blank">AI Singapore</a></li>
                    </ul>
                </div>
            </div>
            <div class="footer-bottom">
                <p>Educational content aligned with IOAI Syllabus. Not affiliated with AI Singapore or IOAI.</p>
            </div>
        </div>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>
