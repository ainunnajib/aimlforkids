<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Computer Vision - ML for NOAI</title>
    <link rel="stylesheet" href="../css/style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Fira+Code&display=swap" rel="stylesheet">
</head>
<body>
    <nav class="navbar">
        <div class="nav-container">
            <a href="../index.html" class="nav-logo">
                <span class="logo-icon">ü§ñ</span>
                <span>ML for NOAI</span>
            </a>
            <button class="nav-toggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            <ul class="nav-menu">
                <li><a href="../index.html" class="nav-link">Home</a></li>
                <li><a href="fundamentals.html" class="nav-link">Fundamentals</a></li>
                <li><a href="supervised.html" class="nav-link">Supervised Learning</a></li>
                <li><a href="unsupervised.html" class="nav-link">Unsupervised Learning</a></li>
                <li><a href="neural-networks.html" class="nav-link">Neural Networks</a></li>
                <li><a href="computer-vision.html" class="nav-link active">Computer Vision</a></li>
                <li><a href="nlp.html" class="nav-link">NLP</a></li>
            </ul>
        </div>
    </nav>

    <div class="breadcrumb">
        <div class="container">
            <ul>
                <li><a href="../index.html">Home</a></li>
                <li>Computer Vision</li>
            </ul>
        </div>
    </div>

    <header class="page-header">
        <div class="container">
            <h1>üëÅÔ∏è Computer Vision</h1>
            <p class="subtitle">Teaching machines to see and understand visual information using deep learning.</p>
        </div>
    </header>

    <div class="progress-container">
        <div class="container">
            <div class="progress-bar">
                <div class="progress-fill"></div>
            </div>
        </div>
    </div>

    <main>
        <section id="overview" class="content-section">
            <div class="container">
                <h2>Introduction to Computer Vision</h2>
                <p>Computer Vision (CV) enables machines to interpret and understand visual information from images and videos. It's one of the most successful applications of deep learning.</p>

                <div class="concept-grid">
                    <div class="concept-card">
                        <h4>üè∑Ô∏è Image Classification</h4>
                        <p>Assign a label to an entire image</p>
                        <p><em>"This image contains a cat"</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>üìç Object Detection</h4>
                        <p>Find and locate objects with bounding boxes</p>
                        <p><em>"Cat at position (x, y, w, h)"</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>üé® Semantic Segmentation</h4>
                        <p>Classify each pixel in the image</p>
                        <p><em>Create a pixel-wise mask</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>üî¢ Instance Segmentation</h4>
                        <p>Separate different instances of same class</p>
                        <p><em>Distinguish cat #1 from cat #2</em></p>
                    </div>
                </div>

                <h3>Images as Data</h3>
                <p>To a computer, an image is a 3D array of numbers:</p>
                <ul>
                    <li><strong>Height √ó Width √ó Channels</strong></li>
                    <li>Grayscale: 1 channel (0-255 intensity)</li>
                    <li>Color (RGB): 3 channels (Red, Green, Blue)</li>
                    <li>Example: 224 √ó 224 √ó 3 = 150,528 values!</li>
                </ul>
            </div>
        </section>

        <section id="cnn" class="content-section">
            <div class="container">
                <h2>Convolutional Neural Networks (CNNs)</h2>
                <p>CNNs are the foundation of modern computer vision. They're designed specifically to process grid-like data (images).</p>

                <div class="algorithm-card">
                    <h3>Why CNNs for Images?</h3>
                    <div class="algorithm-meta">
                        <span>üëÅÔ∏è Type: Deep Learning</span>
                        <span>üìö NOAI: Theory + Practice</span>
                    </div>

                    <h4>Problems with MLPs for Images</h4>
                    <ul>
                        <li>Too many parameters (224√ó224√ó3 = 150K inputs!)</li>
                        <li>No spatial awareness (treats pixels independently)</li>
                        <li>Not translation invariant (cat in corner ‚â† cat in center)</li>
                    </ul>

                    <h4>CNN Solutions</h4>
                    <ul>
                        <li><strong>Local connectivity:</strong> Each neuron only "sees" a small region</li>
                        <li><strong>Weight sharing:</strong> Same filter applied across entire image</li>
                        <li><strong>Hierarchical features:</strong> Early layers detect edges, later layers detect objects</li>
                    </ul>
                </div>

                <h3>Key CNN Components</h3>

                <div class="concept-grid">
                    <div class="concept-card">
                        <h4>Convolutional Layer</h4>
                        <p>Applies learnable filters (kernels) that slide across the image to detect features.</p>
                        <p><em>Detects edges, textures, patterns</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>Pooling Layer</h4>
                        <p>Downsamples feature maps to reduce size and computation.</p>
                        <p><em>Max pooling: take maximum value in region</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>Fully Connected Layer</h4>
                        <p>Traditional neural network layers at the end for classification.</p>
                        <p><em>Combines features for final prediction</em></p>
                    </div>
                </div>

                <h3>Convolution Operation</h3>
                <div class="algorithm-card">
                    <h4>How Convolution Works</h4>
                    <ol>
                        <li>Slide a small filter (e.g., 3√ó3) across the image</li>
                        <li>At each position, compute element-wise multiplication</li>
                        <li>Sum the results to get one output value</li>
                        <li>Move filter by stride and repeat</li>
                    </ol>

                    <h4>Key Parameters</h4>
                    <ul>
                        <li><strong>Kernel size:</strong> Filter dimensions (common: 3√ó3, 5√ó5)</li>
                        <li><strong>Stride:</strong> Step size when sliding (1 = overlap, 2 = skip)</li>
                        <li><strong>Padding:</strong> Add zeros around edges to preserve size</li>
                        <li><strong>Filters:</strong> Number of different patterns to detect</li>
                    </ul>

                    <div class="formula">
                        <div class="formula-title">Output Size Formula</div>
                        Output = (Input - Kernel + 2√óPadding) / Stride + 1
                    </div>
                </div>

                <pre><code># PyTorch: CNN Building Blocks
import torch.nn as nn

# Convolutional layer
conv = nn.Conv2d(
    in_channels=3,      # RGB input
    out_channels=64,    # 64 filters
    kernel_size=3,      # 3x3 filter
    stride=1,
    padding=1           # Same padding
)

# Max pooling
pool = nn.MaxPool2d(kernel_size=2, stride=2)  # Halves dimensions

# Example: Input (3, 224, 224) ‚Üí Conv ‚Üí (64, 224, 224) ‚Üí Pool ‚Üí (64, 112, 112)</code></pre>
            </div>
        </section>

        <section id="architectures" class="content-section">
            <div class="container">
                <h2>Famous CNN Architectures</h2>
                <p>Understanding classic architectures helps you design better models.</p>

                <div class="algorithm-card">
                    <h3>LeNet-5 (1998)</h3>
                    <p>The original CNN for handwritten digit recognition.</p>
                    <p><strong>Architecture:</strong> Conv ‚Üí Pool ‚Üí Conv ‚Üí Pool ‚Üí FC ‚Üí FC ‚Üí Output</p>
                </div>

                <div class="algorithm-card">
                    <h3>VGGNet (2014)</h3>
                    <div class="algorithm-meta">
                        <span>üìä Depth: 16-19 layers</span>
                        <span>üéØ Simple design</span>
                    </div>
                    <p>Key insight: <strong>Stack many 3√ó3 convolutions</strong> instead of large filters.</p>
                    <p>Two 3√ó3 convs = one 5√ó5 conv (same receptive field, fewer parameters)</p>
                </div>

                <div class="algorithm-card">
                    <h3>ResNet (2015)</h3>
                    <div class="algorithm-meta">
                        <span>üìä Depth: 18-152+ layers</span>
                        <span>üìö NOAI: Practice</span>
                        <span>üèÜ Very Important</span>
                    </div>

                    <h4>The Problem</h4>
                    <p>Deeper networks should be better, but they become harder to train (vanishing gradients).</p>

                    <h4>The Solution: Skip Connections</h4>
                    <p>Add the input directly to the output of a layer block:</p>
                    <div class="formula">
                        output = F(x) + x
                    </div>
                    <p>This allows gradients to flow directly backward and enables training of very deep networks.</p>
                </div>

                <pre><code># PyTorch: ResNet Block (Simplified)
class ResidualBlock(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(channels)

    def forward(self, x):
        identity = x  # Save input

        out = self.conv1(x)
        out = self.bn1(out)
        out = torch.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        out = out + identity  # Skip connection!
        out = torch.relu(out)
        return out</code></pre>

                <h3>Using Pre-trained Models</h3>
                <p>Instead of training from scratch, use models pre-trained on ImageNet (1M+ images):</p>

                <pre><code># PyTorch: Using Pre-trained ResNet
import torchvision.models as models

# Load pre-trained ResNet-18
model = models.resnet18(pretrained=True)

# Replace final layer for your task (e.g., 10 classes)
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, 10)

# Option 1: Fine-tune entire network
# Option 2: Freeze early layers, only train final layers
for param in model.parameters():
    param.requires_grad = False  # Freeze all
model.fc.requires_grad = True    # Unfreeze final layer</code></pre>
            </div>
        </section>

        <section id="augmentation" class="content-section">
            <div class="container">
                <h2>Data Augmentation</h2>
                <p>Artificially expand your training data by applying transformations. This improves generalization and reduces overfitting.</p>

                <div class="concept-grid">
                    <div class="concept-card">
                        <h4>Geometric Transforms</h4>
                        <ul>
                            <li>Horizontal/vertical flip</li>
                            <li>Rotation</li>
                            <li>Scaling/cropping</li>
                            <li>Translation</li>
                        </ul>
                    </div>
                    <div class="concept-card">
                        <h4>Color Transforms</h4>
                        <ul>
                            <li>Brightness adjustment</li>
                            <li>Contrast changes</li>
                            <li>Saturation</li>
                            <li>Color jittering</li>
                        </ul>
                    </div>
                    <div class="concept-card">
                        <h4>Other Techniques</h4>
                        <ul>
                            <li>Gaussian noise</li>
                            <li>Cutout (random erasing)</li>
                            <li>Mixup (blend images)</li>
                            <li>CutMix</li>
                        </ul>
                    </div>
                </div>

                <pre><code># PyTorch: Data Augmentation
from torchvision import transforms

# Training transforms (with augmentation)
train_transform = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225])
])

# Validation transforms (no augmentation!)
val_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225])
])</code></pre>

                <div class="warning-box">
                    Only apply augmentation to training data, not validation or test data! Test data should reflect real-world conditions.
                </div>
            </div>
        </section>

        <section id="transfer-learning" class="content-section">
            <div class="container">
                <h2>Transfer Learning</h2>
                <p>Use knowledge from a model trained on one task to help with a different but related task.</p>

                <div class="algorithm-card">
                    <h3>Why Transfer Learning?</h3>
                    <ul>
                        <li>Don't need massive datasets (ImageNet has 1M+ images)</li>
                        <li>Faster training (start with good features)</li>
                        <li>Better performance with limited data</li>
                        <li>Lower layers learn general features (edges, textures)</li>
                    </ul>

                    <h4>Two Approaches</h4>
                    <ol>
                        <li><strong>Feature Extraction:</strong> Freeze pre-trained layers, only train new final layer(s)</li>
                        <li><strong>Fine-tuning:</strong> Train entire network with small learning rate</li>
                    </ol>
                </div>

                <h3>When to Use What</h3>
                <table class="comparison-table">
                    <thead>
                        <tr>
                            <th>Your Data</th>
                            <th>Similarity to Pre-training Data</th>
                            <th>Strategy</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Small</td>
                            <td>Similar</td>
                            <td>Feature extraction (freeze most layers)</td>
                        </tr>
                        <tr>
                            <td>Small</td>
                            <td>Different</td>
                            <td>Feature extraction from earlier layers</td>
                        </tr>
                        <tr>
                            <td>Large</td>
                            <td>Similar</td>
                            <td>Fine-tune entire network</td>
                        </tr>
                        <tr>
                            <td>Large</td>
                            <td>Different</td>
                            <td>Fine-tune or train from scratch</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </section>

        <section id="object-detection" class="content-section">
            <div class="container">
                <h2>Object Detection</h2>
                <p>Detect and localize multiple objects in an image with bounding boxes.</p>

                <div class="concept-grid">
                    <div class="concept-card">
                        <h4>Two-Stage Detectors</h4>
                        <p><strong>R-CNN Family:</strong> First propose regions, then classify each.</p>
                        <ul>
                            <li>R-CNN</li>
                            <li>Fast R-CNN</li>
                            <li>Faster R-CNN</li>
                        </ul>
                        <p><em>More accurate, slower</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>One-Stage Detectors</h4>
                        <p><strong>Direct prediction:</strong> Predict boxes and classes in one pass.</p>
                        <ul>
                            <li>YOLO (You Only Look Once)</li>
                            <li>SSD</li>
                            <li>RetinaNet</li>
                        </ul>
                        <p><em>Faster, good for real-time</em></p>
                    </div>
                </div>

                <h3>Evaluation Metrics</h3>
                <ul>
                    <li><strong>IoU (Intersection over Union):</strong> Overlap between predicted and ground truth boxes</li>
                    <li><strong>mAP (mean Average Precision):</strong> Average precision across all classes and IoU thresholds</li>
                </ul>

                <div class="formula">
                    <div class="formula-title">IoU (Intersection over Union)</div>
                    IoU = Area of Overlap / Area of Union

                    IoU > 0.5 typically considered a "match"
                </div>
            </div>
        </section>

        <section id="advanced" class="content-section">
            <div class="container">
                <h2>Advanced Topics</h2>

                <h3>GANs (Generative Adversarial Networks)</h3>
                <div class="algorithm-card">
                    <div class="algorithm-meta">
                        <span>üé® Type: Generative Model</span>
                        <span>üìö NOAI: Practice</span>
                    </div>
                    <p>Two networks compete: a <strong>Generator</strong> creates fake images, a <strong>Discriminator</strong> tries to distinguish real from fake.</p>
                    <h4>Applications</h4>
                    <ul>
                        <li>Image generation</li>
                        <li>Style transfer</li>
                        <li>Super-resolution</li>
                        <li>Data augmentation</li>
                    </ul>
                </div>

                <h3>Vision Transformers (ViT)</h3>
                <p>Apply transformer architecture (from NLP) to images by treating image patches as "tokens".</p>

                <h3>CLIP (Contrastive Language-Image Pre-training)</h3>
                <div class="algorithm-card">
                    <div class="algorithm-meta">
                        <span>üîó Type: Vision-Language Model</span>
                        <span>üìö NOAI: Practice</span>
                    </div>
                    <p>Learns to match images with text descriptions. Enables:</p>
                    <ul>
                        <li>Zero-shot classification (classify without training examples)</li>
                        <li>Image search with natural language</li>
                        <li>Cross-modal understanding</li>
                    </ul>
                </div>

                <h3>Diffusion Models</h3>
                <p>State-of-the-art image generation. Learn to gradually denoise images, enabling high-quality generation.</p>
                <p><em>Used in: DALL-E, Stable Diffusion, Midjourney</em></p>
            </div>
        </section>

        <section id="quiz" class="content-section">
            <div class="container">
                <h2>NOAI Practice Questions - Computer Vision</h2>

                <div class="tip-box">
                    <strong>NOAI Exam Tips:</strong> Computer vision questions often test your understanding of CNN components, architectures, and object detection methods. Remember: pooling reduces dimensions, skip connections enable deep networks, and YOLO is a single-stage detector!
                </div>

                <h3>CNN Fundamentals</h3>

                <div class="quiz-container" data-correct="b" data-explanation="CNNs use weight sharing (same filter applied across entire image) and local connectivity, dramatically reducing parameters compared to fully connected networks.">
                    <p class="quiz-question">1. Why do CNNs have fewer parameters than MLPs for image tasks?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) They use smaller images</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Weight sharing and local connectivity</span></div>
                        <div class="quiz-option" data-value="c"><span>C) They have fewer layers</span></div>
                        <div class="quiz-option" data-value="d"><span>D) They don't use activation functions</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="Stride controls how much the filter moves at each step. A stride of 2 means the filter moves 2 pixels at a time, reducing output dimensions by half.">
                    <p class="quiz-question">2. In a convolutional layer, what does the 'stride' parameter control?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) How many pixels the filter moves at each step</span></div>
                        <div class="quiz-option" data-value="b"><span>B) The size of the convolutional filter</span></div>
                        <div class="quiz-option" data-value="c"><span>C) The number of output channels</span></div>
                        <div class="quiz-option" data-value="d"><span>D) The activation function used</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="Padding adds zeros around the input borders. 'Same' padding preserves spatial dimensions, while 'valid' (no padding) reduces output size.">
                    <p class="quiz-question">3. What is the purpose of padding in CNNs?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) To increase the number of parameters</span></div>
                        <div class="quiz-option" data-value="b"><span>B) To speed up training</span></div>
                        <div class="quiz-option" data-value="c"><span>C) To preserve spatial dimensions and process border pixels</span></div>
                        <div class="quiz-option" data-value="d"><span>D) To reduce overfitting</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="The number of output channels equals the number of filters (kernels) used. Each filter learns to detect a different feature pattern.">
                    <p class="quiz-question">4. What determines the number of output channels in a convolutional layer?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) The kernel size</span></div>
                        <div class="quiz-option" data-value="b"><span>B) The stride value</span></div>
                        <div class="quiz-option" data-value="c"><span>C) The input image dimensions</span></div>
                        <div class="quiz-option" data-value="d"><span>D) The number of filters used</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h3>Pooling Layers</h3>

                <div class="quiz-container" data-correct="b" data-explanation="NOAI 2025 Style: Pooling layers reduce the spatial dimensions (height and width) of feature maps, decreasing computation and providing translation invariance.">
                    <p class="quiz-question">5. [NOAI Style] What is the primary purpose of pooling layers in CNNs?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) To increase the depth of feature maps</span></div>
                        <div class="quiz-option" data-value="b"><span>B) To reduce spatial dimensions of feature maps</span></div>
                        <div class="quiz-option" data-value="c"><span>C) To add non-linearity to the network</span></div>
                        <div class="quiz-option" data-value="d"><span>D) To normalize the input data</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="Max pooling takes the maximum value in each pooling window, while average pooling computes the mean. Max pooling is more common as it preserves prominent features.">
                    <p class="quiz-question">6. How does max pooling differ from average pooling?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Max pooling takes the highest value, average pooling computes the mean</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Max pooling is faster to compute</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Average pooling produces larger output dimensions</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Max pooling requires more memory</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h3>Famous Architectures</h3>

                <div class="quiz-container" data-correct="c" data-explanation="ResNet introduced skip (residual) connections that allow gradients to flow directly through the network, enabling training of very deep networks (100+ layers).">
                    <p class="quiz-question">7. What innovation did ResNet introduce that enabled training very deep networks?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Batch normalization</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Max pooling</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Skip (residual) connections</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Dropout regularization</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="VGGNet showed that using many small 3x3 filters stacked together is more effective than using larger filters. Two 3x3 convolutions have the same receptive field as one 5x5 but with fewer parameters.">
                    <p class="quiz-question">8. What key insight did VGGNet demonstrate about filter sizes?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Larger filters (7x7) work better for all tasks</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Stacking small 3x3 filters is more effective than large filters</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Filter size doesn't matter for accuracy</span></div>
                        <div class="quiz-option" data-value="d"><span>D) 1x1 filters are sufficient for classification</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="AlexNet (2012) was the breakthrough CNN that won ImageNet with a large margin, popularizing deep learning for computer vision. It used ReLU, dropout, and GPU training.">
                    <p class="quiz-question">9. Which architecture marked the breakthrough moment for deep learning in computer vision by winning ImageNet 2012?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) AlexNet</span></div>
                        <div class="quiz-option" data-value="b"><span>B) VGGNet</span></div>
                        <div class="quiz-option" data-value="c"><span>C) ResNet</span></div>
                        <div class="quiz-option" data-value="d"><span>D) LeNet</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="NOAI 2025 Style: Depthwise separable convolutions split the standard convolution into depthwise and pointwise operations, significantly reducing parameters and computational cost while maintaining similar accuracy.">
                    <p class="quiz-question">10. [NOAI Style] What is the main advantage of depthwise separable convolutions used in MobileNet?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) They increase model accuracy significantly</span></div>
                        <div class="quiz-option" data-value="b"><span>B) They allow processing of larger images</span></div>
                        <div class="quiz-option" data-value="c"><span>C) They reduce parameters and computational cost</span></div>
                        <div class="quiz-option" data-value="d"><span>D) They eliminate the need for pooling layers</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h3>Transfer Learning & Data Augmentation</h3>

                <div class="quiz-container" data-correct="a" data-explanation="Data augmentation should only be applied to training data. Validation and test data should represent real-world conditions without artificial modifications.">
                    <p class="quiz-question">11. When should you apply data augmentation?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Only during training</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Only during testing</span></div>
                        <div class="quiz-option" data-value="c"><span>C) During both training and testing</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Never - it hurts performance</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="With a small dataset similar to ImageNet, feature extraction (freezing pre-trained layers) works best. Fine-tuning all layers risks overfitting on small data.">
                    <p class="quiz-question">12. You have a small dataset of dog breeds (similar to ImageNet). What transfer learning strategy is most appropriate?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Train from scratch</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Feature extraction - freeze most pre-trained layers</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Fine-tune all layers with high learning rate</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Only use the first few layers</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="Horizontal flip is not appropriate for digit recognition because flipped digits represent different numbers (e.g., 6 becomes 9). Rotation and noise are acceptable augmentations.">
                    <p class="quiz-question">13. Which augmentation technique would be INAPPROPRIATE for a digit recognition task?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Small rotation</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Slight scaling</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Adding Gaussian noise</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Horizontal flip</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h3>Object Detection</h3>

                <div class="quiz-container" data-correct="a" data-explanation="NOAI 2025 Style: YOLO (You Only Look Once) is a single-stage detector that combines region proposal and classification into one unified network pass, making it fast for real-time detection.">
                    <p class="quiz-question">14. [NOAI Style] What distinguishes YOLO from two-stage detectors like Faster R-CNN?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) YOLO combines region proposal and classification in one step</span></div>
                        <div class="quiz-option" data-value="b"><span>B) YOLO uses larger input images</span></div>
                        <div class="quiz-option" data-value="c"><span>C) YOLO has higher accuracy</span></div>
                        <div class="quiz-option" data-value="d"><span>D) YOLO requires more training data</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="NOAI 2025 Style: The Region Proposal Network (RPN) in Faster R-CNN suggests candidate object regions (bounding box proposals) in the feature map, which are then classified by the detection head.">
                    <p class="quiz-question">15. [NOAI Style] What is the role of the Region Proposal Network (RPN) in Faster R-CNN?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Classify objects into categories</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Extract feature maps from input images</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Suggest candidate object regions in the feature map</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Perform non-maximum suppression</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="IoU (Intersection over Union) measures the overlap between predicted and ground truth bounding boxes. IoU = Area of Overlap / Area of Union. Higher is better, with 0.5+ typically considered a match.">
                    <p class="quiz-question">16. What does IoU (Intersection over Union) measure in object detection?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) The confidence score of predictions</span></div>
                        <div class="quiz-option" data-value="b"><span>B) The overlap between predicted and ground truth boxes</span></div>
                        <div class="quiz-option" data-value="c"><span>C) The number of detected objects</span></div>
                        <div class="quiz-option" data-value="d"><span>D) The processing speed of the model</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h3>Segmentation Tasks</h3>

                <div class="quiz-container" data-correct="d" data-explanation="NOAI 2025 Style: Mask R-CNN extends Faster R-CNN by adding a mask prediction branch, enabling instance segmentation - detecting objects AND generating pixel-level masks for each instance.">
                    <p class="quiz-question">17. [NOAI Style] Which architecture is specifically designed for instance segmentation?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) VGGNet</span></div>
                        <div class="quiz-option" data-value="b"><span>B) YOLO</span></div>
                        <div class="quiz-option" data-value="c"><span>C) U-Net</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Mask R-CNN</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="Semantic segmentation classifies each pixel but doesn't distinguish between instances of the same class. Instance segmentation identifies separate objects (cat #1 vs cat #2).">
                    <p class="quiz-question">18. What is the key difference between semantic segmentation and instance segmentation?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Semantic segmentation is faster</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Instance segmentation only works on single objects</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Instance segmentation distinguishes different objects of the same class</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Semantic segmentation uses bounding boxes</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h3>GANs and Advanced Topics</h3>

                <div class="quiz-container" data-correct="a" data-explanation="In GANs, the generator creates fake images trying to fool the discriminator, while the discriminator tries to distinguish real images from generated fakes. They compete in a minimax game.">
                    <p class="quiz-question">19. In a GAN, what is the relationship between the generator and discriminator?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Generator creates fakes, discriminator distinguishes real from fake</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Both networks classify images</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Discriminator generates images, generator evaluates quality</span></div>
                        <div class="quiz-option" data-value="d"><span>D) They work together to segment images</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="Vision Transformers (ViT) treat image patches as tokens (like words in NLP), applying self-attention across patches. This allows capturing global relationships from the first layer.">
                    <p class="quiz-question">20. How do Vision Transformers (ViT) process images?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Using convolutional filters exclusively</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Treating image patches as tokens and applying self-attention</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Processing one pixel at a time sequentially</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Using recurrent connections like RNNs</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="algorithm-card">
                    <h3>NOAI Memory Aids - Computer Vision</h3>
                    <ul>
                        <li><strong>Pooling = Shrinking:</strong> Reduces spatial dimensions, provides translation invariance</li>
                        <li><strong>YOLO = One Look:</strong> Single-stage detector, fast but may sacrifice some accuracy</li>
                        <li><strong>R-CNN Family = Two-Stage:</strong> Propose regions first, then classify (more accurate, slower)</li>
                        <li><strong>ResNet = Highway:</strong> Skip connections let gradients flow like a highway</li>
                        <li><strong>IoU = Overlap Score:</strong> Area of intersection / Area of union (0 to 1)</li>
                        <li><strong>Mask R-CNN = Faster R-CNN + Masks:</strong> Adds pixel-level segmentation to detection</li>
                        <li><strong>Depthwise Separable = Split & Save:</strong> Separate spatial and channel processing for efficiency</li>
                    </ul>
                </div>
            </div>
        </section>

        <div class="container">
            <nav class="page-navigation">
                <a href="neural-networks.html" class="page-nav-link prev">
                    <span class="page-nav-label">‚Üê Previous</span>
                    <span class="page-nav-title">Neural Networks</span>
                </a>
                <a href="nlp.html" class="page-nav-link next">
                    <span class="page-nav-label">Next ‚Üí</span>
                    <span class="page-nav-title">Natural Language Processing</span>
                </a>
            </nav>
        </div>
    </main>

    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-section">
                    <h4>ML for NOAI</h4>
                    <p>An educational resource for Singapore secondary school students preparing for the National Olympiad in AI.</p>
                </div>
                <div class="footer-section">
                    <h4>Quick Links</h4>
                    <ul>
                        <li><a href="fundamentals.html">ML Fundamentals</a></li>
                        <li><a href="supervised.html">Supervised Learning</a></li>
                        <li><a href="neural-networks.html">Neural Networks</a></li>
                    </ul>
                </div>
                <div class="footer-section">
                    <h4>References</h4>
                    <ul>
                        <li><a href="https://ioai-official.org/" target="_blank">IOAI Official</a></li>
                        <li><a href="https://aisingapore.org/" target="_blank">AI Singapore</a></li>
                    </ul>
                </div>
            </div>
            <div class="footer-bottom">
                <p>Educational content aligned with IOAI Syllabus. Not affiliated with AI Singapore or IOAI.</p>
            </div>
        </div>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>
