<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Computer Vision - ML for NOAI</title>
    <link rel="stylesheet" href="../css/style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Fira+Code&display=swap" rel="stylesheet">
</head>
<body>
    <nav class="navbar">
        <div class="nav-container">
            <a href="../index.html" class="nav-logo">
                <span class="logo-icon">ü§ñ</span>
                <span>ML for NOAI</span>
            </a>
            <button class="nav-toggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            <ul class="nav-menu">
                <li><a href="../index.html" class="nav-link">Home</a></li>
                <li><a href="fundamentals.html" class="nav-link">Fundamentals</a></li>
                <li><a href="supervised.html" class="nav-link">Supervised Learning</a></li>
                <li><a href="unsupervised.html" class="nav-link">Unsupervised Learning</a></li>
                <li><a href="neural-networks.html" class="nav-link">Neural Networks</a></li>
                <li><a href="computer-vision.html" class="nav-link active">Computer Vision</a></li>
                <li><a href="nlp.html" class="nav-link">NLP</a></li>
            </ul>
        </div>
    </nav>

    <div class="breadcrumb">
        <div class="container">
            <ul>
                <li><a href="../index.html">Home</a></li>
                <li>Computer Vision</li>
            </ul>
        </div>
    </div>

    <header class="page-header">
        <div class="container">
            <h1>üëÅÔ∏è Computer Vision</h1>
            <p class="subtitle">Teaching machines to see and understand visual information using deep learning.</p>
        </div>
    </header>

    <div class="progress-container">
        <div class="container">
            <div class="progress-bar">
                <div class="progress-fill"></div>
            </div>
        </div>
    </div>

    <main>
        <section id="overview" class="content-section">
            <div class="container">
                <h2>Introduction to Computer Vision</h2>
                <p>Computer Vision (CV) enables machines to interpret and understand visual information from images and videos. It's one of the most successful applications of deep learning.</p>

                <div class="concept-grid">
                    <div class="concept-card">
                        <h4>üè∑Ô∏è Image Classification</h4>
                        <p>Assign a label to an entire image</p>
                        <p><em>"This image contains a cat"</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>üìç Object Detection</h4>
                        <p>Find and locate objects with bounding boxes</p>
                        <p><em>"Cat at position (x, y, w, h)"</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>üé® Semantic Segmentation</h4>
                        <p>Classify each pixel in the image</p>
                        <p><em>Create a pixel-wise mask</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>üî¢ Instance Segmentation</h4>
                        <p>Separate different instances of same class</p>
                        <p><em>Distinguish cat #1 from cat #2</em></p>
                    </div>
                </div>

                <h3>Images as Data</h3>
                <p>To a computer, an image is a 3D array of numbers:</p>
                <ul>
                    <li><strong>Height √ó Width √ó Channels</strong></li>
                    <li>Grayscale: 1 channel (0-255 intensity)</li>
                    <li>Color (RGB): 3 channels (Red, Green, Blue)</li>
                    <li>Example: 224 √ó 224 √ó 3 = 150,528 values!</li>
                </ul>
            </div>
        </section>

        <section id="cnn" class="content-section">
            <div class="container">
                <h2>Convolutional Neural Networks (CNNs)</h2>
                <p>CNNs are the foundation of modern computer vision. They're designed specifically to process grid-like data (images).</p>

                <div class="algorithm-card">
                    <h3>Why CNNs for Images?</h3>
                    <div class="algorithm-meta">
                        <span>üëÅÔ∏è Type: Deep Learning</span>
                        <span>üìö NOAI: Theory + Practice</span>
                    </div>

                    <h4>Problems with MLPs for Images</h4>
                    <ul>
                        <li>Too many parameters (224√ó224√ó3 = 150K inputs!)</li>
                        <li>No spatial awareness (treats pixels independently)</li>
                        <li>Not translation invariant (cat in corner ‚â† cat in center)</li>
                    </ul>

                    <h4>CNN Solutions</h4>
                    <ul>
                        <li><strong>Local connectivity:</strong> Each neuron only "sees" a small region</li>
                        <li><strong>Weight sharing:</strong> Same filter applied across entire image</li>
                        <li><strong>Hierarchical features:</strong> Early layers detect edges, later layers detect objects</li>
                    </ul>
                </div>

                <h3>Key CNN Components</h3>

                <div class="concept-grid">
                    <div class="concept-card">
                        <h4>Convolutional Layer</h4>
                        <p>Applies learnable filters (kernels) that slide across the image to detect features.</p>
                        <p><em>Detects edges, textures, patterns</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>Pooling Layer</h4>
                        <p>Downsamples feature maps to reduce size and computation.</p>
                        <p><em>Max pooling: take maximum value in region</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>Fully Connected Layer</h4>
                        <p>Traditional neural network layers at the end for classification.</p>
                        <p><em>Combines features for final prediction</em></p>
                    </div>
                </div>

                <h3>Convolution Operation</h3>
                <div class="algorithm-card">
                    <h4>How Convolution Works</h4>
                    <ol>
                        <li>Slide a small filter (e.g., 3√ó3) across the image</li>
                        <li>At each position, compute element-wise multiplication</li>
                        <li>Sum the results to get one output value</li>
                        <li>Move filter by stride and repeat</li>
                    </ol>

                    <h4>Key Parameters</h4>
                    <ul>
                        <li><strong>Kernel size:</strong> Filter dimensions (common: 3√ó3, 5√ó5)</li>
                        <li><strong>Stride:</strong> Step size when sliding (1 = overlap, 2 = skip)</li>
                        <li><strong>Padding:</strong> Add zeros around edges to preserve size</li>
                        <li><strong>Filters:</strong> Number of different patterns to detect</li>
                    </ul>

                    <div class="formula">
                        <div class="formula-title">Output Size Formula</div>
                        Output = (Input - Kernel + 2√óPadding) / Stride + 1
                    </div>
                </div>

                <pre><code># PyTorch: CNN Building Blocks
import torch.nn as nn

# Convolutional layer
conv = nn.Conv2d(
    in_channels=3,      # RGB input
    out_channels=64,    # 64 filters
    kernel_size=3,      # 3x3 filter
    stride=1,
    padding=1           # Same padding
)

# Max pooling
pool = nn.MaxPool2d(kernel_size=2, stride=2)  # Halves dimensions

# Example: Input (3, 224, 224) ‚Üí Conv ‚Üí (64, 224, 224) ‚Üí Pool ‚Üí (64, 112, 112)</code></pre>
            </div>
        </section>

        <section id="architectures" class="content-section">
            <div class="container">
                <h2>Famous CNN Architectures</h2>
                <p>Understanding classic architectures helps you design better models.</p>

                <div class="algorithm-card">
                    <h3>LeNet-5 (1998)</h3>
                    <p>The original CNN for handwritten digit recognition.</p>
                    <p><strong>Architecture:</strong> Conv ‚Üí Pool ‚Üí Conv ‚Üí Pool ‚Üí FC ‚Üí FC ‚Üí Output</p>
                </div>

                <div class="algorithm-card">
                    <h3>VGGNet (2014)</h3>
                    <div class="algorithm-meta">
                        <span>üìä Depth: 16-19 layers</span>
                        <span>üéØ Simple design</span>
                    </div>
                    <p>Key insight: <strong>Stack many 3√ó3 convolutions</strong> instead of large filters.</p>
                    <p>Two 3√ó3 convs = one 5√ó5 conv (same receptive field, fewer parameters)</p>
                </div>

                <div class="algorithm-card">
                    <h3>ResNet (2015)</h3>
                    <div class="algorithm-meta">
                        <span>üìä Depth: 18-152+ layers</span>
                        <span>üìö NOAI: Practice</span>
                        <span>üèÜ Very Important</span>
                    </div>

                    <h4>The Problem</h4>
                    <p>Deeper networks should be better, but they become harder to train (vanishing gradients).</p>

                    <h4>The Solution: Skip Connections</h4>
                    <p>Add the input directly to the output of a layer block:</p>
                    <div class="formula">
                        output = F(x) + x
                    </div>
                    <p>This allows gradients to flow directly backward and enables training of very deep networks.</p>
                </div>

                <pre><code># PyTorch: ResNet Block (Simplified)
class ResidualBlock(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(channels)

    def forward(self, x):
        identity = x  # Save input

        out = self.conv1(x)
        out = self.bn1(out)
        out = torch.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        out = out + identity  # Skip connection!
        out = torch.relu(out)
        return out</code></pre>

                <h3>Using Pre-trained Models</h3>
                <p>Instead of training from scratch, use models pre-trained on ImageNet (1M+ images):</p>

                <pre><code># PyTorch: Using Pre-trained ResNet
import torchvision.models as models

# Load pre-trained ResNet-18
model = models.resnet18(pretrained=True)

# Replace final layer for your task (e.g., 10 classes)
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, 10)

# Option 1: Fine-tune entire network
# Option 2: Freeze early layers, only train final layers
for param in model.parameters():
    param.requires_grad = False  # Freeze all
model.fc.requires_grad = True    # Unfreeze final layer</code></pre>
            </div>
        </section>

        <section id="augmentation" class="content-section">
            <div class="container">
                <h2>Data Augmentation</h2>
                <p>Artificially expand your training data by applying transformations. This improves generalization and reduces overfitting.</p>

                <div class="concept-grid">
                    <div class="concept-card">
                        <h4>Geometric Transforms</h4>
                        <ul>
                            <li>Horizontal/vertical flip</li>
                            <li>Rotation</li>
                            <li>Scaling/cropping</li>
                            <li>Translation</li>
                        </ul>
                    </div>
                    <div class="concept-card">
                        <h4>Color Transforms</h4>
                        <ul>
                            <li>Brightness adjustment</li>
                            <li>Contrast changes</li>
                            <li>Saturation</li>
                            <li>Color jittering</li>
                        </ul>
                    </div>
                    <div class="concept-card">
                        <h4>Other Techniques</h4>
                        <ul>
                            <li>Gaussian noise</li>
                            <li>Cutout (random erasing)</li>
                            <li>Mixup (blend images)</li>
                            <li>CutMix</li>
                        </ul>
                    </div>
                </div>

                <pre><code># PyTorch: Data Augmentation
from torchvision import transforms

# Training transforms (with augmentation)
train_transform = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225])
])

# Validation transforms (no augmentation!)
val_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225])
])</code></pre>

                <div class="warning-box">
                    Only apply augmentation to training data, not validation or test data! Test data should reflect real-world conditions.
                </div>
            </div>
        </section>

        <section id="transfer-learning" class="content-section">
            <div class="container">
                <h2>Transfer Learning</h2>
                <p>Use knowledge from a model trained on one task to help with a different but related task.</p>

                <div class="algorithm-card">
                    <h3>Why Transfer Learning?</h3>
                    <ul>
                        <li>Don't need massive datasets (ImageNet has 1M+ images)</li>
                        <li>Faster training (start with good features)</li>
                        <li>Better performance with limited data</li>
                        <li>Lower layers learn general features (edges, textures)</li>
                    </ul>

                    <h4>Two Approaches</h4>
                    <ol>
                        <li><strong>Feature Extraction:</strong> Freeze pre-trained layers, only train new final layer(s)</li>
                        <li><strong>Fine-tuning:</strong> Train entire network with small learning rate</li>
                    </ol>
                </div>

                <h3>When to Use What</h3>
                <table class="comparison-table">
                    <thead>
                        <tr>
                            <th>Your Data</th>
                            <th>Similarity to Pre-training Data</th>
                            <th>Strategy</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Small</td>
                            <td>Similar</td>
                            <td>Feature extraction (freeze most layers)</td>
                        </tr>
                        <tr>
                            <td>Small</td>
                            <td>Different</td>
                            <td>Feature extraction from earlier layers</td>
                        </tr>
                        <tr>
                            <td>Large</td>
                            <td>Similar</td>
                            <td>Fine-tune entire network</td>
                        </tr>
                        <tr>
                            <td>Large</td>
                            <td>Different</td>
                            <td>Fine-tune or train from scratch</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </section>

        <section id="object-detection" class="content-section">
            <div class="container">
                <h2>Object Detection</h2>
                <p>Detect and localize multiple objects in an image with bounding boxes.</p>

                <div class="concept-grid">
                    <div class="concept-card">
                        <h4>Two-Stage Detectors</h4>
                        <p><strong>R-CNN Family:</strong> First propose regions, then classify each.</p>
                        <ul>
                            <li>R-CNN</li>
                            <li>Fast R-CNN</li>
                            <li>Faster R-CNN</li>
                        </ul>
                        <p><em>More accurate, slower</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>One-Stage Detectors</h4>
                        <p><strong>Direct prediction:</strong> Predict boxes and classes in one pass.</p>
                        <ul>
                            <li>YOLO (You Only Look Once)</li>
                            <li>SSD</li>
                            <li>RetinaNet</li>
                        </ul>
                        <p><em>Faster, good for real-time</em></p>
                    </div>
                </div>

                <h3>Evaluation Metrics</h3>
                <ul>
                    <li><strong>IoU (Intersection over Union):</strong> Overlap between predicted and ground truth boxes</li>
                    <li><strong>mAP (mean Average Precision):</strong> Average precision across all classes and IoU thresholds</li>
                </ul>

                <div class="formula">
                    <div class="formula-title">IoU (Intersection over Union)</div>
                    IoU = Area of Overlap / Area of Union

                    IoU > 0.5 typically considered a "match"
                </div>
            </div>
        </section>

        <section id="advanced" class="content-section">
            <div class="container">
                <h2>Advanced Topics</h2>

                <h3>GANs (Generative Adversarial Networks)</h3>
                <div class="algorithm-card">
                    <div class="algorithm-meta">
                        <span>üé® Type: Generative Model</span>
                        <span>üìö NOAI: Practice</span>
                    </div>
                    <p>Two networks compete: a <strong>Generator</strong> creates fake images, a <strong>Discriminator</strong> tries to distinguish real from fake.</p>
                    <h4>Applications</h4>
                    <ul>
                        <li>Image generation</li>
                        <li>Style transfer</li>
                        <li>Super-resolution</li>
                        <li>Data augmentation</li>
                    </ul>
                </div>

                <h3>Vision Transformers (ViT)</h3>
                <p>Apply transformer architecture (from NLP) to images by treating image patches as "tokens".</p>

                <h3>CLIP (Contrastive Language-Image Pre-training)</h3>
                <div class="algorithm-card">
                    <div class="algorithm-meta">
                        <span>üîó Type: Vision-Language Model</span>
                        <span>üìö NOAI: Practice</span>
                    </div>
                    <p>Learns to match images with text descriptions. Enables:</p>
                    <ul>
                        <li>Zero-shot classification (classify without training examples)</li>
                        <li>Image search with natural language</li>
                        <li>Cross-modal understanding</li>
                    </ul>
                </div>

                <h3>Diffusion Models</h3>
                <p>State-of-the-art image generation. Learn to gradually denoise images, enabling high-quality generation.</p>
                <p><em>Used in: DALL-E, Stable Diffusion, Midjourney</em></p>
            </div>
        </section>

        <section id="quiz" class="content-section">
            <div class="container">
                <h2>NOAI Practice Questions - Computer Vision</h2>

                <div class="tip-box">
                    <strong>NOAI Exam Tips:</strong> Computer vision questions often test your understanding of CNN components, architectures, and object detection methods. Remember: pooling reduces dimensions, skip connections enable deep networks, and YOLO is a single-stage detector!
                </div>

                <h3>CNN Fundamentals</h3>

                <div class="quiz-container" data-correct="b" data-explanation="CNNs use weight sharing (same filter applied across entire image) and local connectivity, dramatically reducing parameters compared to fully connected networks.">
                    <p class="quiz-question">1. Why do CNNs have fewer parameters than MLPs for image tasks?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) They use smaller images</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Weight sharing and local connectivity</span></div>
                        <div class="quiz-option" data-value="c"><span>C) They have fewer layers</span></div>
                        <div class="quiz-option" data-value="d"><span>D) They don't use activation functions</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="Stride controls how much the filter moves at each step. A stride of 2 means the filter moves 2 pixels at a time, reducing output dimensions by half.">
                    <p class="quiz-question">2. In a convolutional layer, what does the 'stride' parameter control?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) How many pixels the filter moves at each step</span></div>
                        <div class="quiz-option" data-value="b"><span>B) The size of the convolutional filter</span></div>
                        <div class="quiz-option" data-value="c"><span>C) The number of output channels</span></div>
                        <div class="quiz-option" data-value="d"><span>D) The activation function used</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="Padding adds zeros around the input borders. 'Same' padding preserves spatial dimensions, while 'valid' (no padding) reduces output size.">
                    <p class="quiz-question">3. What is the purpose of padding in CNNs?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) To increase the number of parameters</span></div>
                        <div class="quiz-option" data-value="b"><span>B) To speed up training</span></div>
                        <div class="quiz-option" data-value="c"><span>C) To preserve spatial dimensions and process border pixels</span></div>
                        <div class="quiz-option" data-value="d"><span>D) To reduce overfitting</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="The number of output channels equals the number of filters (kernels) used. Each filter learns to detect a different feature pattern.">
                    <p class="quiz-question">4. What determines the number of output channels in a convolutional layer?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) The kernel size</span></div>
                        <div class="quiz-option" data-value="b"><span>B) The stride value</span></div>
                        <div class="quiz-option" data-value="c"><span>C) The input image dimensions</span></div>
                        <div class="quiz-option" data-value="d"><span>D) The number of filters used</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h3>Pooling Layers</h3>

                <div class="quiz-container" data-correct="b" data-explanation="NOAI 2025 Style: Pooling layers reduce the spatial dimensions (height and width) of feature maps, decreasing computation and providing translation invariance.">
                    <p class="quiz-question">5. [NOAI Style] What is the primary purpose of pooling layers in CNNs?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) To increase the depth of feature maps</span></div>
                        <div class="quiz-option" data-value="b"><span>B) To reduce spatial dimensions of feature maps</span></div>
                        <div class="quiz-option" data-value="c"><span>C) To add non-linearity to the network</span></div>
                        <div class="quiz-option" data-value="d"><span>D) To normalize the input data</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="Max pooling takes the maximum value in each pooling window, while average pooling computes the mean. Max pooling is more common as it preserves prominent features.">
                    <p class="quiz-question">6. How does max pooling differ from average pooling?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Max pooling takes the highest value, average pooling computes the mean</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Max pooling is faster to compute</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Average pooling produces larger output dimensions</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Max pooling requires more memory</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h3>Famous Architectures</h3>

                <div class="quiz-container" data-correct="c" data-explanation="ResNet introduced skip (residual) connections that allow gradients to flow directly through the network, enabling training of very deep networks (100+ layers).">
                    <p class="quiz-question">7. What innovation did ResNet introduce that enabled training very deep networks?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Batch normalization</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Max pooling</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Skip (residual) connections</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Dropout regularization</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="VGGNet showed that using many small 3x3 filters stacked together is more effective than using larger filters. Two 3x3 convolutions have the same receptive field as one 5x5 but with fewer parameters.">
                    <p class="quiz-question">8. What key insight did VGGNet demonstrate about filter sizes?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Larger filters (7x7) work better for all tasks</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Stacking small 3x3 filters is more effective than large filters</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Filter size doesn't matter for accuracy</span></div>
                        <div class="quiz-option" data-value="d"><span>D) 1x1 filters are sufficient for classification</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="AlexNet (2012) was the breakthrough CNN that won ImageNet with a large margin, popularizing deep learning for computer vision. It used ReLU, dropout, and GPU training.">
                    <p class="quiz-question">9. Which architecture marked the breakthrough moment for deep learning in computer vision by winning ImageNet 2012?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) AlexNet</span></div>
                        <div class="quiz-option" data-value="b"><span>B) VGGNet</span></div>
                        <div class="quiz-option" data-value="c"><span>C) ResNet</span></div>
                        <div class="quiz-option" data-value="d"><span>D) LeNet</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="NOAI 2025 Style: Depthwise separable convolutions split the standard convolution into depthwise and pointwise operations, significantly reducing parameters and computational cost while maintaining similar accuracy.">
                    <p class="quiz-question">10. [NOAI Style] What is the main advantage of depthwise separable convolutions used in MobileNet?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) They increase model accuracy significantly</span></div>
                        <div class="quiz-option" data-value="b"><span>B) They allow processing of larger images</span></div>
                        <div class="quiz-option" data-value="c"><span>C) They reduce parameters and computational cost</span></div>
                        <div class="quiz-option" data-value="d"><span>D) They eliminate the need for pooling layers</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h3>Transfer Learning & Data Augmentation</h3>

                <div class="quiz-container" data-correct="a" data-explanation="Data augmentation should only be applied to training data. Validation and test data should represent real-world conditions without artificial modifications.">
                    <p class="quiz-question">11. When should you apply data augmentation?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Only during training</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Only during testing</span></div>
                        <div class="quiz-option" data-value="c"><span>C) During both training and testing</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Never - it hurts performance</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="With a small dataset similar to ImageNet, feature extraction (freezing pre-trained layers) works best. Fine-tuning all layers risks overfitting on small data.">
                    <p class="quiz-question">12. You have a small dataset of dog breeds (similar to ImageNet). What transfer learning strategy is most appropriate?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Train from scratch</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Feature extraction - freeze most pre-trained layers</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Fine-tune all layers with high learning rate</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Only use the first few layers</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="Horizontal flip is not appropriate for digit recognition because flipped digits represent different numbers (e.g., 6 becomes 9). Rotation and noise are acceptable augmentations.">
                    <p class="quiz-question">13. Which augmentation technique would be INAPPROPRIATE for a digit recognition task?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Small rotation</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Slight scaling</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Adding Gaussian noise</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Horizontal flip</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h3>Object Detection</h3>

                <div class="quiz-container" data-correct="a" data-explanation="NOAI 2025 Style: YOLO (You Only Look Once) is a single-stage detector that combines region proposal and classification into one unified network pass, making it fast for real-time detection.">
                    <p class="quiz-question">14. [NOAI Style] What distinguishes YOLO from two-stage detectors like Faster R-CNN?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) YOLO combines region proposal and classification in one step</span></div>
                        <div class="quiz-option" data-value="b"><span>B) YOLO uses larger input images</span></div>
                        <div class="quiz-option" data-value="c"><span>C) YOLO has higher accuracy</span></div>
                        <div class="quiz-option" data-value="d"><span>D) YOLO requires more training data</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="NOAI 2025 Style: The Region Proposal Network (RPN) in Faster R-CNN suggests candidate object regions (bounding box proposals) in the feature map, which are then classified by the detection head.">
                    <p class="quiz-question">15. [NOAI Style] What is the role of the Region Proposal Network (RPN) in Faster R-CNN?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Classify objects into categories</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Extract feature maps from input images</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Suggest candidate object regions in the feature map</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Perform non-maximum suppression</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="IoU (Intersection over Union) measures the overlap between predicted and ground truth bounding boxes. IoU = Area of Overlap / Area of Union. Higher is better, with 0.5+ typically considered a match.">
                    <p class="quiz-question">16. What does IoU (Intersection over Union) measure in object detection?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) The confidence score of predictions</span></div>
                        <div class="quiz-option" data-value="b"><span>B) The overlap between predicted and ground truth boxes</span></div>
                        <div class="quiz-option" data-value="c"><span>C) The number of detected objects</span></div>
                        <div class="quiz-option" data-value="d"><span>D) The processing speed of the model</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h3>Segmentation Tasks</h3>

                <div class="quiz-container" data-correct="d" data-explanation="NOAI 2025 Style: Mask R-CNN extends Faster R-CNN by adding a mask prediction branch, enabling instance segmentation - detecting objects AND generating pixel-level masks for each instance.">
                    <p class="quiz-question">17. [NOAI Style] Which architecture is specifically designed for instance segmentation?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) VGGNet</span></div>
                        <div class="quiz-option" data-value="b"><span>B) YOLO</span></div>
                        <div class="quiz-option" data-value="c"><span>C) U-Net</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Mask R-CNN</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="Semantic segmentation classifies each pixel but doesn't distinguish between instances of the same class. Instance segmentation identifies separate objects (cat #1 vs cat #2).">
                    <p class="quiz-question">18. What is the key difference between semantic segmentation and instance segmentation?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Semantic segmentation is faster</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Instance segmentation only works on single objects</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Instance segmentation distinguishes different objects of the same class</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Semantic segmentation uses bounding boxes</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h3>GANs and Advanced Topics</h3>

                <div class="quiz-container" data-correct="a" data-explanation="In GANs, the generator creates fake images trying to fool the discriminator, while the discriminator tries to distinguish real images from generated fakes. They compete in a minimax game.">
                    <p class="quiz-question">19. In a GAN, what is the relationship between the generator and discriminator?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Generator creates fakes, discriminator distinguishes real from fake</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Both networks classify images</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Discriminator generates images, generator evaluates quality</span></div>
                        <div class="quiz-option" data-value="d"><span>D) They work together to segment images</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="Vision Transformers (ViT) treat image patches as tokens (like words in NLP), applying self-attention across patches. This allows capturing global relationships from the first layer.">
                    <p class="quiz-question">20. How do Vision Transformers (ViT) process images?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Using convolutional filters exclusively</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Treating image patches as tokens and applying self-attention</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Processing one pixel at a time sequentially</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Using recurrent connections like RNNs</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h3>Additional Computer Vision Practice</h3>

                <h4>Advanced CNN Concepts</h4>

                <div class="quiz-container" data-correct="c" data-explanation="Global Average Pooling (GAP) averages each feature map to a single value, eliminating the need for fully connected layers and reducing parameters while providing spatial invariance.">
                    <p class="quiz-question">21. What is the purpose of Global Average Pooling (GAP) in modern CNN architectures?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) To increase spatial resolution</span></div>
                        <div class="quiz-option" data-value="b"><span>B) To add more trainable parameters</span></div>
                        <div class="quiz-option" data-value="c"><span>C) To reduce each feature map to a single value, replacing FC layers</span></div>
                        <div class="quiz-option" data-value="d"><span>D) To perform data augmentation</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="Dilated (atrous) convolutions expand the receptive field without increasing parameters or reducing resolution by inserting gaps between filter elements.">
                    <p class="quiz-question">22. What advantage do dilated (atrous) convolutions provide?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Faster training convergence</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Larger receptive field without increasing parameters</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Better handling of color images</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Reduced memory usage during inference</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="DenseNet connects each layer to every other layer in a feed-forward fashion, promoting feature reuse, strengthening gradient flow, and substantially reducing parameters.">
                    <p class="quiz-question">23. What is the key innovation in DenseNet architecture?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Dense connections - each layer receives features from all preceding layers</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Using only 1x1 convolutions throughout</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Removing all pooling layers</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Using larger batch sizes</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="EfficientNet uses compound scaling to uniformly scale network width, depth, and resolution together using a compound coefficient, achieving better accuracy with fewer parameters.">
                    <p class="quiz-question">24. [NOAI Style] What scaling strategy does EfficientNet use to balance network dimensions?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Only increasing network depth</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Only increasing input resolution</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Random scaling of different dimensions</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Compound scaling of width, depth, and resolution together</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="Receptive field is the region of the input image that affects a particular feature map element. Deeper layers have larger receptive fields, allowing them to capture more global patterns.">
                    <p class="quiz-question">25. What does the 'receptive field' of a CNN neuron refer to?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) The number of parameters in the layer</span></div>
                        <div class="quiz-option" data-value="b"><span>B) The output size of the feature map</span></div>
                        <div class="quiz-option" data-value="c"><span>C) The region of input image that influences that neuron's output</span></div>
                        <div class="quiz-option" data-value="d"><span>D) The learning rate used during training</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Object Detection Deep Dive</h4>

                <div class="quiz-container" data-correct="b" data-explanation="Anchor boxes are predefined bounding boxes of various sizes and aspect ratios that serve as reference templates. The detector predicts offsets from these anchors rather than absolute coordinates.">
                    <p class="quiz-question">26. What are anchor boxes in object detection?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Ground truth bounding boxes from the training data</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Predefined reference boxes at various scales and aspect ratios</span></div>
                        <div class="quiz-option" data-value="c"><span>C) The final output predictions of the model</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Boxes used only during data augmentation</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="Non-Maximum Suppression (NMS) removes redundant overlapping detections by keeping only the highest confidence box and suppressing boxes with high IoU overlap.">
                    <p class="quiz-question">27. [NOAI Style] What is the purpose of Non-Maximum Suppression (NMS) in object detection?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Remove duplicate overlapping detections, keeping the most confident</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Normalize the confidence scores</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Increase the number of detected objects</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Train the detector to recognize new classes</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="Feature Pyramid Networks (FPN) create multi-scale feature maps by combining high-resolution low-level features with semantically strong high-level features, improving detection of objects at different scales.">
                    <p class="quiz-question">28. What problem does Feature Pyramid Network (FPN) solve in object detection?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Reducing training time</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Handling occluded objects</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Detecting objects at multiple scales effectively</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Processing video streams in real-time</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="Focal Loss addresses class imbalance by down-weighting the loss from easy (well-classified) examples, focusing training on hard negatives. It was introduced in RetinaNet to handle the extreme imbalance in one-stage detectors.">
                    <p class="quiz-question">29. [NOAI Style] What problem does Focal Loss address in object detection?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Bounding box regression accuracy</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Multi-scale feature extraction</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Inference speed optimization</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Class imbalance between foreground and background</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="mAP (mean Average Precision) is calculated by averaging the AP across all object classes. AP itself is the area under the precision-recall curve for each class.">
                    <p class="quiz-question">30. How is mean Average Precision (mAP) calculated in object detection?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Average IoU across all predictions</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Average of per-class Average Precision (area under PR curve)</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Maximum accuracy across IoU thresholds</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Ratio of correct detections to total detections</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Semantic Segmentation</h4>

                <div class="quiz-container" data-correct="a" data-explanation="U-Net uses an encoder-decoder structure with skip connections that concatenate encoder features to decoder features, preserving fine spatial details essential for precise segmentation.">
                    <p class="quiz-question">31. What architectural feature makes U-Net effective for biomedical image segmentation?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Encoder-decoder with skip connections preserving spatial details</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Very deep network with 152 layers</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Multiple detection heads</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Attention mechanisms only</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="FCN (Fully Convolutional Network) replaces fully connected layers with 1x1 convolutions, allowing the network to accept any input size and produce spatial output maps for dense prediction.">
                    <p class="quiz-question">32. What modification does FCN (Fully Convolutional Network) make to standard CNNs?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Adds recurrent layers for sequence processing</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Uses only pooling layers without convolutions</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Replaces fully connected layers with 1x1 convolutions</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Removes all activation functions</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="Transposed convolution (deconvolution) upsamples feature maps by inserting zeros between input values and performing convolution, effectively learning the upsampling operation.">
                    <p class="quiz-question">33. What operation is used to upsample feature maps in semantic segmentation?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Max pooling</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Transposed convolution (deconvolution)</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Batch normalization</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Dropout</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="Dice loss measures the overlap between predicted and ground truth segmentation masks. It's especially useful for imbalanced datasets where foreground pixels are rare.">
                    <p class="quiz-question">34. [NOAI Style] Which loss function is commonly used for segmentation tasks with imbalanced classes?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Mean Squared Error</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Hinge Loss</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Triplet Loss</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Dice Loss</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Image Preprocessing & Normalization</h4>

                <div class="quiz-container" data-correct="a" data-explanation="ImageNet normalization uses mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225] for RGB channels. These values were computed from the ImageNet dataset and are used for transfer learning.">
                    <p class="quiz-question">35. Why do we use ImageNet mean and standard deviation for image normalization?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Pre-trained models expect inputs normalized with these statistics</span></div>
                        <div class="quiz-option" data-value="b"><span>B) It's the only mathematically correct normalization</span></div>
                        <div class="quiz-option" data-value="c"><span>C) It maximizes the image resolution</span></div>
                        <div class="quiz-option" data-value="d"><span>D) It's required by GPU hardware</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="Bilinear interpolation uses weighted average of the 4 nearest pixels to compute new pixel values, providing smooth resizing. Nearest neighbor just copies the closest pixel value.">
                    <p class="quiz-question">36. What is the difference between bilinear and nearest-neighbor image interpolation?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Bilinear is faster but lower quality</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Nearest-neighbor produces smoother results</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Bilinear uses weighted average of 4 neighbors; nearest copies closest</span></div>
                        <div class="quiz-option" data-value="d"><span>D) There is no difference in output quality</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="Center crop removes equal portions from all edges to get the center region, while random crop takes a random portion. Center crop is used for validation to ensure consistent evaluation.">
                    <p class="quiz-question">37. Why is center cropping typically used for validation/test images rather than random cropping?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Center cropping is faster to compute</span></div>
                        <div class="quiz-option" data-value="b"><span>B) To ensure consistent, reproducible evaluation</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Random cropping doesn't work on validation data</span></div>
                        <div class="quiz-option" data-value="d"><span>D) The center always contains the most important features</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Advanced Augmentation Techniques</h4>

                <div class="quiz-container" data-correct="a" data-explanation="Mixup creates new training examples by taking convex combinations of pairs of images and their labels: x_new = Œªx‚ÇÅ + (1-Œª)x‚ÇÇ, y_new = Œªy‚ÇÅ + (1-Œª)y‚ÇÇ. This regularizes the model and improves generalization.">
                    <p class="quiz-question">38. How does Mixup data augmentation work?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Blends two images and their labels with a mixing coefficient</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Randomly erases rectangular regions of images</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Applies multiple augmentations simultaneously</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Swaps color channels between images</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="CutMix cuts a patch from one image and pastes it onto another, mixing labels proportionally to the patch area. Unlike Cutout, the erased region is filled with useful information.">
                    <p class="quiz-question">39. [NOAI Style] What distinguishes CutMix from Cutout augmentation?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) CutMix only works on grayscale images</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Cutout blends entire images together</span></div>
                        <div class="quiz-option" data-value="c"><span>C) CutMix fills erased regions with patches from other images</span></div>
                        <div class="quiz-option" data-value="d"><span>D) CutMix requires paired training data</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="AutoAugment uses reinforcement learning to search for optimal augmentation policies for a specific dataset, automatically finding combinations of augmentation operations and their magnitudes.">
                    <p class="quiz-question">40. What is the key idea behind AutoAugment?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Manually designing augmentation for each dataset</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Using only geometric transformations</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Applying all possible augmentations at once</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Automatically learning optimal augmentation policies</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>GANs and Generative Models</h4>

                <div class="quiz-container" data-correct="b" data-explanation="Mode collapse occurs when the generator produces only a limited variety of outputs, essentially 'collapsing' to generate similar samples regardless of input noise, failing to capture the full data distribution.">
                    <p class="quiz-question">41. What is 'mode collapse' in GAN training?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) The discriminator becomes too powerful</span></div>
                        <div class="quiz-option" data-value="b"><span>B) The generator produces limited variety, ignoring some data modes</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Training converges too quickly</span></div>
                        <div class="quiz-option" data-value="d"><span>D) The network runs out of memory</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="Conditional GANs (cGANs) provide additional information like class labels to both generator and discriminator, allowing controlled generation of specific types of outputs.">
                    <p class="quiz-question">42. What does a Conditional GAN (cGAN) add to the standard GAN framework?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Class labels or other conditioning information for controlled generation</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Multiple discriminators</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Reinforcement learning objectives</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Recursive network structures</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="StyleGAN introduces a mapping network and style-based generator that allows fine-grained control over generated images at different scales (coarse, medium, fine).">
                    <p class="quiz-question">43. What innovation did StyleGAN introduce for image generation?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Using transformers instead of convolutions</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Training without a discriminator</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Style-based generation with controllable attributes at different scales</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Generating images from text descriptions only</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="Diffusion models learn to gradually denoise images, starting from pure noise and iteratively removing noise to generate high-quality images. They are more stable to train than GANs.">
                    <p class="quiz-question">44. [NOAI Style] How do diffusion models generate images?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) By adversarial training between generator and discriminator</span></div>
                        <div class="quiz-option" data-value="b"><span>B) By encoding and decoding through a bottleneck</span></div>
                        <div class="quiz-option" data-value="c"><span>C) By directly predicting pixel values from noise</span></div>
                        <div class="quiz-option" data-value="d"><span>D) By iteratively denoising from pure noise to image</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Vision-Language Models</h4>

                <div class="quiz-container" data-correct="b" data-explanation="CLIP learns a shared embedding space for images and text through contrastive learning, matching images with their text descriptions and separating non-matching pairs.">
                    <p class="quiz-question">45. How does CLIP learn to connect images and text?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) By generating captions for images</span></div>
                        <div class="quiz-option" data-value="b"><span>B) By contrastive learning to match image-text pairs in embedding space</span></div>
                        <div class="quiz-option" data-value="c"><span>C) By training separate models for each modality</span></div>
                        <div class="quiz-option" data-value="d"><span>D) By using reinforcement learning from human feedback</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="Zero-shot classification means classifying images into categories never seen during training, using only textual descriptions of the classes. CLIP enables this through its image-text alignment.">
                    <p class="quiz-question">46. What is 'zero-shot classification' in the context of CLIP?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Classifying into categories not seen during training using text descriptions</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Training without any labeled data</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Classification that takes zero computational time</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Predicting the absence of objects in images</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Attention in Computer Vision</h4>

                <div class="quiz-container" data-correct="c" data-explanation="Squeeze-and-Excitation (SE) blocks recalibrate channel-wise feature responses by learning to emphasize informative channels and suppress less useful ones through global average pooling and FC layers.">
                    <p class="quiz-question">47. What do Squeeze-and-Excitation (SE) blocks do in CNNs?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Reduce spatial dimensions like pooling</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Add skip connections between layers</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Learn to weight channel importance adaptively</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Perform data augmentation during training</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="CBAM (Convolutional Block Attention Module) combines both channel and spatial attention, first emphasizing 'what' is important (channel) then 'where' it is (spatial).">
                    <p class="quiz-question">48. [NOAI Style] What two types of attention does CBAM (Convolutional Block Attention Module) combine?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Self-attention and cross-attention</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Channel attention and spatial attention</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Local attention and global attention</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Temporal attention and frequency attention</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Practical Considerations</h4>

                <div class="quiz-container" data-correct="d" data-explanation="Gradient checkpointing trades computation for memory by not storing all intermediate activations. During backpropagation, it recomputes activations as needed, enabling training of larger models.">
                    <p class="quiz-question">49. What is the purpose of gradient checkpointing in training large vision models?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) To speed up training significantly</span></div>
                        <div class="quiz-option" data-value="b"><span>B) To improve model accuracy</span></div>
                        <div class="quiz-option" data-value="c"><span>C) To enable distributed training</span></div>
                        <div class="quiz-option" data-value="d"><span>D) To reduce memory usage by recomputing activations</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="Test-Time Augmentation (TTA) applies augmentations to test images and averages predictions across augmented versions, often improving accuracy at the cost of increased inference time.">
                    <p class="quiz-question">50. What is Test-Time Augmentation (TTA)?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Applying augmentations during inference and averaging predictions</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Augmenting test data before final evaluation</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Using a different augmentation strategy for testing</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Generating augmented test datasets</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="Knowledge distillation transfers knowledge from a large teacher model to a smaller student model by training the student to match the teacher's soft probability outputs (softmax with temperature).">
                    <p class="quiz-question">51. In knowledge distillation for CNNs, what does the student model learn from?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Only the ground truth labels</span></div>
                        <div class="quiz-option" data-value="b"><span>B) The teacher's architecture</span></div>
                        <div class="quiz-option" data-value="c"><span>C) The teacher's soft probability outputs</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Random noise from the teacher</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="For imbalanced datasets, using weighted loss (giving higher weight to minority classes), oversampling rare classes, or undersampling majority classes can help the model learn minority classes better.">
                    <p class="quiz-question">52. [NOAI Style] When training on imbalanced image datasets, which strategy helps address class imbalance?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Using larger batch sizes only</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Class-weighted loss or oversampling minority classes</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Training for fewer epochs</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Removing all augmentation</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="Learning rate warmup gradually increases the learning rate from near zero to the target value over several iterations, helping stabilize early training when gradients may be unreliable.">
                    <p class="quiz-question">53. Why is learning rate warmup commonly used when training vision models?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) To stabilize training when initial gradients are unreliable</span></div>
                        <div class="quiz-option" data-value="b"><span>B) To increase final model accuracy</span></div>
                        <div class="quiz-option" data-value="c"><span>C) To reduce the total training time</span></div>
                        <div class="quiz-option" data-value="d"><span>D) To enable larger batch sizes</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Evaluation and Metrics</h4>

                <div class="quiz-container" data-correct="d" data-explanation="Top-5 accuracy measures whether the correct label is among the model's top 5 predictions, which is more lenient than top-1 and useful when classes are ambiguous or similar.">
                    <p class="quiz-question">54. What does Top-5 accuracy measure in image classification?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Accuracy on the 5 most common classes</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Average accuracy across 5 runs</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Accuracy using only 5% of test data</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Whether correct label is in the top 5 predictions</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="Pixel accuracy can be misleading for segmentation because it's dominated by the majority class. Mean IoU weights all classes equally, providing better measure for imbalanced segmentation tasks.">
                    <p class="quiz-question">55. Why is mean IoU preferred over pixel accuracy for evaluating semantic segmentation?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Pixel accuracy is harder to compute</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Pixel accuracy is dominated by majority classes</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Mean IoU is faster to calculate</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Pixel accuracy doesn't work for multi-class tasks</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="FLOPs (Floating Point Operations) measure computational complexity, useful for comparing model efficiency. FPS (Frames Per Second) measures actual inference speed on specific hardware.">
                    <p class="quiz-question">56. What do FLOPs and FPS measure when comparing CV models?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Model accuracy and precision</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Training time and memory usage</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Computational complexity and inference speed</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Dataset size and augmentation factor</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Special Applications</h4>

                <div class="quiz-container" data-correct="a" data-explanation="Siamese networks learn to compare two inputs by processing them through identical (shared weight) networks and comparing their embeddings, useful for verification tasks like face recognition.">
                    <p class="quiz-question">57. What architecture is commonly used for one-shot learning and face verification?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Siamese networks with shared weights</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Standard classification CNNs</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Generative Adversarial Networks</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Recurrent Neural Networks</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="Optical flow estimates pixel motion between consecutive frames, providing information about how objects move in video. It's used for action recognition, video prediction, and tracking.">
                    <p class="quiz-question">58. [NOAI Style] What does optical flow estimation measure in video analysis?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) The brightness changes between frames</span></div>
                        <div class="quiz-option" data-value="b"><span>B) The apparent motion of pixels between consecutive frames</span></div>
                        <div class="quiz-option" data-value="c"><span>C) The depth of objects in the scene</span></div>
                        <div class="quiz-option" data-value="d"><span>D) The number of objects in each frame</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="Super-resolution models learn to reconstruct high-resolution details from low-resolution inputs, often using residual learning to predict the difference between low-res and high-res.">
                    <p class="quiz-question">59. What is the goal of image super-resolution models?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Compress images to smaller file sizes</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Detect objects in low-quality images</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Reconstruct high-resolution images from low-resolution inputs</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Convert color images to grayscale</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="Depth estimation from monocular images uses visual cues like texture gradients, relative size, and occlusion to predict pixel-wise depth maps, useful for robotics and AR applications.">
                    <p class="quiz-question">60. What visual cues do monocular depth estimation models rely on?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Stereo image pairs only</span></div>
                        <div class="quiz-option" data-value="b"><span>B) LiDAR sensor data</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Infrared imaging</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Texture gradients, relative size, and occlusion patterns</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="algorithm-card">
                    <h3>NOAI Memory Aids - Computer Vision</h3>
                    <ul>
                        <li><strong>Pooling = Shrinking:</strong> Reduces spatial dimensions, provides translation invariance</li>
                        <li><strong>YOLO = One Look:</strong> Single-stage detector, fast but may sacrifice some accuracy</li>
                        <li><strong>R-CNN Family = Two-Stage:</strong> Propose regions first, then classify (more accurate, slower)</li>
                        <li><strong>ResNet = Highway:</strong> Skip connections let gradients flow like a highway</li>
                        <li><strong>IoU = Overlap Score:</strong> Area of intersection / Area of union (0 to 1)</li>
                        <li><strong>Mask R-CNN = Faster R-CNN + Masks:</strong> Adds pixel-level segmentation to detection</li>
                        <li><strong>Depthwise Separable = Split & Save:</strong> Separate spatial and channel processing for efficiency</li>
                    </ul>
                </div>
            </div>
        </section>

        <div class="container">
            <nav class="page-navigation">
                <a href="neural-networks.html" class="page-nav-link prev">
                    <span class="page-nav-label">‚Üê Previous</span>
                    <span class="page-nav-title">Neural Networks</span>
                </a>
                <a href="nlp.html" class="page-nav-link next">
                    <span class="page-nav-label">Next ‚Üí</span>
                    <span class="page-nav-title">Natural Language Processing</span>
                </a>
            </nav>
        </div>
    </main>

    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-section">
                    <h4>ML for NOAI</h4>
                    <p>An educational resource for Singapore secondary school students preparing for the National Olympiad in AI.</p>
                </div>
                <div class="footer-section">
                    <h4>Quick Links</h4>
                    <ul>
                        <li><a href="fundamentals.html">ML Fundamentals</a></li>
                        <li><a href="supervised.html">Supervised Learning</a></li>
                        <li><a href="neural-networks.html">Neural Networks</a></li>
                    </ul>
                </div>
                <div class="footer-section">
                    <h4>References</h4>
                    <ul>
                        <li><a href="https://ioai-official.org/" target="_blank">IOAI Official</a></li>
                        <li><a href="https://aisingapore.org/" target="_blank">AI Singapore</a></li>
                    </ul>
                </div>
            </div>
            <div class="footer-bottom">
                <p>Educational content aligned with IOAI Syllabus. Not affiliated with AI Singapore or IOAI.</p>
            </div>
        </div>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>
