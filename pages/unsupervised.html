<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Unsupervised Learning - ML for NOAI</title>
    <link rel="stylesheet" href="../css/style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Fira+Code&display=swap" rel="stylesheet">
</head>
<body>
    <nav class="navbar">
        <div class="nav-container">
            <a href="../index.html" class="nav-logo">
                <span class="logo-icon">ü§ñ</span>
                <span>ML for NOAI</span>
            </a>
            <button class="nav-toggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            <ul class="nav-menu">
                <li><a href="../index.html" class="nav-link">Home</a></li>
                <li><a href="fundamentals.html" class="nav-link">Fundamentals</a></li>
                <li><a href="supervised.html" class="nav-link">Supervised Learning</a></li>
                <li><a href="unsupervised.html" class="nav-link active">Unsupervised Learning</a></li>
                <li><a href="neural-networks.html" class="nav-link">Neural Networks</a></li>
                <li><a href="computer-vision.html" class="nav-link">Computer Vision</a></li>
                <li><a href="nlp.html" class="nav-link">NLP</a></li>
            </ul>
        </div>
    </nav>

    <div class="breadcrumb">
        <div class="container">
            <ul>
                <li><a href="../index.html">Home</a></li>
                <li>Unsupervised Learning</li>
            </ul>
        </div>
    </div>

    <header class="page-header">
        <div class="container">
            <h1>üîç Unsupervised Learning</h1>
            <p class="subtitle">Discover hidden patterns and structures in data without labeled examples.</p>
        </div>
    </header>

    <div class="progress-container">
        <div class="container">
            <div class="progress-bar">
                <div class="progress-fill"></div>
            </div>
        </div>
    </div>

    <main>
        <section id="overview" class="content-section">
            <div class="container">
                <h2>What is Unsupervised Learning?</h2>
                <p>Unlike supervised learning, unsupervised learning works with <strong>unlabeled data</strong>. The algorithm must discover patterns, structures, and relationships on its own without any guidance about what the "correct" answer should be.</p>

                <div class="concept-grid">
                    <div class="concept-card">
                        <h4>üéØ Clustering</h4>
                        <p>Group similar data points together</p>
                        <p><em>Example: Customer segmentation, document grouping</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>üìâ Dimensionality Reduction</h4>
                        <p>Reduce number of features while preserving information</p>
                        <p><em>Example: Data visualization, noise reduction</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>üîó Association</h4>
                        <p>Find rules that describe relationships</p>
                        <p><em>Example: Market basket analysis ("bought X also bought Y")</em></p>
                    </div>
                </div>
            </div>
        </section>

        <section id="kmeans" class="content-section">
            <div class="container">
                <h2>K-Means Clustering</h2>
                <p>The most popular clustering algorithm. It partitions data into K clusters where each point belongs to the cluster with the nearest mean (centroid).</p>

                <div class="algorithm-card">
                    <h3>K-Means Algorithm</h3>
                    <div class="algorithm-meta">
                        <span>üéØ Type: Clustering</span>
                        <span>üìö NOAI: Theory + Practice</span>
                        <span>‚è±Ô∏è O(n√ók√ói√ód)</span>
                    </div>

                    <h4>How It Works</h4>
                    <ol>
                        <li><strong>Initialize:</strong> Randomly place K centroids</li>
                        <li><strong>Assign:</strong> Assign each point to its nearest centroid</li>
                        <li><strong>Update:</strong> Move each centroid to the mean of its assigned points</li>
                        <li><strong>Repeat:</strong> Steps 2-3 until centroids stop moving (convergence)</li>
                    </ol>

                    <div class="formula">
                        <div class="formula-title">Objective: Minimize Within-Cluster Sum of Squares (WCSS)</div>
                        WCSS = Œ£·µ¢ Œ£‚Çì‚ààC·µ¢ ||x - Œº·µ¢||¬≤

                        where Œº·µ¢ is the centroid of cluster C·µ¢
                    </div>
                </div>

                <h3>Choosing K: The Elbow Method</h3>
                <p>How do we know how many clusters to use? The <strong>Elbow Method</strong> helps:</p>
                <ol>
                    <li>Run K-Means for different values of K (e.g., 1 to 10)</li>
                    <li>Calculate WCSS (inertia) for each K</li>
                    <li>Plot K vs WCSS</li>
                    <li>Find the "elbow" where adding more clusters doesn't significantly reduce WCSS</li>
                </ol>

                <pre><code># Python: K-Means Clustering
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Find optimal K using elbow method
wcss = []
K_range = range(1, 11)
for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)

# Plot elbow curve
plt.plot(K_range, wcss, 'bo-')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('WCSS (Inertia)')
plt.title('Elbow Method')
plt.show()

# Train final model
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
clusters = kmeans.fit_predict(X)

# Get cluster centers
print("Centroids:", kmeans.cluster_centers_)</code></pre>

                <div class="warning-box">
                    K-Means assumes spherical clusters of similar size. It may not work well for clusters with different shapes or densities. Also, always scale your features before using K-Means!
                </div>

                <h3>K-Means Limitations</h3>
                <ul>
                    <li>Must specify K in advance</li>
                    <li>Sensitive to initial centroid placement (use k-means++ initialization)</li>
                    <li>Assumes spherical, similar-sized clusters</li>
                    <li>Sensitive to outliers</li>
                    <li>May converge to local minimum</li>
                </ul>
            </div>
        </section>

        <section id="hierarchical" class="content-section">
            <div class="container">
                <h2>Hierarchical Clustering</h2>
                <p>Builds a tree of clusters (dendrogram) that shows how clusters are related at different levels of granularity.</p>

                <div class="algorithm-card">
                    <h3>Agglomerative (Bottom-Up) Clustering</h3>
                    <div class="algorithm-meta">
                        <span>üéØ Type: Clustering</span>
                        <span>üìö NOAI: Practice</span>
                        <span>üå≥ Creates Dendrogram</span>
                    </div>

                    <h4>How It Works</h4>
                    <ol>
                        <li>Start with each point as its own cluster</li>
                        <li>Find the two closest clusters</li>
                        <li>Merge them into one cluster</li>
                        <li>Repeat until all points are in one cluster</li>
                        <li>Cut the dendrogram at desired level to get K clusters</li>
                    </ol>
                </div>

                <h3>Linkage Methods</h3>
                <p>How do we measure distance between clusters?</p>

                <div class="concept-grid">
                    <div class="concept-card">
                        <h4>Single Linkage</h4>
                        <p>Distance = minimum distance between any two points</p>
                        <p><em>Can create long, chain-like clusters</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>Complete Linkage</h4>
                        <p>Distance = maximum distance between any two points</p>
                        <p><em>Creates compact, spherical clusters</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>Average Linkage</h4>
                        <p>Distance = average of all pairwise distances</p>
                        <p><em>Balanced approach</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>Ward's Method</h4>
                        <p>Minimizes variance within clusters</p>
                        <p><em>Often produces best results</em></p>
                    </div>
                </div>

                <pre><code># Python: Hierarchical Clustering
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

# Create linkage matrix and plot dendrogram
Z = linkage(X, method='ward')
plt.figure(figsize=(12, 5))
dendrogram(Z)
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Sample Index')
plt.ylabel('Distance')
plt.show()

# Fit model
model = AgglomerativeClustering(n_clusters=3, linkage='ward')
clusters = model.fit_predict(X)</code></pre>
            </div>
        </section>

        <section id="dbscan" class="content-section">
            <div class="container">
                <h2>DBSCAN</h2>
                <p>Density-Based Spatial Clustering of Applications with Noise. Finds arbitrarily shaped clusters based on density.</p>

                <div class="algorithm-card">
                    <h3>DBSCAN Algorithm</h3>
                    <div class="algorithm-meta">
                        <span>üéØ Type: Clustering</span>
                        <span>üìö NOAI: Practice</span>
                        <span>üé≠ Handles Noise</span>
                    </div>

                    <h4>Key Concepts</h4>
                    <ul>
                        <li><strong>Œµ (epsilon):</strong> Maximum distance between two points to be considered neighbors</li>
                        <li><strong>MinPts:</strong> Minimum points needed to form a dense region</li>
                        <li><strong>Core Point:</strong> Has at least MinPts within Œµ distance</li>
                        <li><strong>Border Point:</strong> Within Œµ of a core point but not core itself</li>
                        <li><strong>Noise Point:</strong> Neither core nor border</li>
                    </ul>

                    <h4>How It Works</h4>
                    <ol>
                        <li>Find all core points</li>
                        <li>Connect core points that are within Œµ of each other</li>
                        <li>Assign border points to nearest cluster</li>
                        <li>Label remaining points as noise (-1)</li>
                    </ol>
                </div>

                <pre><code># Python: DBSCAN
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler

# Scale data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Fit DBSCAN
model = DBSCAN(eps=0.5, min_samples=5)
clusters = model.fit_predict(X_scaled)

# Check results
n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)
n_noise = list(clusters).count(-1)
print(f"Clusters found: {n_clusters}")
print(f"Noise points: {n_noise}")</code></pre>

                <h3>DBSCAN vs K-Means</h3>
                <table class="comparison-table">
                    <thead>
                        <tr>
                            <th>Aspect</th>
                            <th>K-Means</th>
                            <th>DBSCAN</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Number of clusters</td>
                            <td>Must specify K</td>
                            <td>Automatic</td>
                        </tr>
                        <tr>
                            <td>Cluster shape</td>
                            <td>Spherical</td>
                            <td>Arbitrary</td>
                        </tr>
                        <tr>
                            <td>Outliers</td>
                            <td>Assigned to clusters</td>
                            <td>Labeled as noise</td>
                        </tr>
                        <tr>
                            <td>Parameters</td>
                            <td>K</td>
                            <td>Œµ, MinPts</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </section>

        <section id="pca" class="content-section">
            <div class="container">
                <h2>Principal Component Analysis (PCA)</h2>
                <p>PCA is the most popular dimensionality reduction technique. It finds new axes (principal components) that capture the maximum variance in the data.</p>

                <div class="algorithm-card">
                    <h3>PCA Algorithm</h3>
                    <div class="algorithm-meta">
                        <span>üìâ Type: Dimensionality Reduction</span>
                        <span>üìö NOAI: Theory + Practice</span>
                        <span>üìä Linear Transformation</span>
                    </div>

                    <h4>How It Works</h4>
                    <ol>
                        <li><strong>Standardize</strong> the data (mean=0, std=1)</li>
                        <li>Compute the <strong>covariance matrix</strong></li>
                        <li>Calculate <strong>eigenvectors and eigenvalues</strong></li>
                        <li>Sort eigenvectors by eigenvalue (descending)</li>
                        <li>Select top K eigenvectors as <strong>principal components</strong></li>
                        <li>Transform data to new K-dimensional space</li>
                    </ol>

                    <h4>Key Properties</h4>
                    <ul>
                        <li>Principal components are orthogonal (uncorrelated)</li>
                        <li>PC1 captures most variance, PC2 second most, etc.</li>
                        <li>Total variance preserved = sum of selected eigenvalues / total eigenvalues</li>
                    </ul>
                </div>

                <h3>How Many Components?</h3>
                <p>Look at the <strong>explained variance ratio</strong>:</p>
                <ul>
                    <li>Plot cumulative explained variance vs number of components</li>
                    <li>Choose K where you capture enough variance (e.g., 95%)</li>
                </ul>

                <pre><code># Python: PCA
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Standardize first!
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Fit PCA
pca = PCA()
pca.fit(X_scaled)

# Plot explained variance
plt.figure(figsize=(10, 4))

# Individual variance
plt.subplot(1, 2, 1)
plt.bar(range(1, len(pca.explained_variance_ratio_) + 1),
        pca.explained_variance_ratio_)
plt.xlabel('Principal Component')
plt.ylabel('Explained Variance Ratio')

# Cumulative variance
plt.subplot(1, 2, 2)
plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),
         pca.explained_variance_ratio_.cumsum(), 'bo-')
plt.axhline(y=0.95, color='r', linestyle='--', label='95% threshold')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.legend()
plt.show()

# Transform to 2D for visualization
pca_2d = PCA(n_components=2)
X_2d = pca_2d.fit_transform(X_scaled)
print(f"Variance captured: {pca_2d.explained_variance_ratio_.sum():.2%}")</code></pre>

                <h3>PCA Applications</h3>
                <ul>
                    <li><strong>Visualization:</strong> Reduce to 2D/3D for plotting</li>
                    <li><strong>Noise Reduction:</strong> Keep components with signal, drop noisy ones</li>
                    <li><strong>Feature Extraction:</strong> Create new meaningful features</li>
                    <li><strong>Preprocessing:</strong> Reduce dimensionality before feeding to ML models</li>
                </ul>

                <div class="tip-box">
                    PCA works best when features are correlated. If features are independent, PCA won't help much. Also, always standardize before PCA!
                </div>
            </div>
        </section>

        <section id="tsne-umap" class="content-section">
            <div class="container">
                <h2>t-SNE and UMAP</h2>
                <p>Non-linear dimensionality reduction techniques specifically designed for visualization. They preserve local structure better than PCA.</p>

                <div class="concept-grid">
                    <div class="concept-card">
                        <h4>t-SNE</h4>
                        <p><strong>t-distributed Stochastic Neighbor Embedding</strong></p>
                        <ul>
                            <li>Preserves local neighborhoods</li>
                            <li>Great for visualization</li>
                            <li>Computationally expensive</li>
                            <li>Results vary with perplexity parameter</li>
                        </ul>
                    </div>
                    <div class="concept-card">
                        <h4>UMAP</h4>
                        <p><strong>Uniform Manifold Approximation and Projection</strong></p>
                        <ul>
                            <li>Faster than t-SNE</li>
                            <li>Preserves both local and global structure</li>
                            <li>More consistent results</li>
                            <li>Can be used for general dimensionality reduction</li>
                        </ul>
                    </div>
                </div>

                <pre><code># Python: t-SNE
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# t-SNE (for visualization)
tsne = TSNE(n_components=2, perplexity=30, random_state=42)
X_tsne = tsne.fit_transform(X_scaled)

plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='viridis')
plt.title('t-SNE Visualization')
plt.show()

# UMAP (need to install: pip install umap-learn)
import umap

reducer = umap.UMAP(n_components=2, random_state=42)
X_umap = reducer.fit_transform(X_scaled)

plt.scatter(X_umap[:, 0], X_umap[:, 1], c=labels, cmap='viridis')
plt.title('UMAP Visualization')
plt.show()</code></pre>

                <h3>When to Use What?</h3>
                <table class="comparison-table">
                    <thead>
                        <tr>
                            <th>Method</th>
                            <th>Best For</th>
                            <th>Limitations</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>PCA</strong></td>
                            <td>Feature reduction, linear relationships, preprocessing</td>
                            <td>Linear only, may miss complex patterns</td>
                        </tr>
                        <tr>
                            <td><strong>t-SNE</strong></td>
                            <td>Visualizing clusters, exploring data</td>
                            <td>Slow, not for preprocessing, non-deterministic</td>
                        </tr>
                        <tr>
                            <td><strong>UMAP</strong></td>
                            <td>Visualization + preprocessing, large datasets</td>
                            <td>Requires hyperparameter tuning</td>
                        </tr>
                    </tbody>
                </table>

                <div class="warning-box">
                    t-SNE and UMAP are for visualization and exploration. Don't interpret distances between clusters as meaningful‚Äîonly within-cluster relationships are preserved reliably.
                </div>
            </div>
        </section>

        <section id="evaluation" class="content-section">
            <div class="container">
                <h2>Evaluating Clustering</h2>
                <p>Without labels, how do we know if our clustering is good? We use internal metrics that measure cluster quality.</p>

                <div class="concept-grid">
                    <div class="concept-card">
                        <h4>Silhouette Score</h4>
                        <p>Measures how similar points are to their own cluster vs other clusters.</p>
                        <div class="formula">
                            s(i) = (b(i) - a(i)) / max(a(i), b(i))
                        </div>
                        <p>Range: -1 to 1 (higher is better)</p>
                    </div>
                    <div class="concept-card">
                        <h4>Davies-Bouldin Index</h4>
                        <p>Ratio of within-cluster distances to between-cluster distances.</p>
                        <p>Lower is better (0 = perfect clustering)</p>
                    </div>
                    <div class="concept-card">
                        <h4>Calinski-Harabasz Index</h4>
                        <p>Ratio of between-cluster dispersion to within-cluster dispersion.</p>
                        <p>Higher is better</p>
                    </div>
                </div>

                <pre><code># Python: Clustering Evaluation
from sklearn.metrics import silhouette_score, davies_bouldin_score
from sklearn.metrics import calinski_harabasz_score

# Assuming 'clusters' contains cluster labels
print(f"Silhouette Score: {silhouette_score(X, clusters):.3f}")
print(f"Davies-Bouldin Index: {davies_bouldin_score(X, clusters):.3f}")
print(f"Calinski-Harabasz: {calinski_harabasz_score(X, clusters):.3f}")

# Compare different K values
for k in range(2, 8):
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X)
    score = silhouette_score(X, labels)
    print(f"K={k}: Silhouette = {score:.3f}")</code></pre>
            </div>
        </section>

        <section id="quiz" class="content-section">
            <div class="container">
                <h2>MCQ Practice - Unsupervised Learning</h2>
                <p>Practice with these NOAI-style multiple choice questions covering all unsupervised learning topics.</p>

                <!-- K-Means Clustering -->
                <h3>K-Means Clustering</h3>

                <div class="quiz-container" data-correct="b" data-explanation="K-Means requires you to specify the number of clusters (K) beforehand, unlike algorithms like DBSCAN that discover it automatically.">
                    <p class="quiz-question">1. Which clustering algorithm automatically determines the number of clusters?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) K-Means</span></div>
                        <div class="quiz-option" data-value="b"><span>B) DBSCAN</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Both</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Neither</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="The elbow method plots WCSS vs K and looks for the 'elbow' point where adding more clusters gives diminishing returns.">
                    <p class="quiz-question">2. What is the purpose of the elbow method in K-Means clustering?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) To initialize centroids optimally</span></div>
                        <div class="quiz-option" data-value="b"><span>B) To speed up convergence</span></div>
                        <div class="quiz-option" data-value="c"><span>C) To determine the optimal number of clusters</span></div>
                        <div class="quiz-option" data-value="d"><span>D) To evaluate cluster quality</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="K-Means uses Euclidean distance by default, which measures the straight-line distance between points.">
                    <p class="quiz-question">3. Which distance metric is typically used in K-Means clustering?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Euclidean distance</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Cosine similarity</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Hamming distance</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Jaccard distance</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="K-Means++ initializes centroids by choosing points that are far apart, reducing the chance of poor convergence.">
                    <p class="quiz-question">4. What problem does K-Means++ initialization solve?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Slow convergence</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Poor results due to random centroid initialization</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Sensitivity to outliers</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Non-spherical cluster shapes</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="K-Means converges when centroids stop moving (or move less than a threshold) between iterations.">
                    <p class="quiz-question">5. K-Means algorithm converges when:</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) All points are assigned to clusters</span></div>
                        <div class="quiz-option" data-value="b"><span>B) A fixed number of iterations is reached</span></div>
                        <div class="quiz-option" data-value="c"><span>C) The loss reaches zero</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Centroids stop changing between iterations</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <!-- Hierarchical Clustering -->
                <h3>Hierarchical Clustering</h3>

                <div class="quiz-container" data-correct="b" data-explanation="Agglomerative (bottom-up) starts with each point as its own cluster and merges the closest pairs until one cluster remains.">
                    <p class="quiz-question">6. In agglomerative hierarchical clustering, the algorithm starts by:</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Placing all points in one cluster</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Treating each point as its own cluster</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Randomly assigning points to K clusters</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Finding the centroid of all data</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="A dendrogram is a tree diagram showing the hierarchical relationship between clusters at different levels.">
                    <p class="quiz-question">7. What is a dendrogram in hierarchical clustering?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) A scatter plot of clusters</span></div>
                        <div class="quiz-option" data-value="b"><span>B) A distance matrix</span></div>
                        <div class="quiz-option" data-value="c"><span>C) A tree diagram showing cluster hierarchy</span></div>
                        <div class="quiz-option" data-value="d"><span>D) A bar chart of cluster sizes</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="Ward's method minimizes the variance within clusters when merging, often producing the most compact and well-separated clusters.">
                    <p class="quiz-question">8. Which linkage method typically produces the most compact clusters?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Ward's method</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Single linkage</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Average linkage</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Complete linkage</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <!-- DBSCAN -->
                <h3>DBSCAN</h3>

                <div class="quiz-container" data-correct="b" data-explanation="A core point in DBSCAN has at least MinPts neighbors within epsilon distance, making it part of a dense region.">
                    <p class="quiz-question">9. In DBSCAN, a point is classified as a core point if:</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) It is at the center of a cluster</span></div>
                        <div class="quiz-option" data-value="b"><span>B) It has at least MinPts points within epsilon distance</span></div>
                        <div class="quiz-option" data-value="c"><span>C) It is furthest from other clusters</span></div>
                        <div class="quiz-option" data-value="d"><span>D) It was visited first during the algorithm</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="DBSCAN labels points that don't belong to any cluster as noise (label = -1), which is a key advantage for handling outliers.">
                    <p class="quiz-question">10. How does DBSCAN handle outliers?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Assigns them to the nearest cluster</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Creates a separate cluster for outliers</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Labels them as noise (-1)</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Removes them from the dataset</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="DBSCAN uses density-based clustering, allowing it to find clusters of arbitrary shapes, unlike K-Means which assumes spherical clusters.">
                    <p class="quiz-question">11. What is the main advantage of DBSCAN over K-Means?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Can find clusters of arbitrary shapes</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Faster computation time</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Works better with high-dimensional data</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Requires fewer parameters</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="Border points are within epsilon of a core point but don't have MinPts neighbors themselves. They're assigned to the cluster of their nearest core point.">
                    <p class="quiz-question">12. In DBSCAN, a border point is:</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) A point on the edge of the dataset</span></div>
                        <div class="quiz-option" data-value="b"><span>B) A point that is labeled as noise</span></div>
                        <div class="quiz-option" data-value="c"><span>C) A point with the maximum neighbors</span></div>
                        <div class="quiz-option" data-value="d"><span>D) A point within epsilon of a core point but not a core point itself</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <!-- PCA -->
                <h3>Principal Component Analysis (PCA)</h3>

                <div class="quiz-container" data-correct="c" data-explanation="PCA finds the direction of maximum variance first (PC1), then the next orthogonal direction with maximum remaining variance (PC2), and so on.">
                    <p class="quiz-question">13. What does the first principal component (PC1) capture?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) The smallest variance in the data</span></div>
                        <div class="quiz-option" data-value="b"><span>B) The mean of all features</span></div>
                        <div class="quiz-option" data-value="c"><span>C) The direction of maximum variance</span></div>
                        <div class="quiz-option" data-value="d"><span>D) The most important feature</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="Principal components are orthogonal (perpendicular) to each other, meaning they are uncorrelated.">
                    <p class="quiz-question">14. Principal components in PCA are:</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Parallel to each other</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Orthogonal (perpendicular) to each other</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Random directions</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Aligned with original features</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="PCA requires standardization because it's based on variance. Features with larger scales would dominate without standardization.">
                    <p class="quiz-question">15. Why is it important to standardize data before applying PCA?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) PCA is sensitive to feature scales; larger scales dominate variance</span></div>
                        <div class="quiz-option" data-value="b"><span>B) It speeds up the computation</span></div>
                        <div class="quiz-option" data-value="c"><span>C) PCA requires normally distributed data</span></div>
                        <div class="quiz-option" data-value="d"><span>D) It prevents overfitting</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="Eigenvalues represent the amount of variance explained by each principal component. Larger eigenvalues mean more variance captured.">
                    <p class="quiz-question">16. In PCA, eigenvalues represent:</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) The direction of principal components</span></div>
                        <div class="quiz-option" data-value="b"><span>B) The number of features to keep</span></div>
                        <div class="quiz-option" data-value="c"><span>C) The amount of variance explained by each component</span></div>
                        <div class="quiz-option" data-value="d"><span>D) The importance of original features</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <!-- t-SNE and UMAP -->
                <h3>t-SNE and UMAP</h3>

                <div class="quiz-container" data-correct="b" data-explanation="t-SNE is primarily designed for visualization in 2D or 3D, preserving local neighborhood structure.">
                    <p class="quiz-question">17. t-SNE is primarily used for:</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Feature selection</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Data visualization in 2D or 3D</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Clustering</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Classification</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="The perplexity parameter in t-SNE balances attention between local and global aspects, roughly controlling the effective number of neighbors.">
                    <p class="quiz-question">18. In t-SNE, the perplexity parameter controls:</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) The number of dimensions in output</span></div>
                        <div class="quiz-option" data-value="b"><span>B) The learning rate</span></div>
                        <div class="quiz-option" data-value="c"><span>C) The number of iterations</span></div>
                        <div class="quiz-option" data-value="d"><span>D) The effective number of neighbors considered</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="UMAP is generally faster than t-SNE, especially on large datasets, while producing similar quality visualizations.">
                    <p class="quiz-question">19. Compared to t-SNE, UMAP is typically:</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Faster and preserves more global structure</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Slower but more accurate</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Only suitable for small datasets</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Better for supervised learning</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <!-- Clustering Evaluation -->
                <h3>Clustering Evaluation</h3>

                <div class="quiz-container" data-correct="a" data-explanation="A silhouette score close to 1 indicates well-defined clusters where points are close to their own cluster and far from others.">
                    <p class="quiz-question">20. A silhouette score of 0.85 indicates:</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Good clustering with well-separated clusters</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Poor clustering</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Overlapping clusters</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Too many clusters</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="The silhouette score ranges from -1 to 1, where 1 is best, 0 indicates overlapping clusters, and -1 indicates wrong clustering.">
                    <p class="quiz-question">21. The silhouette score ranges from:</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) 0 to 1</span></div>
                        <div class="quiz-option" data-value="b"><span>B) 0 to 100</span></div>
                        <div class="quiz-option" data-value="c"><span>C) -1 to 1</span></div>
                        <div class="quiz-option" data-value="d"><span>D) -‚àû to +‚àû</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="Davies-Bouldin Index measures the average similarity between clusters. Lower values indicate better clustering (0 is perfect).">
                    <p class="quiz-question">22. For the Davies-Bouldin Index, a value closer to 0 indicates:</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Poor clustering</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Better clustering</span></div>
                        <div class="quiz-option" data-value="c"><span>C) More clusters needed</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Overfitting</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="WCSS (Within-Cluster Sum of Squares) measures the total distance of points from their cluster centroids. Lower = tighter clusters.">
                    <p class="quiz-question">23. What does WCSS (inertia) measure in clustering?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Distance between cluster centroids</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Number of points per cluster</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Cluster shape regularity</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Sum of squared distances from points to their centroids</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <!-- Applications -->
                <h3>Applications</h3>

                <div class="quiz-container" data-correct="c" data-explanation="Customer segmentation is a classic use case for clustering, grouping customers by similar behaviors or characteristics.">
                    <p class="quiz-question">24. Which unsupervised learning task is most suitable for grouping customers by purchasing behavior?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) PCA</span></div>
                        <div class="quiz-option" data-value="b"><span>B) t-SNE</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Clustering (K-Means or DBSCAN)</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Association rules</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="PCA is commonly used for noise reduction by keeping only the top principal components that capture the signal and discarding the rest.">
                    <p class="quiz-question">25. PCA can be used for noise reduction by:</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Keeping only components with high eigenvalues</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Removing all outliers first</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Increasing the number of features</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Applying clustering before PCA</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h3>Additional Unsupervised Learning Practice</h3>

                <h4>Advanced Clustering Concepts</h4>

                <div class="quiz-container" data-correct="c" data-explanation="K-Means minimizes within-cluster sum of squares (WCSS/inertia), which is the sum of squared distances from each point to its assigned centroid.">
                    <p class="quiz-question">26. What objective function does K-Means optimize?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Maximize between-cluster distance</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Minimize number of clusters</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Minimize within-cluster sum of squared distances</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Maximize silhouette score</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="Running K-Means multiple times with different initializations and selecting the best result (lowest inertia) helps avoid poor local minima.">
                    <p class="quiz-question">27. [NOAI Style] Why should K-Means be run multiple times with different initializations?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) To get more clusters</span></div>
                        <div class="quiz-option" data-value="b"><span>B) To avoid converging to a poor local minimum</span></div>
                        <div class="quiz-option" data-value="c"><span>C) To speed up computation</span></div>
                        <div class="quiz-option" data-value="d"><span>D) To handle outliers better</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="Mini-batch K-Means uses small random batches to update centroids, making it much faster for large datasets while achieving similar results.">
                    <p class="quiz-question">28. What is the advantage of Mini-batch K-Means over standard K-Means?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Finds more accurate clusters</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Handles non-spherical clusters better</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Automatically determines K</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Much faster on large datasets</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Hierarchical Clustering Extended</h4>

                <div class="quiz-container" data-correct="a" data-explanation="Single linkage uses minimum distance between clusters, which can create elongated chain-like clusters (chaining effect) when clusters are not well-separated.">
                    <p class="quiz-question">29. What is the 'chaining effect' in single linkage clustering?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Points form long chains connecting distant clusters</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Clusters become too compact</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Dendrograms become too deep</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Algorithm runs in an infinite loop</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="Divisive (top-down) starts with all data in one cluster and recursively splits, while agglomerative (bottom-up) starts with each point as its own cluster and merges.">
                    <p class="quiz-question">30. [NOAI Style] How does divisive hierarchical clustering differ from agglomerative?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Divisive is faster</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Divisive uses different distance metrics</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Divisive starts with one cluster and splits; agglomerative starts with N and merges</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Divisive requires specifying K; agglomerative doesn't</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="Cutting the dendrogram at different heights produces different numbers of clusters, allowing flexible choice of granularity without rerunning the algorithm.">
                    <p class="quiz-question">31. Why is the dendrogram useful in hierarchical clustering?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) It speeds up computation</span></div>
                        <div class="quiz-option" data-value="b"><span>B) It allows choosing different numbers of clusters without rerunning</span></div>
                        <div class="quiz-option" data-value="c"><span>C) It identifies outliers automatically</span></div>
                        <div class="quiz-option" data-value="d"><span>D) It works better for high-dimensional data</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>DBSCAN and Density-Based Methods</h4>

                <div class="quiz-container" data-correct="d" data-explanation="HDBSCAN (Hierarchical DBSCAN) can handle varying density clusters and doesn't require specifying epsilon, making it more robust than standard DBSCAN.">
                    <p class="quiz-question">32. What improvement does HDBSCAN offer over standard DBSCAN?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Faster computation</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Better handling of high dimensions</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Guaranteed optimal clusters</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Handles clusters of varying densities without specifying epsilon</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="Using k-distance graphs (plotting sorted distances to k-th nearest neighbor) helps identify the 'elbow' which suggests a good epsilon value for DBSCAN.">
                    <p class="quiz-question">33. How can you choose a good epsilon value for DBSCAN?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Plot k-distance graph and find the elbow point</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Always use epsilon = 0.5</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Set epsilon equal to the data standard deviation</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Use cross-validation on labeled data</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="DBSCAN struggles with varying density clusters because a single epsilon/MinPts cannot capture both dense and sparse regions effectively.">
                    <p class="quiz-question">34. [NOAI Style] When might DBSCAN fail to find good clusters?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) When clusters are spherical</span></div>
                        <div class="quiz-option" data-value="b"><span>B) When the dataset is small</span></div>
                        <div class="quiz-option" data-value="c"><span>C) When clusters have very different densities</span></div>
                        <div class="quiz-option" data-value="d"><span>D) When there are no outliers</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Gaussian Mixture Models</h4>

                <div class="quiz-container" data-correct="b" data-explanation="GMM uses soft assignment where each point has a probability of belonging to each cluster, unlike K-Means which uses hard assignment (each point belongs to exactly one cluster).">
                    <p class="quiz-question">35. What type of cluster assignment does Gaussian Mixture Model (GMM) use?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Hard assignment (point belongs to one cluster)</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Soft assignment (probability of belonging to each cluster)</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Density-based assignment</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Hierarchical assignment</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="GMM uses Expectation-Maximization (EM) algorithm, alternating between estimating cluster assignments (E-step) and updating parameters (M-step).">
                    <p class="quiz-question">36. What algorithm is used to train Gaussian Mixture Models?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Expectation-Maximization (EM)</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Gradient Descent</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Lloyd's Algorithm</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Agglomerative Merging</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="GMM can model elliptical clusters of different sizes and orientations because each component has its own covariance matrix, unlike K-Means which assumes spherical clusters.">
                    <p class="quiz-question">37. Why might GMM produce better clusters than K-Means for elongated data?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) GMM is faster to train</span></div>
                        <div class="quiz-option" data-value="b"><span>B) GMM handles outliers better</span></div>
                        <div class="quiz-option" data-value="c"><span>C) GMM doesn't require specifying K</span></div>
                        <div class="quiz-option" data-value="d"><span>D) GMM can model elliptical clusters with different orientations</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Advanced PCA Concepts</h4>

                <div class="quiz-container" data-correct="c" data-explanation="PCA loadings show how much each original feature contributes to each principal component. Large loadings indicate features that are important for that component.">
                    <p class="quiz-question">38. What do PCA loadings represent?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) The transformed data points</span></div>
                        <div class="quiz-option" data-value="b"><span>B) The variance explained by each component</span></div>
                        <div class="quiz-option" data-value="c"><span>C) The contribution of each original feature to each principal component</span></div>
                        <div class="quiz-option" data-value="d"><span>D) The number of components to keep</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="PCA can only capture linear relationships. For non-linear patterns, techniques like kernel PCA, autoencoders, or t-SNE/UMAP are needed.">
                    <p class="quiz-question">39. [NOAI Style] What is a major limitation of standard PCA?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Cannot handle missing values</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Can only capture linear relationships</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Requires labeled data</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Cannot work with more than 100 features</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="Reconstruction error measures how well the reduced representation can recreate the original data. Lower error means the principal components captured the important information.">
                    <p class="quiz-question">40. What does reconstruction error in PCA measure?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Difference between original data and data reconstructed from principal components</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Error in computing eigenvalues</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Classification accuracy after PCA</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Numerical precision of the algorithm</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Autoencoders for Dimensionality Reduction</h4>

                <div class="quiz-container" data-correct="c" data-explanation="Autoencoders learn a compressed representation (encoding) by training a neural network to reconstruct its input through a bottleneck layer.">
                    <p class="quiz-question">41. How do autoencoders achieve dimensionality reduction?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) By selecting the most important features</span></div>
                        <div class="quiz-option" data-value="b"><span>B) By computing eigenvectors of the data</span></div>
                        <div class="quiz-option" data-value="c"><span>C) By learning to compress and reconstruct data through a bottleneck</span></div>
                        <div class="quiz-option" data-value="d"><span>D) By clustering similar data points</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="Unlike PCA, autoencoders can learn non-linear representations through their non-linear activation functions, capturing more complex patterns in data.">
                    <p class="quiz-question">42. [NOAI Style] What advantage do autoencoders have over PCA for dimensionality reduction?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Faster training time</span></div>
                        <div class="quiz-option" data-value="b"><span>B) No hyperparameters to tune</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Guaranteed optimal solution</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Can learn non-linear representations</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Anomaly Detection</h4>

                <div class="quiz-container" data-correct="b" data-explanation="Isolation Forest isolates anomalies by randomly selecting features and split values. Anomalies are easier to isolate (require fewer splits) because they are different from normal points.">
                    <p class="quiz-question">43. How does Isolation Forest detect anomalies?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) By finding points far from cluster centers</span></div>
                        <div class="quiz-option" data-value="b"><span>B) By isolating points with fewer random splits (anomalies are easier to isolate)</span></div>
                        <div class="quiz-option" data-value="c"><span>C) By computing distance to nearest neighbors</span></div>
                        <div class="quiz-option" data-value="d"><span>D) By fitting a Gaussian distribution to the data</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="One-Class SVM learns a boundary around normal data during training, then classifies points outside this boundary as anomalies during testing.">
                    <p class="quiz-question">44. What is the key idea behind One-Class SVM for anomaly detection?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Learn a boundary enclosing normal data; points outside are anomalies</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Train on both normal and anomalous examples</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Use clustering to identify small clusters as anomalies</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Apply PCA and flag high reconstruction error as anomalies</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="Points with high reconstruction error when using PCA (or autoencoders) are likely anomalies because they don't fit the patterns learned from normal data.">
                    <p class="quiz-question">45. How can PCA be used for anomaly detection?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) By clustering in the reduced space</span></div>
                        <div class="quiz-option" data-value="b"><span>B) By examining the principal component values</span></div>
                        <div class="quiz-option" data-value="c"><span>C) By flagging points with high reconstruction error</span></div>
                        <div class="quiz-option" data-value="d"><span>D) PCA cannot be used for anomaly detection</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Association Rules</h4>

                <div class="quiz-container" data-correct="d" data-explanation="Support measures how frequently an itemset appears in the dataset. Support(A) = (Transactions containing A) / (Total transactions).">
                    <p class="quiz-question">46. In association rule mining, what does 'support' measure?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) How strongly items are correlated</span></div>
                        <div class="quiz-option" data-value="b"><span>B) The profitability of an itemset</span></div>
                        <div class="quiz-option" data-value="c"><span>C) How much one item increases likelihood of another</span></div>
                        <div class="quiz-option" data-value="d"><span>D) How frequently an itemset appears in transactions</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="Confidence(A‚ÜíB) = Support(A‚à™B) / Support(A), measuring how often B appears when A is present.">
                    <p class="quiz-question">47. [NOAI Style] What does confidence measure in the rule A ‚Üí B?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) How often A and B appear together</span></div>
                        <div class="quiz-option" data-value="b"><span>B) The probability of B given that A is present</span></div>
                        <div class="quiz-option" data-value="c"><span>C) How much A increases the likelihood of B compared to random</span></div>
                        <div class="quiz-option" data-value="d"><span>D) The correlation between A and B</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="Lift > 1 indicates positive association (buying A increases likelihood of B), lift = 1 indicates independence, lift < 1 indicates negative association.">
                    <p class="quiz-question">48. If lift(A ‚Üí B) = 2.5, what does this indicate?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Buying A makes buying B 2.5 times more likely than random</span></div>
                        <div class="quiz-option" data-value="b"><span>B) A and B appear together in 2.5% of transactions</span></div>
                        <div class="quiz-option" data-value="c"><span>C) A and B are negatively correlated</span></div>
                        <div class="quiz-option" data-value="d"><span>D) The rule has 25% confidence</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Advanced Evaluation and Comparison</h4>

                <div class="quiz-container" data-correct="c" data-explanation="The Calinski-Harabasz Index (Variance Ratio Criterion) measures the ratio of between-cluster variance to within-cluster variance. Higher values indicate better-defined clusters.">
                    <p class="quiz-question">49. What does the Calinski-Harabasz Index measure?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Overlap between clusters</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Density of clusters</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Ratio of between-cluster to within-cluster variance</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Number of optimal clusters</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="Adjusted Rand Index (ARI) compares clustering results to ground truth labels, accounting for chance. ARI = 1 means perfect agreement, 0 means random clustering.">
                    <p class="quiz-question">50. [NOAI Style] When would you use the Adjusted Rand Index (ARI)?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) When you don't have true labels</span></div>
                        <div class="quiz-option" data-value="b"><span>B) When comparing clustering results to known ground truth</span></div>
                        <div class="quiz-option" data-value="c"><span>C) When evaluating PCA quality</span></div>
                        <div class="quiz-option" data-value="d"><span>D) When tuning DBSCAN parameters</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Practical Considerations</h4>

                <div class="quiz-container" data-correct="d" data-explanation="BIC (Bayesian Information Criterion) penalizes model complexity, helping select the optimal number of components in GMM that balances fit and simplicity.">
                    <p class="quiz-question">51. Which criterion is commonly used to select the number of components in GMM?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Silhouette score</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Elbow method</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Cross-validation accuracy</span></div>
                        <div class="quiz-option" data-value="d"><span>D) BIC (Bayesian Information Criterion)</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="Spectral clustering first transforms data using eigenvalues of similarity matrix, then applies K-Means in this space. It can find non-convex clusters that K-Means cannot.">
                    <p class="quiz-question">52. What is spectral clustering?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Uses eigenvalues of similarity matrix, then K-Means on transformed data</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Clusters based on spectral (frequency) features</span></div>
                        <div class="quiz-option" data-value="c"><span>C) A faster version of hierarchical clustering</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Clustering in the PCA-reduced space</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="The curse of dimensionality makes distance metrics less meaningful in high dimensions, causing clustering algorithms to struggle. Dimensionality reduction before clustering often helps.">
                    <p class="quiz-question">53. Why might clustering fail on very high-dimensional data?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Algorithms cannot process many features</span></div>
                        <div class="quiz-option" data-value="b"><span>B) There are too many possible clusters</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Distances become less meaningful in high dimensions</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Visualization becomes impossible</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="Kernel PCA applies the kernel trick to PCA, allowing it to capture non-linear relationships by implicitly mapping data to a higher-dimensional space.">
                    <p class="quiz-question">54. [NOAI Style] What does Kernel PCA add to standard PCA?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Faster computation</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Ability to capture non-linear relationships</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Automatic component selection</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Handling of missing values</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="The Gap statistic compares within-cluster dispersion to expected dispersion under a null reference distribution, helping determine when adding clusters no longer provides significant improvement.">
                    <p class="quiz-question">55. What is the Gap statistic used for in clustering?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Determining optimal number of clusters by comparing to null reference</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Measuring cluster density</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Evaluating cluster shape</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Detecting outliers in clusters</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>
            </div>
        </section>

        <div class="container">
            <nav class="page-navigation">
                <a href="supervised.html" class="page-nav-link prev">
                    <span class="page-nav-label">‚Üê Previous</span>
                    <span class="page-nav-title">Supervised Learning</span>
                </a>
                <a href="neural-networks.html" class="page-nav-link next">
                    <span class="page-nav-label">Next ‚Üí</span>
                    <span class="page-nav-title">Neural Networks</span>
                </a>
            </nav>
        </div>
    </main>

    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-section">
                    <h4>ML for NOAI</h4>
                    <p>An educational resource for Singapore secondary school students preparing for the National Olympiad in AI.</p>
                </div>
                <div class="footer-section">
                    <h4>Quick Links</h4>
                    <ul>
                        <li><a href="fundamentals.html">ML Fundamentals</a></li>
                        <li><a href="supervised.html">Supervised Learning</a></li>
                        <li><a href="neural-networks.html">Neural Networks</a></li>
                    </ul>
                </div>
                <div class="footer-section">
                    <h4>References</h4>
                    <ul>
                        <li><a href="https://ioai-official.org/" target="_blank">IOAI Official</a></li>
                        <li><a href="https://aisingapore.org/" target="_blank">AI Singapore</a></li>
                    </ul>
                </div>
            </div>
            <div class="footer-bottom">
                <p>Educational content aligned with IOAI Syllabus. Not affiliated with AI Singapore or IOAI.</p>
            </div>
        </div>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>
