<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Supervised Learning - ML for NOAI</title>
    <link rel="stylesheet" href="../css/style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Fira+Code&display=swap" rel="stylesheet">
</head>
<body>
    <nav class="navbar">
        <div class="nav-container">
            <a href="../index.html" class="nav-logo">
                <span class="logo-icon">ü§ñ</span>
                <span>ML for NOAI</span>
            </a>
            <button class="nav-toggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            <ul class="nav-menu">
                <li><a href="../index.html" class="nav-link">Home</a></li>
                <li><a href="fundamentals.html" class="nav-link">Fundamentals</a></li>
                <li><a href="supervised.html" class="nav-link active">Supervised Learning</a></li>
                <li><a href="unsupervised.html" class="nav-link">Unsupervised Learning</a></li>
                <li><a href="neural-networks.html" class="nav-link">Neural Networks</a></li>
                <li><a href="computer-vision.html" class="nav-link">Computer Vision</a></li>
                <li><a href="nlp.html" class="nav-link">NLP</a></li>
            </ul>
        </div>
    </nav>

    <div class="breadcrumb">
        <div class="container">
            <ul>
                <li><a href="../index.html">Home</a></li>
                <li>Supervised Learning</li>
            </ul>
        </div>
    </div>

    <header class="page-header">
        <div class="container">
            <h1>üéì Supervised Learning</h1>
            <p class="subtitle">Learn to build models that predict outcomes from labeled data‚Äîthe most common type of machine learning.</p>
        </div>
    </header>

    <div class="progress-container">
        <div class="container">
            <div class="progress-bar">
                <div class="progress-fill"></div>
            </div>
        </div>
    </div>

    <main>
        <section id="overview" class="content-section">
            <div class="container">
                <h2>What is Supervised Learning?</h2>
                <p>In supervised learning, we train models using <strong>labeled data</strong>‚Äîexamples where we know the correct answer. The model learns to map inputs (features) to outputs (labels) and can then make predictions on new, unseen data.</p>

                <div class="concept-grid">
                    <div class="concept-card">
                        <h4>üìà Regression</h4>
                        <p>Predicting <strong>continuous values</strong></p>
                        <p><em>Examples: house prices, temperature, stock prices, age</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>üè∑Ô∏è Classification</h4>
                        <p>Predicting <strong>discrete categories</strong></p>
                        <p><em>Examples: spam/not spam, cat/dog, disease diagnosis</em></p>
                    </div>
                </div>
            </div>
        </section>

        <section id="linear-regression" class="content-section">
            <div class="container">
                <h2>Linear Regression</h2>
                <p>The simplest regression algorithm. It finds the best straight line (or hyperplane) through the data.</p>

                <div class="algorithm-card">
                    <h3>Simple Linear Regression</h3>
                    <div class="algorithm-meta">
                        <span>üìä Type: Regression</span>
                        <span>üìö NOAI: Theory + Practice</span>
                    </div>

                    <div class="formula">
                        <div class="formula-title">Equation</div>
                        y = wx + b

                        where:
                        ‚Ä¢ y = predicted value
                        ‚Ä¢ x = input feature
                        ‚Ä¢ w = weight (slope)
                        ‚Ä¢ b = bias (y-intercept)
                    </div>

                    <h4>How It Works</h4>
                    <ol>
                        <li>Initialize weights randomly</li>
                        <li>Calculate predictions: ≈∑ = wx + b</li>
                        <li>Compute loss (Mean Squared Error)</li>
                        <li>Update weights to minimize loss using gradient descent</li>
                        <li>Repeat until convergence</li>
                    </ol>

                    <div class="formula">
                        <div class="formula-title">Mean Squared Error (MSE)</div>
                        MSE = (1/n) √ó Œ£(y_actual - y_predicted)¬≤
                    </div>
                </div>

                <h3>Multiple Linear Regression</h3>
                <p>When we have multiple input features:</p>
                <div class="formula">
                    y = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô + b
                </div>

                <pre><code># Python: Linear Regression
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Create and train model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate
print(f"Coefficients: {model.coef_}")
print(f"Intercept: {model.intercept_}")
print(f"MSE: {mean_squared_error(y_test, y_pred):.3f}")
print(f"R¬≤ Score: {r2_score(y_test, y_pred):.3f}")</code></pre>

                <div class="tip-box">
                    Linear regression assumes a linear relationship between features and target. If the relationship is non-linear, consider polynomial features or other algorithms.
                </div>
            </div>
        </section>

        <section id="logistic-regression" class="content-section">
            <div class="container">
                <h2>Logistic Regression</h2>
                <p>Despite its name, logistic regression is used for <strong>classification</strong>. It predicts the probability that an input belongs to a class.</p>

                <div class="algorithm-card">
                    <h3>Binary Classification</h3>
                    <div class="algorithm-meta">
                        <span>üè∑Ô∏è Type: Classification</span>
                        <span>üìö NOAI: Theory + Practice</span>
                    </div>

                    <div class="formula">
                        <div class="formula-title">Sigmoid Function</div>
                        œÉ(z) = 1 / (1 + e^(-z))

                        where z = wx + b

                        Output is between 0 and 1 (probability)
                    </div>

                    <h4>Decision Boundary</h4>
                    <p>If P(class=1) > 0.5, predict class 1; otherwise predict class 0.</p>

                    <div class="formula">
                        <div class="formula-title">Binary Cross-Entropy Loss</div>
                        Loss = -[y √ó log(≈∑) + (1-y) √ó log(1-≈∑)]
                    </div>
                </div>

                <pre><code># Python: Logistic Regression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Create and train model
model = LogisticRegression()
model.fit(X_train, y_train)

# Predict classes
y_pred = model.predict(X_test)

# Predict probabilities
y_prob = model.predict_proba(X_test)

# Evaluate
print(f"Accuracy: {accuracy_score(y_test, y_pred):.3f}")
print(classification_report(y_test, y_pred))</code></pre>

                <h3>Multi-class Classification</h3>
                <p>For more than two classes, logistic regression uses:</p>
                <ul>
                    <li><strong>One-vs-Rest (OvR):</strong> Train N binary classifiers, one for each class</li>
                    <li><strong>Softmax (Multinomial):</strong> Directly output probabilities for all classes</li>
                </ul>
            </div>
        </section>

        <section id="knn" class="content-section">
            <div class="container">
                <h2>K-Nearest Neighbors (KNN)</h2>
                <p>A simple, intuitive algorithm: classify points based on the majority class of their k nearest neighbors.</p>

                <div class="algorithm-card">
                    <h3>KNN Algorithm</h3>
                    <div class="algorithm-meta">
                        <span>üè∑Ô∏è Type: Classification / Regression</span>
                        <span>üìö NOAI: Theory + Practice</span>
                        <span>‚ö° Lazy Learner</span>
                    </div>

                    <h4>How It Works</h4>
                    <ol>
                        <li>Store all training data (no explicit training phase)</li>
                        <li>For a new point, calculate distance to all training points</li>
                        <li>Find the k nearest neighbors</li>
                        <li><strong>Classification:</strong> Vote by majority class</li>
                        <li><strong>Regression:</strong> Average of neighbors' values</li>
                    </ol>

                    <div class="formula">
                        <div class="formula-title">Euclidean Distance</div>
                        d(p, q) = ‚àö[(p‚ÇÅ-q‚ÇÅ)¬≤ + (p‚ÇÇ-q‚ÇÇ)¬≤ + ... + (p‚Çô-q‚Çô)¬≤]
                    </div>
                </div>

                <h3>Choosing K</h3>
                <ul>
                    <li><strong>Small k (e.g., 1-3):</strong> More sensitive to noise, can overfit</li>
                    <li><strong>Large k:</strong> Smoother boundaries, may underfit</li>
                    <li><strong>Odd k:</strong> Avoids ties in binary classification</li>
                    <li>Use cross-validation to find optimal k</li>
                </ul>

                <pre><code># Python: K-Nearest Neighbors
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler

# IMPORTANT: Scale features for KNN!
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Create and train model
model = KNeighborsClassifier(n_neighbors=5)
model.fit(X_train_scaled, y_train)

# Predict
y_pred = model.predict(X_test_scaled)
print(f"Accuracy: {accuracy_score(y_test, y_pred):.3f}")</code></pre>

                <div class="warning-box">
                    KNN is sensitive to feature scales. Always normalize or standardize your features before using KNN!
                </div>
            </div>
        </section>

        <section id="decision-trees" class="content-section">
            <div class="container">
                <h2>Decision Trees</h2>
                <p>Decision trees make predictions by learning simple decision rules from data features. They're intuitive and easy to interpret.</p>

                <div class="algorithm-card">
                    <h3>How Decision Trees Work</h3>
                    <div class="algorithm-meta">
                        <span>üè∑Ô∏è Type: Classification / Regression</span>
                        <span>üìö NOAI: Theory + Practice</span>
                        <span>üëÅÔ∏è Interpretable</span>
                    </div>

                    <h4>Building the Tree</h4>
                    <ol>
                        <li>Start with all data at the root</li>
                        <li>Find the best feature and threshold to split the data</li>
                        <li>Create child nodes for each split</li>
                        <li>Recursively repeat for each child</li>
                        <li>Stop when a stopping criterion is met (max depth, min samples, etc.)</li>
                    </ol>

                    <h4>Splitting Criteria</h4>
                    <p>The "best" split maximizes information gain (or minimizes impurity):</p>
                </div>

                <div class="concept-grid">
                    <div class="concept-card">
                        <h4>Gini Impurity</h4>
                        <div class="formula">
                            Gini = 1 - Œ£(p·µ¢)¬≤
                        </div>
                        <p>Probability of misclassifying a randomly chosen element. Range: 0 (pure) to 0.5 (binary)</p>
                    </div>
                    <div class="concept-card">
                        <h4>Entropy</h4>
                        <div class="formula">
                            Entropy = -Œ£ p·µ¢ √ó log‚ÇÇ(p·µ¢)
                        </div>
                        <p>Measure of disorder/uncertainty. Range: 0 (pure) to 1 (binary)</p>
                    </div>
                </div>

                <pre><code># Python: Decision Tree
from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt

# Create and train model
model = DecisionTreeClassifier(
    max_depth=5,           # Limit tree depth
    min_samples_split=10,  # Min samples to split a node
    min_samples_leaf=5     # Min samples in a leaf
)
model.fit(X_train, y_train)

# Visualize the tree
plt.figure(figsize=(20, 10))
plot_tree(model, feature_names=feature_names, filled=True)
plt.show()

# Feature importance
importance = dict(zip(feature_names, model.feature_importances_))
print("Feature Importance:", importance)</code></pre>

                <div class="note-box">
                    Decision trees are prone to overfitting. Control depth and use pruning, or better yet, use ensemble methods like Random Forest.
                </div>
            </div>
        </section>

        <section id="svm" class="content-section">
            <div class="container">
                <h2>Support Vector Machines (SVM)</h2>
                <p>SVM finds the optimal hyperplane that maximally separates classes with the largest margin.</p>

                <div class="algorithm-card">
                    <h3>SVM Concepts</h3>
                    <div class="algorithm-meta">
                        <span>üè∑Ô∏è Type: Classification / Regression</span>
                        <span>üìö NOAI: Theory + Practice</span>
                        <span>üí™ Works in High Dimensions</span>
                    </div>

                    <h4>Key Concepts</h4>
                    <ul>
                        <li><strong>Hyperplane:</strong> Decision boundary that separates classes</li>
                        <li><strong>Support Vectors:</strong> Data points closest to the hyperplane</li>
                        <li><strong>Margin:</strong> Distance between hyperplane and support vectors</li>
                        <li><strong>Goal:</strong> Maximize the margin</li>
                    </ul>
                </div>

                <h3>The Kernel Trick</h3>
                <p>For non-linearly separable data, SVM uses <strong>kernels</strong> to map data to higher dimensions where it becomes linearly separable:</p>

                <div class="concept-grid">
                    <div class="concept-card">
                        <h4>Linear Kernel</h4>
                        <p>K(x, y) = x ¬∑ y</p>
                        <p><em>Use when data is linearly separable</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>Polynomial Kernel</h4>
                        <p>K(x, y) = (x ¬∑ y + c)^d</p>
                        <p><em>Captures polynomial relationships</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>RBF (Gaussian) Kernel</h4>
                        <p>K(x, y) = exp(-Œ≥||x-y||¬≤)</p>
                        <p><em>Most common, works well in practice</em></p>
                    </div>
                </div>

                <h3>Hyperparameters</h3>
                <ul>
                    <li><strong>C (Regularization):</strong> Trade-off between smooth boundary and classifying training points correctly</li>
                    <li><strong>Œ≥ (Gamma):</strong> For RBF kernel, controls how far influence of single point reaches</li>
                </ul>

                <pre><code># Python: SVM
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler

# Scale features (important for SVM!)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Create and train model
model = SVC(kernel='rbf', C=1.0, gamma='scale')
model.fit(X_train_scaled, y_train)

# Predict
y_pred = model.predict(X_test_scaled)
print(f"Accuracy: {accuracy_score(y_test, y_pred):.3f}")

# Support vectors
print(f"Number of support vectors: {len(model.support_vectors_)}")</code></pre>
            </div>
        </section>

        <section id="ensemble" class="content-section">
            <div class="container">
                <h2>Ensemble Methods</h2>
                <p>Ensemble methods combine multiple models to achieve better performance than any single model.</p>

                <h3>Random Forest</h3>
                <div class="algorithm-card">
                    <h3>Bagging + Decision Trees</h3>
                    <div class="algorithm-meta">
                        <span>üè∑Ô∏è Type: Classification / Regression</span>
                        <span>üìö NOAI: Practice</span>
                        <span>üå≤ Ensemble of Trees</span>
                    </div>

                    <h4>How It Works</h4>
                    <ol>
                        <li>Create N bootstrap samples (random sampling with replacement)</li>
                        <li>Train a decision tree on each sample</li>
                        <li>At each split, consider only a random subset of features</li>
                        <li><strong>Classification:</strong> Majority vote across all trees</li>
                        <li><strong>Regression:</strong> Average predictions</li>
                    </ol>
                </div>

                <pre><code># Python: Random Forest
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(
    n_estimators=100,     # Number of trees
    max_depth=10,         # Max depth per tree
    min_samples_split=5,
    random_state=42
)
model.fit(X_train, y_train)

# Feature importance
importance = dict(zip(feature_names, model.feature_importances_))
print("Top features:", sorted(importance.items(), key=lambda x: -x[1])[:5])</code></pre>

                <h3>Gradient Boosting</h3>
                <div class="algorithm-card">
                    <h3>Sequential Learning</h3>
                    <div class="algorithm-meta">
                        <span>üè∑Ô∏è Type: Classification / Regression</span>
                        <span>üìö NOAI: Practice</span>
                        <span>üèÜ Often Wins Competitions</span>
                    </div>

                    <h4>How It Works</h4>
                    <ol>
                        <li>Train a weak model (shallow tree) on the data</li>
                        <li>Calculate the residuals (errors)</li>
                        <li>Train the next model to predict the residuals</li>
                        <li>Add this model to the ensemble (with a learning rate)</li>
                        <li>Repeat, each time fitting residuals of the ensemble so far</li>
                    </ol>
                </div>

                <div class="concept-grid">
                    <div class="concept-card">
                        <h4>XGBoost</h4>
                        <p>Extreme Gradient Boosting. Optimized, regularized gradient boosting. Industry standard.</p>
                        <code>from xgboost import XGBClassifier</code>
                    </div>
                    <div class="concept-card">
                        <h4>LightGBM</h4>
                        <p>Fast gradient boosting by Microsoft. Great for large datasets.</p>
                        <code>from lightgbm import LGBMClassifier</code>
                    </div>
                </div>

                <pre><code># Python: XGBoost
from xgboost import XGBClassifier

model = XGBClassifier(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    random_state=42
)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, y_pred):.3f}")</code></pre>

                <h3>Bagging vs Boosting</h3>
                <table class="comparison-table">
                    <thead>
                        <tr>
                            <th>Aspect</th>
                            <th>Bagging (Random Forest)</th>
                            <th>Boosting (XGBoost)</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Training</td>
                            <td>Parallel (independent trees)</td>
                            <td>Sequential (each learns from previous)</td>
                        </tr>
                        <tr>
                            <td>Focus</td>
                            <td>Reduces variance (overfitting)</td>
                            <td>Reduces bias (underfitting)</td>
                        </tr>
                        <tr>
                            <td>Base Models</td>
                            <td>Deep trees</td>
                            <td>Shallow trees (stumps)</td>
                        </tr>
                        <tr>
                            <td>Overfitting Risk</td>
                            <td>Lower</td>
                            <td>Higher (needs careful tuning)</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </section>

        <section id="regularization" class="content-section">
            <div class="container">
                <h2>Regularization: L1 and L2</h2>
                <p>Regularization prevents overfitting by penalizing complex models.</p>

                <div class="concept-grid">
                    <div class="concept-card">
                        <h4>L1 Regularization (Lasso)</h4>
                        <div class="formula">
                            Loss = MSE + Œª √ó Œ£|w·µ¢|
                        </div>
                        <p>Tends to create <strong>sparse models</strong> (some weights become exactly 0).</p>
                        <p><em>Use for: Feature selection, when you expect few important features</em></p>
                    </div>
                    <div class="concept-card">
                        <h4>L2 Regularization (Ridge)</h4>
                        <div class="formula">
                            Loss = MSE + Œª √ó Œ£w·µ¢¬≤
                        </div>
                        <p>Shrinks all weights toward zero but rarely exactly zero.</p>
                        <p><em>Use for: When all features may be relevant, handling multicollinearity</em></p>
                    </div>
                </div>

                <pre><code># Python: Regularized Regression
from sklearn.linear_model import Ridge, Lasso, ElasticNet

# Ridge (L2)
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)

# Lasso (L1)
lasso = Lasso(alpha=1.0)
lasso.fit(X_train, y_train)

# ElasticNet (L1 + L2)
elastic = ElasticNet(alpha=1.0, l1_ratio=0.5)
elastic.fit(X_train, y_train)

# Check which features Lasso eliminated
print("Non-zero features:", sum(lasso.coef_ != 0))</code></pre>
            </div>
        </section>

        <section id="comparison" class="content-section">
            <div class="container">
                <h2>Algorithm Comparison</h2>
                <p>When to use which algorithm? Here's a quick guide:</p>

                <table class="comparison-table">
                    <thead>
                        <tr>
                            <th>Algorithm</th>
                            <th>Strengths</th>
                            <th>Weaknesses</th>
                            <th>Best For</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Linear/Logistic Regression</strong></td>
                            <td>Simple, interpretable, fast</td>
                            <td>Assumes linearity</td>
                            <td>Baseline, linear relationships</td>
                        </tr>
                        <tr>
                            <td><strong>KNN</strong></td>
                            <td>Simple, no training, intuitive</td>
                            <td>Slow inference, sensitive to scale</td>
                            <td>Small datasets, quick prototyping</td>
                        </tr>
                        <tr>
                            <td><strong>Decision Tree</strong></td>
                            <td>Interpretable, handles non-linearity</td>
                            <td>Overfits easily</td>
                            <td>When interpretability is crucial</td>
                        </tr>
                        <tr>
                            <td><strong>SVM</strong></td>
                            <td>Works in high dimensions, effective with clear margins</td>
                            <td>Slow on large datasets, sensitive to scale</td>
                            <td>Medium-sized datasets, text classification</td>
                        </tr>
                        <tr>
                            <td><strong>Random Forest</strong></td>
                            <td>Robust, handles non-linearity, feature importance</td>
                            <td>Less interpretable, slower</td>
                            <td>General purpose, when accuracy matters</td>
                        </tr>
                        <tr>
                            <td><strong>XGBoost</strong></td>
                            <td>Often best performance, handles missing data</td>
                            <td>Many hyperparameters, can overfit</td>
                            <td>Competitions, tabular data</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </section>

        <section id="quiz" class="content-section">
            <div class="container">
                <h2>MCQ Practice - Supervised Learning</h2>
                <p>Practice with these NOAI-style multiple choice questions covering all supervised learning topics.</p>

                <!-- Linear Regression -->
                <h3>Linear Regression</h3>

                <div class="quiz-container" data-correct="b" data-explanation="MSE (Mean Squared Error) squares the errors, which penalizes larger errors more heavily and is commonly used in linear regression.">
                    <p class="quiz-question">1. In linear regression, the Mean Squared Error (MSE) loss function penalizes:</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) All errors equally regardless of magnitude</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Larger errors more heavily than smaller errors</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Smaller errors more heavily than larger errors</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Only errors above a certain threshold</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="R¬≤ (coefficient of determination) measures how much variance in the target variable is explained by the model. A value of 0.85 means 85% of variance is explained.">
                    <p class="quiz-question">2. An R¬≤ score of 0.85 in linear regression indicates that:</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) The model has 85% accuracy</span></div>
                        <div class="quiz-option" data-value="b"><span>B) 85% of predictions are correct</span></div>
                        <div class="quiz-option" data-value="c"><span>C) 85% of variance in the target is explained by the model</span></div>
                        <div class="quiz-option" data-value="d"><span>D) The model uses 85% of the features</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="In the equation y = wx + b, the intercept (b) represents the predicted value when all input features are zero.">
                    <p class="quiz-question">3. In the linear regression equation y = wx + b, what does the intercept (b) represent?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) The predicted y when x equals zero</span></div>
                        <div class="quiz-option" data-value="b"><span>B) The rate of change of y with respect to x</span></div>
                        <div class="quiz-option" data-value="c"><span>C) The maximum value of y</span></div>
                        <div class="quiz-option" data-value="d"><span>D) The error in the prediction</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <!-- Logistic Regression -->
                <h3>Logistic Regression</h3>

                <div class="quiz-container" data-correct="b" data-explanation="The sigmoid function outputs values between 0 and 1, representing probabilities.">
                    <p class="quiz-question">4. What is the output range of the sigmoid function in logistic regression?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) -1 to 1</span></div>
                        <div class="quiz-option" data-value="b"><span>B) 0 to 1</span></div>
                        <div class="quiz-option" data-value="c"><span>C) -‚àû to +‚àû</span></div>
                        <div class="quiz-option" data-value="d"><span>D) 0 to +‚àû</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="Despite its name, logistic regression is used for classification tasks, not regression. It predicts class probabilities.">
                    <p class="quiz-question">5. Why is logistic regression called "regression" even though it's used for classification?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) It can also be used for regression tasks</span></div>
                        <div class="quiz-option" data-value="b"><span>B) It regresses features to find patterns</span></div>
                        <div class="quiz-option" data-value="c"><span>C) The name is completely arbitrary</span></div>
                        <div class="quiz-option" data-value="d"><span>D) It models the log-odds as a linear regression</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="Binary cross-entropy loss is specifically designed for binary classification and measures the difference between predicted probabilities and actual labels.">
                    <p class="quiz-question">6. Which loss function is typically used for binary logistic regression?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Mean Squared Error</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Mean Absolute Error</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Binary Cross-Entropy</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Hinge Loss</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <!-- K-Nearest Neighbors -->
                <h3>K-Nearest Neighbors</h3>

                <div class="quiz-container" data-correct="c" data-explanation="KNN stores all training data and computes distances at prediction time, making it a lazy learner with no explicit training phase.">
                    <p class="quiz-question">7. Why is KNN called a "lazy learner"?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) It only uses a subset of training data</span></div>
                        <div class="quiz-option" data-value="b"><span>B) It trains very slowly</span></div>
                        <div class="quiz-option" data-value="c"><span>C) It does no work during training, all computation at prediction</span></div>
                        <div class="quiz-option" data-value="d"><span>D) It ignores some features</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="Feature scaling is crucial for KNN because it uses distance metrics. Without scaling, features with larger scales dominate the distance calculation.">
                    <p class="quiz-question">8. Why is feature scaling important for KNN?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) It speeds up training time</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Distance calculations are dominated by features with larger scales</span></div>
                        <div class="quiz-option" data-value="c"><span>C) It prevents overfitting</span></div>
                        <div class="quiz-option" data-value="d"><span>D) It reduces memory usage</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="Using an odd k in binary classification prevents ties when voting, ensuring a clear majority for one class.">
                    <p class="quiz-question">9. Why is it recommended to use an odd value of k in binary classification with KNN?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) To avoid ties when voting</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Odd numbers give better accuracy</span></div>
                        <div class="quiz-option" data-value="c"><span>C) It reduces computation time</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Even k values cause overfitting</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <!-- Decision Trees -->
                <h3>Decision Trees</h3>

                <div class="quiz-container" data-correct="b" data-explanation="Gini impurity measures the probability of incorrectly classifying a randomly chosen element. Lower Gini = purer node.">
                    <p class="quiz-question">10. In decision trees, what does Gini impurity measure?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) The depth of the tree</span></div>
                        <div class="quiz-option" data-value="b"><span>B) The probability of misclassifying a randomly chosen element</span></div>
                        <div class="quiz-option" data-value="c"><span>C) The number of features used</span></div>
                        <div class="quiz-option" data-value="d"><span>D) The size of each node</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="Information gain is the reduction in entropy after a split. Decision trees select splits that maximize information gain.">
                    <p class="quiz-question">11. What is information gain in the context of decision trees?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) The increase in accuracy after adding a feature</span></div>
                        <div class="quiz-option" data-value="b"><span>B) The amount of data used in training</span></div>
                        <div class="quiz-option" data-value="c"><span>C) The reduction in entropy after a split</span></div>
                        <div class="quiz-option" data-value="d"><span>D) The number of branches in the tree</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="Pruning reduces tree complexity by removing branches, which helps prevent overfitting and improves generalization.">
                    <p class="quiz-question">12. What is the purpose of pruning in decision trees?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) To increase the depth of the tree</span></div>
                        <div class="quiz-option" data-value="b"><span>B) To add more branches</span></div>
                        <div class="quiz-option" data-value="c"><span>C) To speed up training</span></div>
                        <div class="quiz-option" data-value="d"><span>D) To prevent overfitting by removing branches</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <!-- SVM -->
                <h3>Support Vector Machines</h3>

                <div class="quiz-container" data-correct="b" data-explanation="Support vectors are the data points closest to the decision boundary (hyperplane). They 'support' or define the margin.">
                    <p class="quiz-question">13. In SVM, what are support vectors?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) All training data points</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Data points closest to the decision boundary</span></div>
                        <div class="quiz-option" data-value="c"><span>C) The feature vectors used for training</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Outliers in the dataset</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="The kernel trick maps data to higher dimensions where it becomes linearly separable, without explicitly computing the transformation.">
                    <p class="quiz-question">14. What is the purpose of the kernel trick in SVM?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) To handle non-linearly separable data by mapping to higher dimensions</span></div>
                        <div class="quiz-option" data-value="b"><span>B) To reduce computational complexity</span></div>
                        <div class="quiz-option" data-value="c"><span>C) To normalize the input features</span></div>
                        <div class="quiz-option" data-value="d"><span>D) To prevent overfitting</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="The RBF (Radial Basis Function) kernel is the most commonly used kernel in SVM as it works well for a wide variety of problems.">
                    <p class="quiz-question">15. Which SVM kernel is most commonly used for general-purpose classification?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Linear kernel</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Polynomial kernel</span></div>
                        <div class="quiz-option" data-value="c"><span>C) RBF (Gaussian) kernel</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Sigmoid kernel</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <!-- Ensemble Methods -->
                <h3>Ensemble Methods</h3>

                <div class="quiz-container" data-correct="b" data-explanation="Bagging (Bootstrap Aggregating) trains models on random subsets of data sampled with replacement, then averages predictions. It reduces variance.">
                    <p class="quiz-question">16. What is the key characteristic of bagging ensemble methods?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Models are trained sequentially, each focusing on previous errors</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Models are trained in parallel on bootstrap samples</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Models share weights during training</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Only the best model is selected</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="Random Forest adds extra randomness by considering only a random subset of features at each split, in addition to bootstrap sampling.">
                    <p class="quiz-question">17. What makes Random Forest different from simple bagging of decision trees?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) It also randomly selects features at each split</span></div>
                        <div class="quiz-option" data-value="b"><span>B) It uses boosting instead of bagging</span></div>
                        <div class="quiz-option" data-value="c"><span>C) It only uses shallow trees</span></div>
                        <div class="quiz-option" data-value="d"><span>D) It weights trees by their accuracy</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="Boosting trains models sequentially, where each new model focuses on correcting the errors made by previous models.">
                    <p class="quiz-question">18. In gradient boosting, each subsequent tree is trained to predict:</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) The original target values</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Random subsets of the data</span></div>
                        <div class="quiz-option" data-value="c"><span>C) The residuals (errors) of the previous ensemble</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Feature importance scores</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="Bagging primarily reduces variance by averaging multiple models trained on different data samples. Boosting focuses on reducing bias.">
                    <p class="quiz-question">19. Bagging helps to reduce which type of error?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Bias</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Irreducible error</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Training error only</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Variance</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <!-- Regularization -->
                <h3>Regularization</h3>

                <div class="quiz-container" data-correct="a" data-explanation="L1 regularization can shrink weights to exactly zero, effectively selecting features and creating sparse models.">
                    <p class="quiz-question">20. Which regularization technique is useful for feature selection?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) L1 (Lasso)</span></div>
                        <div class="quiz-option" data-value="b"><span>B) L2 (Ridge)</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Both equally</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Neither</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="L2 (Ridge) regularization shrinks all weights toward zero but rarely makes them exactly zero. It's useful when all features may be relevant.">
                    <p class="quiz-question">21. L2 regularization (Ridge) adds which term to the loss function?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Sum of absolute values of weights</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Sum of squared weights</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Maximum weight value</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Number of non-zero weights</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="ElasticNet combines both L1 and L2 regularization, providing a balance between feature selection (L1) and weight shrinkage (L2).">
                    <p class="quiz-question">22. What is ElasticNet regularization?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) A stronger version of L1</span></div>
                        <div class="quiz-option" data-value="b"><span>B) A type of dropout for neural networks</span></div>
                        <div class="quiz-option" data-value="c"><span>C) A combination of L1 and L2 regularization</span></div>
                        <div class="quiz-option" data-value="d"><span>D) An alternative to cross-validation</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <!-- Model Selection -->
                <h3>Model Selection & Evaluation</h3>

                <div class="quiz-container" data-correct="b" data-explanation="Precision measures how many of the predicted positives are actually positive: TP / (TP + FP).">
                    <p class="quiz-question">23. In classification, precision is calculated as:</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) TP / (TP + FN)</span></div>
                        <div class="quiz-option" data-value="b"><span>B) TP / (TP + FP)</span></div>
                        <div class="quiz-option" data-value="c"><span>C) (TP + TN) / Total</span></div>
                        <div class="quiz-option" data-value="d"><span>D) TN / (TN + FP)</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="Recall (sensitivity) measures how many actual positives were correctly identified: TP / (TP + FN). High recall is important when missing positives is costly.">
                    <p class="quiz-question">24. When is high recall more important than high precision?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) When missing actual positives is costly (e.g., disease detection)</span></div>
                        <div class="quiz-option" data-value="b"><span>B) When false positives are very costly</span></div>
                        <div class="quiz-option" data-value="c"><span>C) When classes are perfectly balanced</span></div>
                        <div class="quiz-option" data-value="d"><span>D) When training data is limited</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="Cross-validation helps assess how well a model generalizes to unseen data by training and testing on different subsets of the data.">
                    <p class="quiz-question">25. The main purpose of k-fold cross-validation is to:</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Increase the size of training data</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Select the best features</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Speed up model training</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Get a reliable estimate of model performance on unseen data</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h3>Additional Supervised Learning Practice</h3>

                <h4>Advanced Linear Models</h4>

                <div class="quiz-container" data-correct="b" data-explanation="Multicollinearity occurs when independent variables are highly correlated with each other, making it difficult to determine individual variable effects and causing unstable coefficient estimates.">
                    <p class="quiz-question">26. What is multicollinearity in linear regression?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) When the target variable has multiple values</span></div>
                        <div class="quiz-option" data-value="b"><span>B) When independent variables are highly correlated with each other</span></div>
                        <div class="quiz-option" data-value="c"><span>C) When the model predicts multiple outputs</span></div>
                        <div class="quiz-option" data-value="d"><span>D) When residuals are correlated with predictions</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="Polynomial features create non-linear combinations of original features (x¬≤, x¬≥, x‚ÇÅ√óx‚ÇÇ), allowing linear models to capture non-linear relationships while still using linear regression.">
                    <p class="quiz-question">27. [NOAI Style] What is the purpose of adding polynomial features to linear regression?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) To reduce the number of features</span></div>
                        <div class="quiz-option" data-value="b"><span>B) To speed up training</span></div>
                        <div class="quiz-option" data-value="c"><span>C) To capture non-linear relationships while using linear methods</span></div>
                        <div class="quiz-option" data-value="d"><span>D) To handle missing values</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="Linear regression assumes: linear relationship between features and target, independence of errors, homoscedasticity (constant error variance), and normally distributed errors.">
                    <p class="quiz-question">28. Which is NOT an assumption of linear regression?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Features must be normally distributed</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Linear relationship between features and target</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Errors are independent of each other</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Errors have constant variance (homoscedasticity)</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Classification Details</h4>

                <div class="quiz-container" data-correct="d" data-explanation="Softmax generalizes sigmoid to multi-class classification, producing probability distribution over all classes that sum to 1.">
                    <p class="quiz-question">29. For multi-class classification with logistic regression, which function replaces sigmoid?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) ReLU</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Tanh</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Step function</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Softmax</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="One-vs-Rest trains N binary classifiers (one per class), each distinguishing one class from all others. Final prediction is the class with highest confidence.">
                    <p class="quiz-question">30. In One-vs-Rest (OvR) multi-class classification with 5 classes, how many binary classifiers are trained?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) 4</span></div>
                        <div class="quiz-option" data-value="b"><span>B) 5</span></div>
                        <div class="quiz-option" data-value="c"><span>C) 10</span></div>
                        <div class="quiz-option" data-value="d"><span>D) 25</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="Log loss (binary cross-entropy) heavily penalizes confident wrong predictions. Predicting 0.01 for a true positive incurs much higher loss than predicting 0.4.">
                    <p class="quiz-question">31. [NOAI Style] Why does log loss severely penalize confident but wrong predictions?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) It uses absolute differences</span></div>
                        <div class="quiz-option" data-value="b"><span>B) It squares the errors</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Log of values near 0 approaches negative infinity</span></div>
                        <div class="quiz-option" data-value="d"><span>D) It clips probabilities at 0.5</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Distance-Based Methods</h4>

                <div class="quiz-container" data-correct="a" data-explanation="Manhattan distance (L1) sums absolute differences: |x‚ÇÅ-y‚ÇÅ| + |x‚ÇÇ-y‚ÇÇ|. It's named after the grid-like street layout of Manhattan.">
                    <p class="quiz-question">32. What is the Manhattan distance formula?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Sum of absolute differences: |x‚ÇÅ-y‚ÇÅ| + |x‚ÇÇ-y‚ÇÇ| + ...</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Square root of sum of squared differences</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Maximum absolute difference</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Dot product of vectors</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="The curse of dimensionality means that in high dimensions, distances become increasingly similar (all points are roughly equidistant), making KNN less effective.">
                    <p class="quiz-question">33. Why does KNN suffer from the curse of dimensionality?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) More dimensions require more memory</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Training becomes slower</span></div>
                        <div class="quiz-option" data-value="c"><span>C) It cannot handle categorical features</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Distances become less meaningful as dimensions increase</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="Weighted KNN gives closer neighbors more influence on the prediction, typically using inverse distance weighting. This makes the algorithm more robust to outliers among neighbors.">
                    <p class="quiz-question">34. What is weighted KNN?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Using weighted features instead of original features</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Giving closer neighbors more influence on prediction</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Weighting different classes by their frequency</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Using different k values for different samples</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Tree-Based Methods</h4>

                <div class="quiz-container" data-correct="c" data-explanation="Decision trees naturally provide feature importance by measuring how much each feature reduces impurity across all splits where it's used.">
                    <p class="quiz-question">35. How do decision trees calculate feature importance?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) By counting how many times each feature appears in the tree</span></div>
                        <div class="quiz-option" data-value="b"><span>B) By the correlation with the target variable</span></div>
                        <div class="quiz-option" data-value="c"><span>C) By the total reduction in impurity from splits on that feature</span></div>
                        <div class="quiz-option" data-value="d"><span>D) By the depth at which the feature first appears</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="CART (Classification and Regression Trees) creates binary splits, while ID3 can create multi-way splits. CART also handles both classification and regression.">
                    <p class="quiz-question">36. [NOAI Style] What distinguishes CART from ID3 decision tree algorithms?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) CART creates binary splits only, ID3 can have multi-way splits</span></div>
                        <div class="quiz-option" data-value="b"><span>B) CART uses entropy, ID3 uses Gini</span></div>
                        <div class="quiz-option" data-value="c"><span>C) CART is only for regression, ID3 for classification</span></div>
                        <div class="quiz-option" data-value="d"><span>D) CART requires numerical features only</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="An entropy of 0 indicates a pure node where all samples belong to the same class - there's no uncertainty about the classification.">
                    <p class="quiz-question">37. What does an entropy value of 0 indicate for a decision tree node?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Maximum uncertainty - equal mix of all classes</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Pure node - all samples belong to the same class</span></div>
                        <div class="quiz-option" data-value="c"><span>C) The node should not be split further</span></div>
                        <div class="quiz-option" data-value="d"><span>D) The feature is not useful for splitting</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>SVM Deep Dive</h4>

                <div class="quiz-container" data-correct="d" data-explanation="The C parameter controls the trade-off between a wider margin and fewer misclassifications. Higher C means smaller margin but fewer training errors.">
                    <p class="quiz-question">38. What does the C parameter control in SVM?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) The number of support vectors</span></div>
                        <div class="quiz-option" data-value="b"><span>B) The kernel type to use</span></div>
                        <div class="quiz-option" data-value="c"><span>C) The dimensionality of the feature space</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Trade-off between margin width and classification errors</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="Soft margin SVM allows some misclassifications, controlled by C, making it suitable for non-linearly separable or noisy data. Hard margin requires perfect separation.">
                    <p class="quiz-question">39. What is the difference between soft margin and hard margin SVM?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Soft margin allows some misclassifications, hard margin doesn't</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Soft margin uses kernels, hard margin doesn't</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Hard margin is faster to train</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Soft margin only works for binary classification</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="Gamma in RBF kernel controls how far the influence of a single training example reaches. High gamma means close points have influence (potential overfitting), low gamma means far points also have influence.">
                    <p class="quiz-question">40. [NOAI Style] In RBF kernel SVM, what happens when gamma is very high?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Decision boundary becomes more linear</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Training becomes faster</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Model may overfit - only close points influence decision</span></div>
                        <div class="quiz-option" data-value="d"><span>D) More support vectors are created</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Ensemble Methods Extended</h4>

                <div class="quiz-container" data-correct="b" data-explanation="Out-of-bag (OOB) error uses the ~37% of samples not included in each bootstrap sample to estimate test error without needing a separate validation set.">
                    <p class="quiz-question">41. What is out-of-bag (OOB) error in Random Forest?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Error from samples that couldn't be classified</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Validation error using samples not in bootstrap samples</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Training error from outlier samples</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Error when bag-of-words features are used</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="Stacking uses predictions from base models as inputs to a meta-model that learns how to best combine them. This often outperforms simple averaging or voting.">
                    <p class="quiz-question">42. What is stacking (stacked generalization) in ensemble learning?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Training deeper decision trees</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Adding more features to the dataset</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Training models on sequential data subsets</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Using a meta-model to combine predictions of base models</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="Learning rate in boosting scales the contribution of each tree. Lower learning rate requires more trees but often leads to better generalization.">
                    <p class="quiz-question">43. What is the effect of a low learning rate in gradient boosting?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Each tree contributes less, requiring more trees for good performance</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Training becomes faster</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Fewer trees are needed</span></div>
                        <div class="quiz-option" data-value="d"><span>D) The model becomes more prone to overfitting</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Naive Bayes</h4>

                <div class="quiz-container" data-correct="c" data-explanation="Naive Bayes assumes all features are conditionally independent given the class label. This 'naive' assumption simplifies computation but is rarely true in practice.">
                    <p class="quiz-question">44. What assumption makes Naive Bayes 'naive'?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Features follow a normal distribution</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Classes are equally likely</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Features are conditionally independent given the class</span></div>
                        <div class="quiz-option" data-value="d"><span>D) All features have equal importance</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="Naive Bayes works well for text classification despite the independence assumption because word presence/absence patterns, even without considering dependencies, are informative.">
                    <p class="quiz-question">45. [NOAI Style] Why does Naive Bayes often work well for text classification despite its assumptions?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Text data always satisfies the independence assumption</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Word patterns are informative even without modeling dependencies</span></div>
                        <div class="quiz-option" data-value="c"><span>C) It uses more training data than other methods</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Text has fewer features than numerical data</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Model Evaluation Metrics</h4>

                <div class="quiz-container" data-correct="d" data-explanation="F1 score is the harmonic mean of precision and recall: 2 √ó (precision √ó recall) / (precision + recall). It balances both metrics equally.">
                    <p class="quiz-question">46. How is the F1 score calculated?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Arithmetic mean of precision and recall</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Geometric mean of precision and recall</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Weighted average based on class distribution</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Harmonic mean of precision and recall</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="AUC-ROC measures the probability that a randomly chosen positive example ranks higher than a randomly chosen negative example. 0.5 means random, 1.0 means perfect.">
                    <p class="quiz-question">47. What does AUC-ROC measure in classification?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Probability of ranking a positive higher than a negative sample</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Overall classification accuracy</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Training time efficiency</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Feature importance</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="Specificity (True Negative Rate) measures how well the model identifies actual negatives: TN / (TN + FP). High specificity means few false alarms.">
                    <p class="quiz-question">48. What is specificity in classification metrics?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) TP / (TP + FN) - same as recall</span></div>
                        <div class="quiz-option" data-value="b"><span>B) TP / (TP + FP) - same as precision</span></div>
                        <div class="quiz-option" data-value="c"><span>C) TN / (TN + FP) - true negative rate</span></div>
                        <div class="quiz-option" data-value="d"><span>D) (TP + TN) / Total - same as accuracy</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Hyperparameter Tuning</h4>

                <div class="quiz-container" data-correct="b" data-explanation="Grid search exhaustively tries all combinations of hyperparameters in a predefined grid. It's thorough but computationally expensive.">
                    <p class="quiz-question">49. [NOAI Style] How does grid search find optimal hyperparameters?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Using gradient descent on hyperparameters</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Exhaustively trying all combinations in a predefined grid</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Randomly sampling from hyperparameter distributions</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Using Bayesian optimization</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="a" data-explanation="Random search often finds good hyperparameters faster than grid search because it can explore more values for important hyperparameters while wasting less time on unimportant ones.">
                    <p class="quiz-question">50. When might random search outperform grid search for hyperparameter tuning?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) When some hyperparameters are more important than others</span></div>
                        <div class="quiz-option" data-value="b"><span>B) When the search space is very small</span></div>
                        <div class="quiz-option" data-value="c"><span>C) When you need deterministic results</span></div>
                        <div class="quiz-option" data-value="d"><span>D) When all hyperparameters interact strongly</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Handling Class Imbalance</h4>

                <div class="quiz-container" data-correct="c" data-explanation="SMOTE (Synthetic Minority Over-sampling Technique) creates synthetic examples by interpolating between existing minority class samples, rather than just duplicating them.">
                    <p class="quiz-question">51. How does SMOTE address class imbalance?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) By removing majority class samples</span></div>
                        <div class="quiz-option" data-value="b"><span>B) By duplicating minority class samples exactly</span></div>
                        <div class="quiz-option" data-value="c"><span>C) By creating synthetic minority samples through interpolation</span></div>
                        <div class="quiz-option" data-value="d"><span>D) By changing the loss function</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="d" data-explanation="Class weights make misclassifying minority class examples more costly. Higher weight for minority class forces the model to pay more attention to correctly classifying them.">
                    <p class="quiz-question">52. What is the effect of setting higher class weights for the minority class?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) The model trains faster</span></div>
                        <div class="quiz-option" data-value="b"><span>B) The decision boundary moves away from minority class</span></div>
                        <div class="quiz-option" data-value="c"><span>C) More features are selected</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Misclassifying minority class becomes more costly</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="b" data-explanation="Accuracy can be misleading with imbalanced classes. A model predicting only the majority class could achieve 95% accuracy on a 95/5 split while being useless.">
                    <p class="quiz-question">53. [NOAI Style] Why is accuracy a poor metric for imbalanced classification?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) It requires balanced classes to compute</span></div>
                        <div class="quiz-option" data-value="b"><span>B) A model predicting only majority class can achieve high accuracy</span></div>
                        <div class="quiz-option" data-value="c"><span>C) It doesn't account for training time</span></div>
                        <div class="quiz-option" data-value="d"><span>D) It's too computationally expensive</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <h4>Practical Considerations</h4>

                <div class="quiz-container" data-correct="a" data-explanation="Early stopping monitors validation performance and stops training when it starts degrading, preventing overfitting without needing to specify the exact number of iterations.">
                    <p class="quiz-question">54. What is early stopping in machine learning?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) Stopping training when validation performance degrades</span></div>
                        <div class="quiz-option" data-value="b"><span>B) Stopping after a fixed number of epochs</span></div>
                        <div class="quiz-option" data-value="c"><span>C) Stopping when training accuracy reaches 100%</span></div>
                        <div class="quiz-option" data-value="d"><span>D) Stopping when loss reaches zero</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-container" data-correct="c" data-explanation="Stratified k-fold maintains the same class distribution in each fold as in the original dataset. This is important for imbalanced datasets to ensure each fold is representative.">
                    <p class="quiz-question">55. What makes stratified k-fold cross-validation different from regular k-fold?</p>
                    <div class="quiz-options">
                        <div class="quiz-option" data-value="a"><span>A) It uses more folds</span></div>
                        <div class="quiz-option" data-value="b"><span>B) It shuffles data before splitting</span></div>
                        <div class="quiz-option" data-value="c"><span>C) It maintains class distribution in each fold</span></div>
                        <div class="quiz-option" data-value="d"><span>D) It uses overlapping folds</span></div>
                    </div>
                    <div class="quiz-feedback"></div>
                </div>
            </div>
        </section>

        <div class="container">
            <nav class="page-navigation">
                <a href="fundamentals.html" class="page-nav-link prev">
                    <span class="page-nav-label">‚Üê Previous</span>
                    <span class="page-nav-title">ML Fundamentals</span>
                </a>
                <a href="unsupervised.html" class="page-nav-link next">
                    <span class="page-nav-label">Next ‚Üí</span>
                    <span class="page-nav-title">Unsupervised Learning</span>
                </a>
            </nav>
        </div>
    </main>

    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-section">
                    <h4>ML for NOAI</h4>
                    <p>An educational resource for Singapore secondary school students preparing for the National Olympiad in AI.</p>
                </div>
                <div class="footer-section">
                    <h4>Quick Links</h4>
                    <ul>
                        <li><a href="fundamentals.html">ML Fundamentals</a></li>
                        <li><a href="supervised.html">Supervised Learning</a></li>
                        <li><a href="neural-networks.html">Neural Networks</a></li>
                    </ul>
                </div>
                <div class="footer-section">
                    <h4>References</h4>
                    <ul>
                        <li><a href="https://ioai-official.org/" target="_blank">IOAI Official</a></li>
                        <li><a href="https://aisingapore.org/" target="_blank">AI Singapore</a></li>
                    </ul>
                </div>
            </div>
            <div class="footer-bottom">
                <p>Educational content aligned with IOAI Syllabus. Not affiliated with AI Singapore or IOAI.</p>
            </div>
        </div>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>
